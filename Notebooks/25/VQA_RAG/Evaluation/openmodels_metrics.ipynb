{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "\"\"\"Direct QA evaluation\"\"\"\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "from pycocotools.coco import COCO\n",
    "from bert_score import BERTScorer\n",
    "from .eval import COCOEvalCap\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79478b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Evaluate on Qasa/Qasper.')\n",
    "parser.add_argument('--response_root', type=str, help='Response Root path.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def save_result(result, result_dir, filename, remove_duplicate='', is_gt=False):\n",
    "    final_result_file = os.path.join(result_dir, f'{filename}.json')\n",
    "\n",
    "    if remove_duplicate:\n",
    "        result_new = []\n",
    "        id_list = []\n",
    "        for res in result:\n",
    "            if res[remove_duplicate] not in id_list:\n",
    "                id_list.append(res[remove_duplicate])\n",
    "                result_new.append(res)\n",
    "        result = result_new\n",
    "\n",
    "    if is_gt:\n",
    "        images = []\n",
    "        for res in result:\n",
    "            images.append({\"id\": res[\"id\"]})\n",
    "        result = dict(annotations=result, images=images)\n",
    "\n",
    "    json.dump(result, open(final_result_file, 'w'))\n",
    "    print(f'result file saved to {final_result_file}')\n",
    "\n",
    "    return final_result_file\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_metrics(_RESPONSE_ROOT, scorer):\n",
    "  BERTScore_F1 = 0\n",
    "  all = 0\n",
    "  failed_parsing = 0\n",
    "  counter = 0\n",
    "  no_samples = 0\n",
    "\n",
    "  pycocoeval_like_pred = []\n",
    "  pycocoeval_like_gt = []\n",
    "\n",
    "  for paper_response in os.listdir(_RESPONSE_ROOT):\n",
    "    with open(os.path.join(_RESPONSE_ROOT, paper_response), 'r') as f:\n",
    "      saved_results = json.load(f)\n",
    "\n",
    "    for key, value in saved_results.items():\n",
    "\n",
    "      image_response = value['response']\n",
    "      gt = value['answer']\n",
    "\n",
    "      flag = 0 \n",
    "\n",
    "      for referred_figure, answer in image_response.items():\n",
    "\n",
    "        if 'no' in answer[0].lower():\n",
    "          no_samples += 1\n",
    "          continue\n",
    "        \n",
    "        else:\n",
    "            _, _, F1 = scorer.score([answer[1]], [gt])\n",
    "            all += 1\n",
    "            BERTScore_F1 += F1\n",
    "            pycocoeval_like_pred.append({\"image_id\": counter, \"caption\": answer[1]})\n",
    "            pycocoeval_like_gt.append({\"image_id\": counter, \"id\": counter, \"caption\": gt})\n",
    "            counter += 1 \n",
    "            flag = 1\n",
    "      \n",
    "      if flag == 0: ## For a questions, if the model says No for every referred image, then we consider the case as a failure\n",
    "        all += 1\n",
    "        BERTScore_F1 += 0\n",
    "        pycocoeval_like_pred.append({\"image_id\": counter, \"caption\": ''})\n",
    "        pycocoeval_like_gt.append({\"image_id\": counter, \"id\": counter, \"caption\": gt})\n",
    "        counter += 1 \n",
    "        \n",
    "\n",
    "  pycocoeval_pred_file = save_result(pycocoeval_like_pred, '.', 'pycocoeval_pred') # remove_duplicate='image_id'\n",
    "  pycocoeval_gt_file = save_result(pycocoeval_like_gt, '.', 'pycocoeval_gt', is_gt=True) # remove_duplicate='image_id'\n",
    "\n",
    "  coco = COCO(pycocoeval_gt_file)\n",
    "  coco_result = coco.loadRes(pycocoeval_pred_file)\n",
    "\n",
    "  # create coco_eval object by taking coco and coco_result\n",
    "  coco_eval = COCOEvalCap(coco, coco_result)\n",
    "  # evaluate results\n",
    "  coco_eval.evaluate(eval_metrics=['Bleu', 'METEOR', 'ROUGE_L', 'CIDEr'])\n",
    "\n",
    "  print(\".......Printing results.......\")\n",
    "  for metric, score in coco_eval.eval.items():\n",
    "    print(f'{metric}: {score:.3f}')\n",
    "  print(\"BERTScore F1: \", BERTScore_F1 / all)\n",
    "  print(\"Examples with Failed Parsing: {}\".format(failed_parsing))\n",
    "  print(\"all: \", all)\n",
    "  print(\"No samples: \", no_samples)\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "calculate_all_metrics(_RESPONSE_ROOT=args.response_root, scorer=scorer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
