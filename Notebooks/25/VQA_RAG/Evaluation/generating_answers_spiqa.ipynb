{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f960208",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai pandas scikit-learn nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc28a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0b5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0a05e",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c0f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../../Data/VisDoM-main/spiqa/spiqa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113df4c",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742c3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def compute_metrics(pred, true):\n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_score = rouge.score(true, pred)['rougeL'].fmeasure\n",
    "\n",
    "    bleu_score = sentence_bleu([true.split()], pred.split())\n",
    "    exact_match = int(pred.strip().lower() == true.strip().lower())\n",
    "    \n",
    "    return {\"bleu\": bleu_score, \"rougeL\": rouge_score, \"exact_match\": exact_match}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ea99c",
   "metadata": {},
   "source": [
    "# Runnning Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a6538",
   "metadata": {},
   "source": [
    "## Text only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb5665",
   "metadata": {},
   "source": [
    "### Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_answer_mistral(question, caption):\n",
    "    prompt = f\"\"\"Caption: {caption}\n",
    "\n",
    "Please provide a brief and accurate answer to the following question based on the above caption.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistral\",  # make sure this matches your LM Studio model name\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# üîç Example\n",
    "caption = \"Figure 4. Visualizations of the preferred inputs for different class units on layer fc8...\"\n",
    "question = \"How many hyperparameter combinations were used for the random hyperparameter search?\"\n",
    "\n",
    "print(generate_answer_mistral(question, caption))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fee12",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eeaad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text_only_spiqa(df):\n",
    "    results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        caption = row['caption']  # assumes pre-extracted caption\n",
    "        true_answer = row['answer']\n",
    "\n",
    "        pred_answer = generate_answer_mistral(question, caption)\n",
    "        metrics = compute_metrics(pred_answer, true_answer)\n",
    "\n",
    "        results.append({\n",
    "            \"q_id\": row['q_id'],\n",
    "            \"question\": question,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"generated_answer\": pred_answer,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad12efb",
   "metadata": {},
   "source": [
    "### Simple Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfadcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"Figure 4. Visualizations of the preferred inputs for different class units on layer fc8...\"\n",
    "question = \"How many hyperparameter combinations were used for the random hyperparameter search?\"\n",
    "\n",
    "print(generate_answer_mistral(question, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"question\": \"How many hyperparameter combinations were used for the random hyperparameter search?\",\n",
    "    \"answer\": \"300 sets of possible hyperparameter combinations then choose four of them that complement each other well.\",\n",
    "    \"caption\": \"To pick a reasonable set of hyperparameters for all methods at once, we ran a random hyperparameter search of 300 possible combinations and settled on four that complement each other well. The four selected combinations are listed in Table 1...\"\n",
    "}\n",
    "\n",
    "# Run\n",
    "question = example[\"question\"]\n",
    "caption = example[\"caption\"]\n",
    "true_answer = example[\"answer\"]\n",
    "\n",
    "generated = generate_answer_mistral(question, caption)\n",
    "metrics = compute_metrics(generated, true_answer)\n",
    "\n",
    "print(\"Q:\", question)\n",
    "print(\"Generated:\", generated)\n",
    "print(\"True:\", true_answer)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e54ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]\n",
    "image_path = os.path.join(\"/your/images/folder\", row[\"reference_figure\"])\n",
    "caption = row[\"caption\"]\n",
    "question = row[\"question\"]\n",
    "true_answer = row[\"answer\"]\n",
    "\n",
    "# LLaVA example\n",
    "# gen_answer = generate_llava_answer(image_path, caption, question)\n",
    "\n",
    "# Mistral example\n",
    "gen_answer = generate_answer_mistral(caption, question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Generated:\", gen_answer)\n",
    "print(\"Gold:\", true_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5db1ed",
   "metadata": {},
   "source": [
    "### Batch based Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ead8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# === Config ===\n",
    "CSV_PATH = \"../../../../Data/VisDoM-main/spiqa/spiqa.csv\"\n",
    "# IMAGE_FOLDER = \"/path/to/your/image/folder\"  # not used here, but kept for consistency\n",
    "RESULTS_PATH = \"../../../../Data/spiqa_eval/mistral_only\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# === Connect to LM Studio ===\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1235/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db437b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mistral generation function ===\n",
    "def generate_mistral_answer(caption, question):\n",
    "    prompt = f\"Caption: {caption}\\n\\nPlease answer the following question based on the caption above:\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistral\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96a2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_INDEX = 0  # Change this for each run: 0, 10, 20, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ensure output folder exists ===\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "end_index = START_INDEX + BATCH_SIZE\n",
    "batch_file = os.path.join(RESULTS_PATH, f\"mistral_generated_batch_{START_INDEX}_{end_index}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b90d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af3a0110da44f6c9ca058f212654e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral batch saved to ../../../../Data/spiqa_eval/mistral_only\\mistral_generated_batch_0_10.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Load the dataset and select the batch ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "batch_df = df.iloc[START_INDEX:end_index]\n",
    "\n",
    "# === Process each row in the batch ===\n",
    "results = []\n",
    "for idx, row in tqdm(batch_df.iterrows()):\n",
    "    generated = generate_mistral_answer(row[\"caption\"], row[\"question\"])\n",
    "    results.append({\n",
    "        \"q_id\": row[\"q_id\"],\n",
    "        \"doc_id\": row[\"doc_id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"true_answer\": row[\"answer\"],\n",
    "        \"generated_answer\": generated,\n",
    "        \"reference_figure\": row[\"reference_figure\"],\n",
    "        \"caption\": row[\"caption\"]\n",
    "    })\n",
    "\n",
    "# === Save batch results ===\n",
    "pd.DataFrame(results).to_csv(batch_file, index=False)\n",
    "print(f\"‚úÖ Mistral batch saved to {batch_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4122d",
   "metadata": {},
   "source": [
    "mistral only 1st batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c738e4",
   "metadata": {},
   "source": [
    "## Vision Model Function and Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2047db3",
   "metadata": {},
   "source": [
    "### Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fbb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def image_to_base64(image_path, size=(336, 336)):\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.resize(size)\n",
    "        return base64.b64encode(img.tobytes()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9c0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1235/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def generate_answer_llava(image_path, caption, question, prompt_template):\n",
    "    prompt = prompt_template.replace(\"<caption>\", caption).replace(\"<question>\", question)\n",
    "\n",
    "    # prompt = f\"<image>\\nCaption: {caption} Please provide a brief answer to the following question after looking into the input image and caption.\\nQuestion: {question}.\\nASSISTANT:\"\n",
    "\n",
    "    # Encode image to base64\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llava-v1.5-7b@q5_k_m\",  # replace with your model name\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b104bb",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f520ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "def evaluate_generation(df, image_base_path):\n",
    "    results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        true_answer = row['answer']\n",
    "        fig_ids = row.get('reference_figure', [])\n",
    "\n",
    "        # Full paths to the referenced images\n",
    "        image_paths = [f\"{image_base_path}/{fig}\" for fig in fig_ids if isinstance(fig, str)]\n",
    "\n",
    "        # Generate using LLaVA\n",
    "        pred_answer = generate_answer_llava(question, image_paths)\n",
    "        \n",
    "        metrics = compute_metrics(pred_answer, true_answer)\n",
    "        results.append({\n",
    "            \"q_id\": row['q_id'],\n",
    "            \"question\": question,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"generated_answer\": pred_answer,\n",
    "            **metrics\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063cd14",
   "metadata": {},
   "source": [
    "### Simple Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_FOLDER_PATH = \"../../../../Data/spiqa/test-B/Images/SPIQA_testB_Images/SPIQA_testB_Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a930df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text in the image is not clear enough to discern the number of hyperparameter combinations that were used for the random hyperparameter search.\n"
     ]
    }
   ],
   "source": [
    "caption = \"Figure 4. Visualizations of the preferred inputs for different class units on layer fc8...\"\n",
    "question = \"How many hyperparameter combinations were used for the random hyperparameter search?\"\n",
    "image_path = IMG_FOLDER_PATH+\"1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png\"\n",
    "\n",
    "_PROMPT = \"\"\"<image>\\n Caption: <caption> Please provide a brief answer to the following question after looking into the input image and caption. Question: <question>.\\nASSISTANT:\"\"\"\n",
    "\n",
    "answer = generate_answer_llava(image_path, caption, question, _PROMPT)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86430827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "image_path = IMG_FOLDER_PATH+\"1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png\"\n",
    "caption = \"Figure 4. Visualizations of the preferred inputs for different class units on layer fc8...\"\n",
    "question = \"How many hyperparameter combinations were used for the random hyperparameter search?\"\n",
    "\n",
    "print(generate_answer_llava(image_path, caption, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f69f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  \n",
      "\n",
      "What is the difference between the original and pre-processed SMD Navigate data? \n",
      "Generated: The original SMD Navigate data contains unstructured textual information, which is difficult to process and analyze. On the other hand, the pre-processed SMD Navigate data has been transformed into a structured format that can be easily understood and analyzed. This transformation involves steps such as tokenization, lemmatization, stemming or stopword removal, and converting the text into numerical vectors suitable for machine learning algorithms. By doing so, the pre-processed data becomes more accessible and easier to work with, which facilitates tasks like classification or clustering of the textual content in the dataset.\n",
      "Gold:  \n",
      "\n",
      "The pre-processed SMD Navigate data combines all the properties (such as distance, address) of a point of interest (POI) into a single subject with the object being \"poi\". The original data had separate entries for each property. \n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "image_path = os.path.join(\"../../../../Data/spiqa/test-A/SPIQA_testA_Images/SPIQA_testA_Images\",row['doc_id'], row[\"reference_figure\"])\n",
    "caption = row[\"caption\"]\n",
    "question = row[\"question\"]\n",
    "true_answer = row[\"answer\"]\n",
    "\n",
    "# LLaVA example\n",
    "gen_answer = generate_answer_llava(image_path, caption, question, _PROMPT)\n",
    "\n",
    "# Mistral example\n",
    "# gen_answer = generate_mistral_answer(caption, question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Generated:\", gen_answer)\n",
    "print(\"Gold:\", true_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a86b52",
   "metadata": {},
   "source": [
    "### Total Answer Generation for -SPIQA - ViSDOMRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8268808",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load modified SPIQA DataFrame here:\n",
    "df = pd.read_csv(\"../../../../Data/VisDoM-main/spiqa/spiqa.csv\")  # or .json, or load however you have it\n",
    "\n",
    "_PROMPT = \"\"\"<image>\\n Caption: <caption> Please provide a brief answer to the following question after looking into the input image and caption. Question: <question>.\\nASSISTANT:\"\"\"\n",
    "\n",
    "# Path to image folder\n",
    "image_folder = \"../../../../Data/spiqa/test-A/SPIQA_testA_Images/SPIQA_testA_Images\"\n",
    "\n",
    "# Evaluate all rows\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_path = os.path.join(image_folder,row['doc_id'], row[\"reference_figure\"])\n",
    "    generated = generate_answer_llava(image_path, row[\"caption\"], row[\"question\"],_PROMPT)\n",
    "    results.append({\n",
    "        \"q_id\": row[\"q_id\"],\n",
    "        \"doc_id\": row[\"doc_id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"true_answer\": row[\"answer\"],\n",
    "        \"generated_answer\": generated,\n",
    "        \"reference_figure\": row[\"reference_figure\"],\n",
    "        \"caption\": row[\"caption\"]\n",
    "    })\n",
    "\n",
    "# Save to CSV or further process\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"llava_generated_answers_spiqa_visdom_vision.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc180e79",
   "metadata": {},
   "source": [
    "### Batchwise Generation for LLava SpiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc425ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c6b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1235/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# === Config ===\n",
    "CSV_PATH = \"../../../../Data/VisDoM-main/spiqa/spiqa.csv\"\n",
    "RESULTS_PATH = \"../../../../Data/spiqa_eval/llava_only\"\n",
    "IMAGE_FOLDER = \"../../../../Data/spiqa/test-A/SPIQA_testA_Images/SPIQA_testA_Images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8a7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Prompt ===\n",
    "_PROMPT = \"\"\"<image>\\n Caption: <caption> Please provide a brief answer to the following question after looking into the input image and caption. Question: <question>.\\nASSISTANT:\"\"\"\n",
    "\n",
    "# === Generation Function ===\n",
    "def generate_answer_llava(image_path, caption, question, prompt_template):\n",
    "    prompt = prompt_template.replace(\"<caption>\", caption).replace(\"<question>\", question)\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        return \"[Image not found]\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llava\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }],\n",
    "            temperature=0.7,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c8fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "START_INDEX = 0  # Change this for each run: 0, 10, 20...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3517c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbde86b70119494fb57f398a1f9eb549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# === Load Data ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "end_index = min(START_INDEX + BATCH_SIZE, len(df))\n",
    "batch_df = df.iloc[START_INDEX:end_index]\n",
    "\n",
    "# === Process Batch ===\n",
    "results = []\n",
    "for idx, row in tqdm(batch_df.iterrows()):\n",
    "    \n",
    "    image_path = os.path.join(IMAGE_FOLDER, row['doc_id'], row[\"reference_figure\"])\n",
    "    generated = generate_answer_llava(image_path, row[\"caption\"], row[\"question\"], _PROMPT)\n",
    "    results.append({\n",
    "        \"q_id\": row[\"q_id\"],\n",
    "        \"doc_id\": row[\"doc_id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"true_answer\": row[\"answer\"],\n",
    "        \"generated_answer\": generated,\n",
    "        \"reference_figure\": row[\"reference_figure\"],\n",
    "        \"caption\": row[\"caption\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ea1863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch saved to ../../../../Data/spiqa_eval/llava_only\\llava_generated_batch_0_10.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Save Batch ===\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "batch_file = os.path.join(RESULTS_PATH,f\"llava_generated_batch_{START_INDEX}_{end_index}.csv\")\n",
    "pd.DataFrame(results).to_csv(batch_file, index=False)\n",
    "print(f\"‚úÖ Batch saved to {batch_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e75b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st batch  - 82m 35s - llava only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
