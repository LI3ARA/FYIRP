{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6543601c",
   "metadata": {},
   "source": [
    "- Experiment only\n",
    "- In the implementation use pymupdfloader from langchian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21042d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '../../../Data/pdfs_for_Vanilla_RAG/1706.03762v7.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e6760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "g\n",
      "u\n",
      "A\n",
      "2\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "7\n",
      "v\n",
      "2\n",
      "6\n",
      "7\n",
      "3\n",
      "0\n",
      ".\n",
      "6\n",
      "0\n",
      "7\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "2\n",
      "\n",
      "\fFigure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "3\n",
      "\n",
      "\fScaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "dk, and apply a softmax function to obtain the weights on the\n",
      "query with all keys, divide each by\n",
      "values.\n",
      "\n",
      "√\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "\n",
      "Attention(Q, K, V ) = softmax(\n",
      "\n",
      "QK T\n",
      "√\n",
      "dk\n",
      "\n",
      ")V\n",
      "\n",
      "(1)\n",
      "\n",
      "1√\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "of\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "\n",
      "dk\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√\n",
      "dk\n",
      "\n",
      ".\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\n",
      "\n",
      "4\n",
      "\n",
      "\foutput values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
      "\n",
      "where headi = Attention(QW Q\n",
      "\n",
      "i , KW K\n",
      "i\n",
      "\n",
      ", V W V\n",
      "\n",
      "i )\n",
      "\n",
      "Where the projections are parameter matrices W Q\n",
      "and W O ∈ Rhdv×dmodel.\n",
      "\n",
      "i ∈ Rdmodel×dk , W K\n",
      "\n",
      "i ∈ Rdmodel×dk , W V\n",
      "\n",
      "i ∈ Rdmodel×dv\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "\n",
      "(2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "df f = 2048.\n",
      "\n",
      "3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "dmodel.\n",
      "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by\n",
      "\n",
      "√\n",
      "\n",
      "5\n",
      "\n",
      "\fTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "Layer Type\n",
      "\n",
      "Complexity per Layer\n",
      "\n",
      "Self-Attention\n",
      "Recurrent\n",
      "Convolutional\n",
      "Self-Attention (restricted)\n",
      "\n",
      "O(n2 · d)\n",
      "O(n · d2)\n",
      "O(k · n · d2)\n",
      "O(r · n · d)\n",
      "\n",
      "Sequential Maximum Path Length\n",
      "Operations\n",
      "O(1)\n",
      "O(n)\n",
      "O(1)\n",
      "O(1)\n",
      "\n",
      "O(1)\n",
      "O(n)\n",
      "O(logk(n))\n",
      "O(n/r)\n",
      "\n",
      "3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
      "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
      "P Epos.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "\n",
      "4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "6\n",
      "\n",
      "\flength n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
      "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "\n",
      "5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "\n",
      "5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "\n",
      "5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "\n",
      "lrate = d−0.5\n",
      "\n",
      "model · min(step_num−0.5, step_num · warmup_steps−1.5)\n",
      "\n",
      "(3)\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps = 4000.\n",
      "\n",
      "5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "7\n",
      "\n",
      "\fTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "\n",
      "Model\n",
      "\n",
      "ByteNet [18]\n",
      "Deep-Att + PosUnk [39]\n",
      "GNMT + RL [38]\n",
      "ConvS2S [9]\n",
      "MoE [32]\n",
      "Deep-Att + PosUnk Ensemble [39]\n",
      "GNMT + RL Ensemble [38]\n",
      "ConvS2S Ensemble [9]\n",
      "Transformer (base model)\n",
      "Transformer (big)\n",
      "\n",
      "BLEU\n",
      "\n",
      "Training Cost (FLOPs)\n",
      "\n",
      "EN-DE EN-FR\n",
      "23.75\n",
      "\n",
      "24.6\n",
      "25.16\n",
      "26.03\n",
      "\n",
      "26.30\n",
      "26.36\n",
      "27.3\n",
      "28.4\n",
      "\n",
      "39.2\n",
      "39.92\n",
      "40.46\n",
      "40.56\n",
      "40.4\n",
      "41.16\n",
      "41.29\n",
      "38.1\n",
      "41.8\n",
      "\n",
      "EN-DE\n",
      "\n",
      "EN-FR\n",
      "\n",
      "2.3 · 1019\n",
      "9.6 · 1018\n",
      "2.0 · 1019\n",
      "\n",
      "1.8 · 1020\n",
      "7.7 · 1019\n",
      "\n",
      "1.0 · 1020\n",
      "1.4 · 1020\n",
      "1.5 · 1020\n",
      "1.2 · 1020\n",
      "8.0 · 1020\n",
      "1.1 · 1021\n",
      "1.2 · 1021\n",
      "\n",
      "3.3 · 1018\n",
      "2.3 · 1019\n",
      "\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "6 Results\n",
      "\n",
      "6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "\n",
      "6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "8\n",
      "\n",
      "\fTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "\n",
      "N dmodel\n",
      "\n",
      "dff\n",
      "\n",
      "base\n",
      "\n",
      "6\n",
      "\n",
      "512\n",
      "\n",
      "2048\n",
      "\n",
      "ϵls\n",
      "\n",
      "0.1\n",
      "\n",
      "dv\n",
      "\n",
      "Pdrop\n",
      "\n",
      "0.1\n",
      "\n",
      "64\n",
      "512\n",
      "128\n",
      "32\n",
      "16\n",
      "\n",
      "h\n",
      "\n",
      "8\n",
      "1\n",
      "4\n",
      "16\n",
      "32\n",
      "\n",
      "dk\n",
      "\n",
      "64\n",
      "512\n",
      "128\n",
      "32\n",
      "16\n",
      "16\n",
      "32\n",
      "\n",
      "2\n",
      "4\n",
      "8\n",
      "\n",
      "256\n",
      "1024\n",
      "\n",
      "32\n",
      "128\n",
      "\n",
      "32\n",
      "128\n",
      "\n",
      "1024\n",
      "4096\n",
      "\n",
      "0.0\n",
      "0.2\n",
      "\n",
      "0.0\n",
      "0.2\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "6\n",
      "\n",
      "1024\n",
      "\n",
      "4096\n",
      "\n",
      "16\n",
      "\n",
      "0.3\n",
      "\n",
      "(A)\n",
      "\n",
      "(B)\n",
      "\n",
      "(C)\n",
      "\n",
      "(D)\n",
      "\n",
      "(E)\n",
      "big\n",
      "\n",
      "PPL\n",
      "train\n",
      "steps\n",
      "(dev)\n",
      "100K 4.92\n",
      "5.29\n",
      "5.00\n",
      "4.91\n",
      "5.01\n",
      "5.16\n",
      "5.01\n",
      "6.11\n",
      "5.19\n",
      "4.88\n",
      "5.75\n",
      "4.66\n",
      "5.12\n",
      "4.75\n",
      "5.77\n",
      "4.95\n",
      "4.67\n",
      "5.47\n",
      "4.92\n",
      "300K 4.33\n",
      "\n",
      "BLEU params\n",
      "×106\n",
      "(dev)\n",
      "25.8\n",
      "65\n",
      "24.9\n",
      "25.5\n",
      "25.8\n",
      "25.4\n",
      "25.1\n",
      "25.4\n",
      "23.7\n",
      "25.3\n",
      "25.5\n",
      "24.5\n",
      "26.0\n",
      "25.4\n",
      "26.2\n",
      "24.6\n",
      "25.5\n",
      "25.3\n",
      "25.7\n",
      "25.7\n",
      "26.4\n",
      "\n",
      "58\n",
      "60\n",
      "36\n",
      "50\n",
      "80\n",
      "28\n",
      "168\n",
      "53\n",
      "90\n",
      "\n",
      "213\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "\n",
      "6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "9\n",
      "\n",
      "\fTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "\n",
      "Parser\n",
      "\n",
      "Training\n",
      "\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "multi-task\n",
      "generative\n",
      "\n",
      "Petrov et al. (2006) [29]\n",
      "Zhu et al. (2013) [40]\n",
      "Dyer et al. (2016) [8]\n",
      "Transformer (4 layers)\n",
      "Zhu et al. (2013) [40]\n",
      "Huang & Harper (2009) [14]\n",
      "McClosky et al. (2006) [26]\n",
      "Vinyals & Kaiser el al. (2014) [37]\n",
      "Transformer (4 layers)\n",
      "Luong et al. (2015) [23]\n",
      "Dyer et al. (2016) [8]\n",
      "\n",
      "WSJ 23 F1\n",
      "88.3\n",
      "90.4\n",
      "90.4\n",
      "91.7\n",
      "91.3\n",
      "91.3\n",
      "91.3\n",
      "92.1\n",
      "92.1\n",
      "92.7\n",
      "93.0\n",
      "93.3\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "\n",
      "References\n",
      "\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "\n",
      "arXiv:1607.06450, 2016.\n",
      "\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
      "\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "\n",
      "10\n",
      "\n",
      "\f[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR, abs/1406.1078, 2014.\n",
      "\n",
      "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "\n",
      "preprint arXiv:1610.02357, 2016.\n",
      "\n",
      "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "\n",
      "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "\n",
      "network grammars. In Proc. of NAACL, 2016.\n",
      "\n",
      "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks.\n",
      "\n",
      "arXiv preprint\n",
      "\n",
      "arXiv:1308.0850, 2013.\n",
      "\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 770–778, 2016.\n",
      "\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "\n",
      "9(8):1735–1780, 1997.\n",
      "\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing, pages 832–841. ACL, August 2009.\n",
      "\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "\n",
      "on Learning Representations (ICLR), 2016.\n",
      "\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "\n",
      "In International Conference on Learning Representations, 2017.\n",
      "\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "\n",
      "arXiv:1703.10722, 2017.\n",
      "\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "\n",
      "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "\n",
      "11\n",
      "\n",
      "\f[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
      "\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      "pages 152–159. ACL, June 2006.\n",
      "\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "\n",
      "model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
      "2006.\n",
      "\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "\n",
      "preprint arXiv:1608.05859, 2016.\n",
      "\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "\n",
      "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "\n",
      "12\n",
      "\n",
      "\fAttention Visualizations\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "13\n",
      "\n",
      "Input-Input Layer5ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>\fFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "14\n",
      "\n",
      "Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>\fFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "15\n",
      "\n",
      "Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>\f\n"
     ]
    }
   ],
   "source": [
    "# The extract_text() function is the easiest way to get plain text\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text = extract_text(pdf_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c9a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providedproperattributionisprovided,Googleherebygrantspermissiontoreproducethetablesandfiguresinthispapersolelyforuseinjournalisticorscholarlyworks.AttentionIsAllYouNeedAshishVaswani∗GoogleBrainavaswani@google.comNoamShazeer∗GoogleBrainnoam@google.comNikiParmar∗GoogleResearchnikip@google.comJakobUszkoreit∗GoogleResearchusz@google.comLlionJones∗GoogleResearchllion@google.comAidanN.Gomez∗†UniversityofTorontoaidan@cs.toronto.eduŁukaszKaiser∗GoogleBrainlukaszkaiser@google.comIlliaPolosukhin∗‡illia.polosukhin@gmail.comAbstractThedominantsequencetransductionmodelsarebasedoncomplexrecurrentorconvolutionalneuralnetworksthatincludeanencoderandadecoder.Thebestperformingmodelsalsoconnecttheencoderanddecoderthroughanattentionmechanism.Weproposeanewsimplenetworkarchitecture,theTransformer,basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutionsentirely.Experimentsontwomachinetranslationtasksshowthesemodelstobesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantlylesstimetotrain.Ourmodelachieves28.4BLEUontheWMT2014English-to-Germantranslationtask,improvingovertheexistingbestresults,includingensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8aftertrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthebestmodelsfromtheliterature.WeshowthattheTransformergeneralizeswelltoothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwithlargeandlimitedtrainingdata.∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstartedtheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsandhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-headattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyeverydetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseandtensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,andefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofandimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyacceleratingourresearch.†WorkperformedwhileatGoogleBrain.‡WorkperformedwhileatGoogleResearch.31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\f1IntroductionRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworksinparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingandtransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5].Numerouseffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoderarchitectures[38,24,15].Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutputsequences.Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhiddenstatesht,asafunctionoftheprevioushiddenstateht−1andtheinputforpositiont.Thisinherentlysequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlongersequencelengths,asmemoryconstraintslimitbatchingacrossexamples.Recentworkhasachievedsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditionalcomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter.Thefundamentalconstraintofsequentialcomputation,however,remains.Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistanceintheinputoroutputsequences[2,19].Inallbutafewcases[27],however,suchattentionmechanismsareusedinconjunctionwitharecurrentnetwork.InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinsteadrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartintranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.2BackgroundThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuildingblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrowsinthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.Thismakesitmoredifficulttolearndependenciesbetweendistantpositions[12].IntheTransformerthisisreducedtoaconstantnumberofoperations,albeitatthecostofreducedeffectiveresolutionduetoaveragingattention-weightedpositions,aneffectwecounteractwithMulti-HeadAttentionasdescribedinsection3.2.Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositionsofasinglesequenceinordertocomputearepresentationofthesequence.Self-attentionhasbeenusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringandlanguagemodelingtasks[34].Tothebestofourknowledge,however,theTransformeristhefirsttransductionmodelrelyingentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-alignedRNNsorconvolution.Inthefollowingsections,wewilldescribetheTransformer,motivateself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].3ModelArchitectureMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].Here,theencodermapsaninputsequenceofsymbolrepresentations(x1,...,xn)toasequenceofcontinuousrepresentationsz=(z1,...,zn).Givenz,thedecoderthengeneratesanoutputsequence(y1,...,ym)ofsymbolsoneelementatatime.Ateachstepthemodelisauto-regressive[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.2\fFigure1:TheTransformer-modelarchitecture.TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fullyconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,respectively.3.1EncoderandDecoderStacksEncoder:TheencoderiscomposedofastackofN=6identicallayers.Eachlayerhastwosub-layers.Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-wisefullyconnectedfeed-forwardnetwork.Weemployaresidualconnection[11]aroundeachofthetwosub-layers,followedbylayernormalization[1].Thatis,theoutputofeachsub-layerisLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layeritself.Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembeddinglayers,produceoutputsofdimensiondmodel=512.Decoder:ThedecoderisalsocomposedofastackofN=6identicallayers.Inadditiontothetwosub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-headattentionovertheoutputoftheencoderstack.Similartotheencoder,weemployresidualconnectionsaroundeachofthesub-layers,followedbylayernormalization.Wealsomodifytheself-attentionsub-layerinthedecoderstacktopreventpositionsfromattendingtosubsequentpositions.Thismasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthepredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.3.2AttentionAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,wherethequery,keys,values,andoutputareallvectors.Theoutputiscomputedasaweightedsum3\fScaledDot-ProductAttentionMulti-HeadAttentionFigure2:(left)ScaledDot-ProductAttention.(right)Multi-HeadAttentionconsistsofseveralattentionlayersrunninginparallel.ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthequerywiththecorrespondingkey.3.2.1ScaledDot-ProductAttentionWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2).Theinputconsistsofqueriesandkeysofdimensiondk,andvaluesofdimensiondv.Wecomputethedotproductsofthequerywithallkeys,divideeachby√dk,andapplyasoftmaxfunctiontoobtaintheweightsonthevalues.Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogetherintoamatrixQ.ThekeysandvaluesarealsopackedtogetherintomatricesKandV.Wecomputethematrixofoutputsas:Attention(Q,K,V)=softmax(QKT√dk)V(1)Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-plicative)attention.Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactorof1√dk.Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwithasinglehiddenlayer.Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionismuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimizedmatrixmultiplicationcode.Whileforsmallvaluesofdkthetwomechanismsperformsimilarly,additiveattentionoutperformsdotproductattentionwithoutscalingforlargervaluesofdk[3].Wesuspectthatforlargevaluesofdk,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithasextremelysmallgradients4.Tocounteractthiseffect,wescalethedotproductsby1√dk.3.2.2Multi-HeadAttentionInsteadofperformingasingleattentionfunctionwithdmodel-dimensionalkeys,valuesandqueries,wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learnedlinearprojectionstodk,dkanddvdimensions,respectively.Oneachoftheseprojectedversionsofqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingdv-dimensional4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandomvariableswithmean0andvariance1.Thentheirdotproduct,q·k=(cid:80)dki=1qiki,hasmean0andvariancedk.4\foutputvalues.Theseareconcatenatedandonceagainprojected,resultinginthefinalvalues,asdepictedinFigure2.Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentationsubspacesatdifferentpositions.Withasingleattentionhead,averaginginhibitsthis.MultiHead(Q,K,V)=Concat(head1,...,headh)WOwhereheadi=Attention(QWQi,KWKi,VWVi)WheretheprojectionsareparametermatricesWQi∈Rdmodel×dk,WKi∈Rdmodel×dk,WVi∈Rdmodel×dvandWO∈Rhdv×dmodel.Inthisworkweemployh=8parallelattentionlayers,orheads.Foreachoftheseweusedk=dv=dmodel/h=64.Duetothereduceddimensionofeachhead,thetotalcomputationalcostissimilartothatofsingle-headattentionwithfulldimensionality.3.2.3ApplicationsofAttentioninourModelTheTransformerusesmulti-headattentioninthreedifferentways:•In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,andthememorykeysandvaluescomefromtheoutputoftheencoder.Thisallowseverypositioninthedecodertoattendoverallpositionsintheinputsequence.Thismimicsthetypicalencoder-decoderattentionmechanismsinsequence-to-sequencemodelssuchas[38,2,9].•Theencodercontainsself-attentionlayers.Inaself-attentionlayerallofthekeys,valuesandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerintheencoder.Eachpositionintheencodercanattendtoallpositionsinthepreviouslayeroftheencoder.•Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendtoallpositionsinthedecoderuptoandincludingthatposition.Weneedtopreventleftwardinformationflowinthedecodertopreservetheauto-regressiveproperty.Weimplementthisinsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinputofthesoftmaxwhichcorrespondtoillegalconnections.SeeFigure2.3.3Position-wiseFeed-ForwardNetworksInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafullyconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically.ThisconsistsoftwolineartransformationswithaReLUactivationinbetween.FFN(x)=max(0,xW1+b1)W2+b2(2)Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparametersfromlayertolayer.Anotherwayofdescribingthisisastwoconvolutionswithkernelsize1.Thedimensionalityofinputandoutputisdmodel=512,andtheinner-layerhasdimensionalitydff=2048.3.4EmbeddingsandSoftmaxSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinputtokensandoutputtokenstovectorsofdimensiondmodel.Wealsousetheusuallearnedlineartransfor-mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities.Inourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-softmaxlineartransformation,similarto[30].Intheembeddinglayers,wemultiplythoseweightsby√dmodel.5\fTable1:Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperationsfordifferentlayertypes.nisthesequencelength,distherepresentationdimension,kisthekernelsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.LayerTypeComplexityperLayerSequentialMaximumPathLengthOperationsSelf-AttentionO(n2·d)O(1)O(1)RecurrentO(n·d2)O(n)O(n)ConvolutionalO(k·n·d2)O(1)O(logk(n))Self-Attention(restricted)O(r·n·d)O(1)O(n/r)3.5PositionalEncodingSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseoftheorderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthetokensinthesequence.Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthebottomsoftheencoderanddecoderstacks.Thepositionalencodingshavethesamedimensiondmodelastheembeddings,sothatthetwocanbesummed.Therearemanychoicesofpositionalencodings,learnedandfixed[9].Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:PE(pos,2i)=sin(pos/100002i/dmodel)PE(pos,2i+1)=cos(pos/100002i/dmodel)whereposisthepositionandiisthedimension.Thatis,eachdimensionofthepositionalencodingcorrespondstoasinusoid.Thewavelengthsformageometricprogressionfrom2πto10000·2π.Wechosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendbyrelativepositions,sinceforanyfixedoffsetk,PEpos+kcanberepresentedasalinearfunctionofPEpos.Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwoversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversionbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencounteredduringtraining.4WhySelf-AttentionInthissectionwecomparevariousaspectsofself-attentionlayerstotherecurrentandconvolu-tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations(x1,...,xn)toanothersequenceofequallength(z1,...,zn),withxi,zi∈Rd,suchasahiddenlayerinatypicalsequencetransductionencoderordecoder.Motivatingouruseofself-attentionweconsiderthreedesiderata.Oneisthetotalcomputationalcomplexityperlayer.Anotheristheamountofcomputationthatcanbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork.Learninglong-rangedependenciesisakeychallengeinmanysequencetransductiontasks.Onekeyfactoraffectingtheabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshavetotraverseinthenetwork.Theshorterthesepathsbetweenanycombinationofpositionsintheinputandoutputsequences,theeasieritistolearnlong-rangedependencies[12].Hencewealsocomparethemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthedifferentlayertypes.AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentiallyexecutedoperations,whereasarecurrentlayerrequiresO(n)sequentialoperations.Intermsofcomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence6\flengthnissmallerthantherepresentationdimensionalityd,whichismostoftenthecasewithsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece[38]andbyte-pair[31]representations.Toimprovecomputationalperformancefortasksinvolvingverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerintheinputsequencecenteredaroundtherespectiveoutputposition.ThiswouldincreasethemaximumpathlengthtoO(n/r).Weplantoinvestigatethisapproachfurtherinfuturework.Asingleconvolutionallayerwithkernelwidthk<ndoesnotconnectallpairsofinputandoutputpositions.DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,orO(logk(n))inthecaseofdilatedconvolutions[18],increasingthelengthofthelongestpathsbetweenanytwopositionsinthenetwork.Convolutionallayersaregenerallymoreexpensivethanrecurrentlayers,byafactorofk.Separableconvolutions[6],however,decreasethecomplexityconsiderably,toO(k·n·d+n·d2).Evenwithk=n,however,thecomplexityofaseparableconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,theapproachwetakeinourmodel.Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributionsfromourmodelsandpresentanddiscussexamplesintheappendix.Notonlydoindividualattentionheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntacticandsemanticstructureofthesentences.5TrainingThissectiondescribesthetrainingregimeforourmodels.5.1TrainingDataandBatchingWetrainedonthestandardWMT2014English-Germandatasetconsistingofabout4.5millionsentencepairs.Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-targetvocabularyofabout37000tokens.ForEnglish-French,weusedthesignificantlylargerWMT2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piecevocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtrainingbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000targettokens.5.2HardwareandScheduleWetrainedourmodelsononemachinewith8NVIDIAP100GPUs.Forourbasemodelsusingthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds.Wetrainedthebasemodelsforatotalof100,000stepsor12hours.Forourbigmodels,(describedonthebottomlineoftable3),steptimewas1.0seconds.Thebigmodelsweretrainedfor300,000steps(3.5days).5.3OptimizerWeusedtheAdamoptimizer[20]withβ1=0.9,β2=0.98andϵ=10−9.Wevariedthelearningrateoverthecourseoftraining,accordingtotheformula:lrate=d−0.5model·min(step_num−0.5,step_num·warmup_steps−1.5)(3)Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber.Weusedwarmup_steps=4000.5.4RegularizationWeemploythreetypesofregularizationduringtraining:7\fTable2:TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsontheEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.ModelBLEUTrainingCost(FLOPs)EN-DEEN-FREN-DEEN-FRByteNet[18]23.75Deep-Att+PosUnk[39]39.21.0·1020GNMT+RL[38]24.639.922.3·10191.4·1020ConvS2S[9]25.1640.469.6·10181.5·1020MoE[32]26.0340.562.0·10191.2·1020Deep-Att+PosUnkEnsemble[39]40.48.0·1020GNMT+RLEnsemble[38]26.3041.161.8·10201.1·1021ConvS2SEnsemble[9]26.3641.297.7·10191.2·1021Transformer(basemodel)27.338.13.3·1018Transformer(big)28.441.82.3·1019ResidualDropoutWeapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothesub-layerinputandnormalized.Inaddition,weapplydropouttothesumsoftheembeddingsandthepositionalencodingsinboththeencoderanddecoderstacks.Forthebasemodel,weusearateofPdrop=0.1.LabelSmoothingDuringtraining,weemployedlabelsmoothingofvalueϵls=0.1[36].Thishurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.6Results6.1MachineTranslationOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0BLEU,establishinganewstate-of-the-artBLEUscoreof28.4.TheconfigurationofthismodelislistedinthebottomlineofTable3.Trainingtook3.5dayson8P100GPUs.Evenourbasemodelsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyofthecompetitivemodels.OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthepreviousstate-of-the-artmodel.TheTransformer(big)modeltrainedforEnglish-to-FrenchuseddropoutratePdrop=0.1,insteadof0.3.Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,whichwerewrittenat10-minuteintervals.Forthebigmodels,weaveragedthelast20checkpoints.Weusedbeamsearchwithabeamsizeof4andlengthpenaltyα=0.6[38].Thesehyperparameterswerechosenafterexperimentationonthedevelopmentset.Wesetthemaximumoutputlengthduringinferencetoinputlength+50,butterminateearlywhenpossible[38].Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodelarchitecturesfromtheliterature.Weestimatethenumberoffloatingpointoperationsusedtotrainamodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustainedsingle-precisionfloating-pointcapacityofeachGPU5.6.2ModelVariationsToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodelindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.8\fTable3:VariationsontheTransformerarchitecture.Unlistedvaluesareidenticaltothoseofthebasemodel.AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.Listedperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedtoper-wordperplexities.NdmodeldffhdkdvPdropϵlstrainPPLBLEUparamssteps(dev)(dev)×106base65122048864640.10.1100K4.9225.865(A)15125125.2924.941281285.0025.51632324.9125.83216165.0125.4(B)165.1625.158325.0125.460(C)26.1123.73645.1925.35084.8825.58025632325.7524.52810241281284.6626.016810245.1225.45340964.7526.290(D)0.05.7724.60.24.9525.50.04.6725.30.25.4725.7(E)positionalembeddinginsteadofsinusoids4.9225.7big610244096160.3300K4.3326.4213developmentset,newstest2013.Weusedbeamsearchasdescribedintheprevioussection,butnocheckpointaveraging.WepresenttheseresultsinTable3.InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,keepingtheamountofcomputationconstant,asdescribedinSection3.2.2.Whilesingle-headattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.InTable3rows(B),weobservethatreducingtheattentionkeysizedkhurtsmodelquality.Thissuggeststhatdeterminingcompatibilityisnoteasyandthatamoresophisticatedcompatibilityfunctionthandotproductmaybebeneficial.Wefurtherobserveinrows(C)and(D)that,asexpected,biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceoursinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidenticalresultstothebasemodel.6.3EnglishConstituencyParsingToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglishconstituencyparsing.Thistaskpresentsspecificchallenges:theoutputissubjecttostrongstructuralconstraintsandissignificantlylongerthantheinput.Furthermore,RNNsequence-to-sequencemodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].Wetraineda4-layertransformerwithdmodel=1024ontheWallStreetJournal(WSJ)portionofthePennTreebank[25],about40Ktrainingsentences.Wealsotraineditinasemi-supervisedsetting,usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences[37].Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokensforthesemi-supervisedsetting.Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparametersremainedunchangedfromtheEnglish-to-Germanbasetranslationmodel.Duringinference,we9\fTable4:TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23ofWSJ)ParserTrainingWSJ23F1Vinyals&Kaiserelal.(2014)[37]WSJonly,discriminative88.3Petrovetal.(2006)[29]WSJonly,discriminative90.4Zhuetal.(2013)[40]WSJonly,discriminative90.4Dyeretal.(2016)[8]WSJonly,discriminative91.7Transformer(4layers)WSJonly,discriminative91.3Zhuetal.(2013)[40]semi-supervised91.3Huang&Harper(2009)[14]semi-supervised91.3McCloskyetal.(2006)[26]semi-supervised92.1Vinyals&Kaiserelal.(2014)[37]semi-supervised92.1Transformer(4layers)semi-supervised92.7Luongetal.(2015)[23]multi-task93.0Dyeretal.(2016)[8]generative93.3increasedthemaximumoutputlengthtoinputlength+300.Weusedabeamsizeof21andα=0.3forbothWSJonlyandthesemi-supervisedsetting.OurresultsinTable4showthatdespitethelackoftask-specifictuningourmodelperformssur-prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionoftheRecurrentNeuralNetworkGrammar[8].IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.7ConclusionInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyonattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswithmulti-headedself-attention.Fortranslationtasks,theTransformercanbetrainedsignificantlyfasterthanarchitecturesbasedonrecurrentorconvolutionallayers.OnbothWMT2014English-to-GermanandWMT2014English-to-Frenchtranslationtasks,weachieveanewstateoftheart.Intheformertaskourbestmodeloutperformsevenallpreviouslyreportedensembles.Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.WeplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextandtoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputssuchasimages,audioandvideo.Makinggenerationlesssequentialisanotherresearchgoalsofours.Thecodeweusedtotrainandevaluateourmodelsisavailableathttps://github.com/tensorflow/tensor2tensor.AcknowledgementsWearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitfulcomments,correctionsandinspiration.References[1]JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.Layernormalization.arXivpreprintarXiv:1607.06450,2016.[2]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.CoRR,abs/1409.0473,2014.[3]DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.Massiveexplorationofneuralmachinetranslationarchitectures.CoRR,abs/1703.03906,2017.[4]JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachinereading.arXivpreprintarXiv:1601.06733,2016.10\f[5]KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.CoRR,abs/1406.1078,2014.[6]FrancoisChollet.Xception:Deeplearningwithdepthwiseseparableconvolutions.arXivpreprintarXiv:1610.02357,2016.[7]JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio.Empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling.CoRR,abs/1412.3555,2014.[8]ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahA.Smith.Recurrentneuralnetworkgrammars.InProc.ofNAACL,2016.[9]JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.Convolu-tionalsequencetosequencelearning.arXivpreprintarXiv:1705.03122v2,2017.[10]AlexGraves.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprintarXiv:1308.0850,2013.[11]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforim-agerecognition.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages770–778,2016.[12]SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber.Gradientflowinrecurrentnets:thedifficultyoflearninglong-termdependencies,2001.[13]SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.Neuralcomputation,9(8):1735–1780,1997.[14]ZhongqiangHuangandMaryHarper.Self-trainingPCFGgrammarswithlatentannotationsacrosslanguages.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages832–841.ACL,August2009.[15]RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploringthelimitsoflanguagemodeling.arXivpreprintarXiv:1602.02410,2016.[16]ŁukaszKaiserandSamyBengio.Canactivememoryreplaceattention?InAdvancesinNeuralInformationProcessingSystems,(NIPS),2016.[17]ŁukaszKaiserandIlyaSutskever.NeuralGPUslearnalgorithms.InInternationalConferenceonLearningRepresentations(ICLR),2016.[18]NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,2017.[19]YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.Structuredattentionnetworks.InInternationalConferenceonLearningRepresentations,2017.[20]DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InICLR,2015.[21]OleksiiKuchaievandBorisGinsburg.FactorizationtricksforLSTMnetworks.arXivpreprintarXiv:1703.10722,2017.[22]ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,BowenZhou,andYoshuaBengio.Astructuredself-attentivesentenceembedding.arXivpreprintarXiv:1703.03130,2017.[23]Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser.Multi-tasksequencetosequencelearning.arXivpreprintarXiv:1511.06114,2015.[24]Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproachestoattention-basedneuralmachinetranslation.arXivpreprintarXiv:1508.04025,2015.11\f[25]MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotatedcorpusofenglish:Thepenntreebank.Computationallinguistics,19(2):313–330,1993.[26]DavidMcClosky,EugeneCharniak,andMarkJohnson.Effectiveself-trainingforparsing.InProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,pages152–159.ACL,June2006.[27]AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit.Adecomposableattentionmodel.InEmpiricalMethodsinNaturalLanguageProcessing,2016.[28]RomainPaulus,CaimingXiong,andRichardSocher.Adeepreinforcedmodelforabstractivesummarization.arXivpreprintarXiv:1705.04304,2017.[29]SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.Learningaccurate,compact,andinterpretabletreeannotation.InProceedingsofthe21stInternationalConferenceonComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July2006.[30]OfirPressandLiorWolf.Usingtheoutputembeddingtoimprovelanguagemodels.arXivpreprintarXiv:1608.05859,2016.[31]RicoSennrich,BarryHaddow,andAlexandraBirch.Neuralmachinetranslationofrarewordswithsubwordunits.arXivpreprintarXiv:1508.07909,2015.[32]NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,andJeffDean.Outrageouslylargeneuralnetworks:Thesparsely-gatedmixture-of-expertslayer.arXivpreprintarXiv:1701.06538,2017.[33]NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-nov.Dropout:asimplewaytopreventneuralnetworksfromoverfitting.JournalofMachineLearningResearch,15(1):1929–1958,2014.[34]SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus.End-to-endmemorynetworks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,Inc.,2015.[35]IlyaSutskever,OriolVinyals,andQuocVVLe.Sequencetosequencelearningwithneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.[36]ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.Rethinkingtheinceptionarchitectureforcomputervision.CoRR,abs/1512.00567,2015.[37]Vinyals&Kaiser,Koo,Petrov,Sutskever,andHinton.Grammarasaforeignlanguage.InAdvancesinNeuralInformationProcessingSystems,2015.[38]YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.Google’sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.arXivpreprintarXiv:1609.08144,2016.[39]JieZhou,YingCao,XuguangWang,PengLi,andWeiXu.Deeprecurrentmodelswithfast-forwardconnectionsforneuralmachinetranslation.CoRR,abs/1606.04199,2016.[40]MuhuaZhu,YueZhang,WenliangChen,MinZhang,andJingboZhu.Fastandaccurateshift-reduceconstituentparsing.InProceedingsofthe51stAnnualMeetingoftheACL(Volume1:LongPapers),pages434–443.ACL,August2013.12\fAttentionVisualizationsInput-Input Layer5ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>Figure3:Anexampleoftheattentionmechanismfollowinglong-distancedependenciesintheencoderself-attentioninlayer5of6.Manyoftheattentionheadsattendtoadistantdependencyoftheverb‘making’,completingthephrase‘making...moredifficult’.Attentionshereshownonlyfortheword‘making’.Differentcolorsrepresentdifferentheads.Bestviewedincolor.13\fInput-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Figure4:Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution.Top:Fullattentionsforhead5.Bottom:Isolatedattentionsfromjusttheword‘its’forattentionheads5and6.Notethattheattentionsareverysharpforthisword.14\fInput-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Figure5:Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthesentence.Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attentionatlayer5of6.Theheadsclearlylearnedtoperformdifferenttasks.15\f\n"
     ]
    }
   ],
   "source": [
    "# If you want to control layout or parse metadata like positions:\n",
    "\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from io import StringIO\n",
    "\n",
    "output_string = StringIO()\n",
    "with open(pdf_path, 'rb') as fh:\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    extract_text_to_fp(fh, output_string, rsrcmgr=rsrcmgr)\n",
    "    \n",
    "print(output_string.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467f7651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "g\n",
      "u\n",
      "A\n",
      "2\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "7\n",
      "v\n",
      "2\n",
      "6\n",
      "7\n",
      "3\n",
      "0\n",
      ".\n",
      "6\n",
      "0\n",
      "7\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "2\n",
      "\n",
      "\fFigure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "3\n",
      "\n",
      "\fScaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "dk, and apply a softmax function to obtain the weights on the\n",
      "query with all keys, divide each by\n",
      "values.\n",
      "\n",
      "√\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "\n",
      "Attention(Q, K, V ) = softmax(\n",
      "\n",
      "QK T\n",
      "√\n",
      "dk\n",
      "\n",
      ")V\n",
      "\n",
      "(1)\n",
      "\n",
      "1√\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "of\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "\n",
      "dk\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√\n",
      "dk\n",
      "\n",
      ".\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\n",
      "\n",
      "4\n",
      "\n",
      "\foutput values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
      "\n",
      "where headi = Attention(QW Q\n",
      "\n",
      "i , KW K\n",
      "i\n",
      "\n",
      ", V W V\n",
      "\n",
      "i )\n",
      "\n",
      "Where the projections are parameter matrices W Q\n",
      "and W O ∈ Rhdv×dmodel.\n",
      "\n",
      "i ∈ Rdmodel×dk , W K\n",
      "\n",
      "i ∈ Rdmodel×dk , W V\n",
      "\n",
      "i ∈ Rdmodel×dv\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "\n",
      "(2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "df f = 2048.\n",
      "\n",
      "3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "dmodel.\n",
      "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by\n",
      "\n",
      "√\n",
      "\n",
      "5\n",
      "\n",
      "\fTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "Layer Type\n",
      "\n",
      "Complexity per Layer\n",
      "\n",
      "Self-Attention\n",
      "Recurrent\n",
      "Convolutional\n",
      "Self-Attention (restricted)\n",
      "\n",
      "O(n2 · d)\n",
      "O(n · d2)\n",
      "O(k · n · d2)\n",
      "O(r · n · d)\n",
      "\n",
      "Sequential Maximum Path Length\n",
      "Operations\n",
      "O(1)\n",
      "O(n)\n",
      "O(1)\n",
      "O(1)\n",
      "\n",
      "O(1)\n",
      "O(n)\n",
      "O(logk(n))\n",
      "O(n/r)\n",
      "\n",
      "3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
      "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
      "P Epos.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "\n",
      "4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "6\n",
      "\n",
      "\flength n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
      "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "\n",
      "5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "\n",
      "5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "\n",
      "5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "\n",
      "lrate = d−0.5\n",
      "\n",
      "model · min(step_num−0.5, step_num · warmup_steps−1.5)\n",
      "\n",
      "(3)\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps = 4000.\n",
      "\n",
      "5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "7\n",
      "\n",
      "\fTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "\n",
      "Model\n",
      "\n",
      "ByteNet [18]\n",
      "Deep-Att + PosUnk [39]\n",
      "GNMT + RL [38]\n",
      "ConvS2S [9]\n",
      "MoE [32]\n",
      "Deep-Att + PosUnk Ensemble [39]\n",
      "GNMT + RL Ensemble [38]\n",
      "ConvS2S Ensemble [9]\n",
      "Transformer (base model)\n",
      "Transformer (big)\n",
      "\n",
      "BLEU\n",
      "\n",
      "Training Cost (FLOPs)\n",
      "\n",
      "EN-DE EN-FR\n",
      "23.75\n",
      "\n",
      "24.6\n",
      "25.16\n",
      "26.03\n",
      "\n",
      "26.30\n",
      "26.36\n",
      "27.3\n",
      "28.4\n",
      "\n",
      "39.2\n",
      "39.92\n",
      "40.46\n",
      "40.56\n",
      "40.4\n",
      "41.16\n",
      "41.29\n",
      "38.1\n",
      "41.8\n",
      "\n",
      "EN-DE\n",
      "\n",
      "EN-FR\n",
      "\n",
      "2.3 · 1019\n",
      "9.6 · 1018\n",
      "2.0 · 1019\n",
      "\n",
      "1.8 · 1020\n",
      "7.7 · 1019\n",
      "\n",
      "1.0 · 1020\n",
      "1.4 · 1020\n",
      "1.5 · 1020\n",
      "1.2 · 1020\n",
      "8.0 · 1020\n",
      "1.1 · 1021\n",
      "1.2 · 1021\n",
      "\n",
      "3.3 · 1018\n",
      "2.3 · 1019\n",
      "\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "6 Results\n",
      "\n",
      "6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "\n",
      "6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "8\n",
      "\n",
      "\fTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "\n",
      "N dmodel\n",
      "\n",
      "dff\n",
      "\n",
      "base\n",
      "\n",
      "6\n",
      "\n",
      "512\n",
      "\n",
      "2048\n",
      "\n",
      "ϵls\n",
      "\n",
      "0.1\n",
      "\n",
      "dv\n",
      "\n",
      "Pdrop\n",
      "\n",
      "0.1\n",
      "\n",
      "64\n",
      "512\n",
      "128\n",
      "32\n",
      "16\n",
      "\n",
      "h\n",
      "\n",
      "8\n",
      "1\n",
      "4\n",
      "16\n",
      "32\n",
      "\n",
      "dk\n",
      "\n",
      "64\n",
      "512\n",
      "128\n",
      "32\n",
      "16\n",
      "16\n",
      "32\n",
      "\n",
      "2\n",
      "4\n",
      "8\n",
      "\n",
      "256\n",
      "1024\n",
      "\n",
      "32\n",
      "128\n",
      "\n",
      "32\n",
      "128\n",
      "\n",
      "1024\n",
      "4096\n",
      "\n",
      "0.0\n",
      "0.2\n",
      "\n",
      "0.0\n",
      "0.2\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "6\n",
      "\n",
      "1024\n",
      "\n",
      "4096\n",
      "\n",
      "16\n",
      "\n",
      "0.3\n",
      "\n",
      "(A)\n",
      "\n",
      "(B)\n",
      "\n",
      "(C)\n",
      "\n",
      "(D)\n",
      "\n",
      "(E)\n",
      "big\n",
      "\n",
      "PPL\n",
      "train\n",
      "steps\n",
      "(dev)\n",
      "100K 4.92\n",
      "5.29\n",
      "5.00\n",
      "4.91\n",
      "5.01\n",
      "5.16\n",
      "5.01\n",
      "6.11\n",
      "5.19\n",
      "4.88\n",
      "5.75\n",
      "4.66\n",
      "5.12\n",
      "4.75\n",
      "5.77\n",
      "4.95\n",
      "4.67\n",
      "5.47\n",
      "4.92\n",
      "300K 4.33\n",
      "\n",
      "BLEU params\n",
      "×106\n",
      "(dev)\n",
      "25.8\n",
      "65\n",
      "24.9\n",
      "25.5\n",
      "25.8\n",
      "25.4\n",
      "25.1\n",
      "25.4\n",
      "23.7\n",
      "25.3\n",
      "25.5\n",
      "24.5\n",
      "26.0\n",
      "25.4\n",
      "26.2\n",
      "24.6\n",
      "25.5\n",
      "25.3\n",
      "25.7\n",
      "25.7\n",
      "26.4\n",
      "\n",
      "58\n",
      "60\n",
      "36\n",
      "50\n",
      "80\n",
      "28\n",
      "168\n",
      "53\n",
      "90\n",
      "\n",
      "213\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "\n",
      "6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "9\n",
      "\n",
      "\fTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "\n",
      "Parser\n",
      "\n",
      "Training\n",
      "\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "WSJ only, discriminative\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "semi-supervised\n",
      "multi-task\n",
      "generative\n",
      "\n",
      "Petrov et al. (2006) [29]\n",
      "Zhu et al. (2013) [40]\n",
      "Dyer et al. (2016) [8]\n",
      "Transformer (4 layers)\n",
      "Zhu et al. (2013) [40]\n",
      "Huang & Harper (2009) [14]\n",
      "McClosky et al. (2006) [26]\n",
      "Vinyals & Kaiser el al. (2014) [37]\n",
      "Transformer (4 layers)\n",
      "Luong et al. (2015) [23]\n",
      "Dyer et al. (2016) [8]\n",
      "\n",
      "WSJ 23 F1\n",
      "88.3\n",
      "90.4\n",
      "90.4\n",
      "91.7\n",
      "91.3\n",
      "91.3\n",
      "91.3\n",
      "92.1\n",
      "92.1\n",
      "92.7\n",
      "93.0\n",
      "93.3\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "\n",
      "References\n",
      "\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "\n",
      "arXiv:1607.06450, 2016.\n",
      "\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
      "\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "\n",
      "10\n",
      "\n",
      "\f[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR, abs/1406.1078, 2014.\n",
      "\n",
      "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "\n",
      "preprint arXiv:1610.02357, 2016.\n",
      "\n",
      "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "\n",
      "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "\n",
      "network grammars. In Proc. of NAACL, 2016.\n",
      "\n",
      "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks.\n",
      "\n",
      "arXiv preprint\n",
      "\n",
      "arXiv:1308.0850, 2013.\n",
      "\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 770–778, 2016.\n",
      "\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "\n",
      "9(8):1735–1780, 1997.\n",
      "\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing, pages 832–841. ACL, August 2009.\n",
      "\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "\n",
      "on Learning Representations (ICLR), 2016.\n",
      "\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "\n",
      "In International Conference on Learning Representations, 2017.\n",
      "\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "\n",
      "arXiv:1703.10722, 2017.\n",
      "\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "\n",
      "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "\n",
      "11\n",
      "\n",
      "\f[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
      "\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      "pages 152–159. ACL, June 2006.\n",
      "\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "\n",
      "model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
      "2006.\n",
      "\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "\n",
      "preprint arXiv:1608.05859, 2016.\n",
      "\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "\n",
      "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "\n",
      "12\n",
      "\n",
      "\fAttention Visualizations\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "13\n",
      "\n",
      "Input-Input Layer5ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>\fFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "14\n",
      "\n",
      "Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>\fFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "15\n",
      "\n",
      "Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>\f\n"
     ]
    }
   ],
   "source": [
    "# You can access layout information like boxes, fonts, etc., using this lower-level approach:\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "output_string = StringIO()\n",
    "with open(pdf_path, 'rb') as in_file:\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    for page in PDFPage.get_pages(in_file):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "text = output_string.getvalue()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10d34d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 3\n",
      "Bounding box: (16.34, 568.0799999999999, 36.34, 578.0799999999999)\n",
      "Text: 2\n",
      "Bounding box: (16.34, 558.0799999999999, 36.34, 568.0799999999999)\n",
      "Text: 0\n",
      "Bounding box: (16.34, 548.0799999999999, 36.34, 558.0799999999999)\n",
      "Text: 2\n",
      "Bounding box: (16.34, 538.0799999999999, 36.34, 548.0799999999999)\n",
      "Text: g\n",
      "Bounding box: (16.34, 523.0799999999999, 36.34, 533.0799999999999)\n",
      "Text: u\n",
      "Bounding box: (16.34, 513.0799999999999, 36.34, 523.0799999999999)\n",
      "Text: A\n",
      "Bounding box: (16.34, 498.64, 36.34, 513.08)\n",
      "Text: 2\n",
      "Bounding box: (16.34, 483.64, 36.34, 493.64)\n",
      "Text: ]\n",
      "Bounding box: (16.34, 466.98, 36.34, 473.64000000000004)\n",
      "Text: L\n",
      "Bounding box: (16.34, 454.76, 36.34, 466.98)\n",
      "Text: C\n",
      "Bounding box: (16.34, 441.41999999999996, 36.34, 454.75999999999993)\n",
      "Text: .\n",
      "Bounding box: (16.34, 436.41999999999996, 36.34, 441.41999999999996)\n",
      "Text: s\n",
      "Bounding box: (16.34, 428.64, 36.34, 436.41999999999996)\n",
      "Text: c\n",
      "Bounding box: (16.34, 419.76, 36.34, 428.64)\n",
      "Text: [\n",
      "Bounding box: (16.34, 413.1, 36.34, 419.76000000000005)\n",
      "Text: 7\n",
      "Bounding box: (16.34, 393.1, 36.34, 403.1)\n",
      "Text: v\n",
      "Bounding box: (16.34, 383.1, 36.34, 393.1)\n",
      "Text: 2\n",
      "Bounding box: (16.34, 373.1, 36.34, 383.1)\n",
      "Text: 6\n",
      "Bounding box: (16.34, 363.1, 36.34, 373.1)\n",
      "Text: 7\n",
      "Bounding box: (16.34, 353.1, 36.34, 363.1)\n",
      "Text: 3\n",
      "Bounding box: (16.34, 343.1, 36.34, 353.1)\n",
      "Text: 0\n",
      "Bounding box: (16.34, 333.1, 36.34, 343.1)\n",
      "Text: .\n",
      "Bounding box: (16.34, 328.1, 36.34, 333.1)\n",
      "Text: 6\n",
      "Bounding box: (16.34, 318.1, 36.34, 328.1)\n",
      "Text: 0\n",
      "Bounding box: (16.34, 308.1, 36.34, 318.1)\n",
      "Text: 7\n",
      "Bounding box: (16.34, 298.1, 36.34, 308.1)\n",
      "Text: 1\n",
      "Bounding box: (16.34, 288.1, 36.34, 298.1)\n",
      "Text: :\n",
      "Bounding box: (16.34, 282.54, 36.34, 288.1)\n",
      "Text: v\n",
      "Bounding box: (16.34, 272.54, 36.34, 282.54)\n",
      "Text: i\n",
      "Bounding box: (16.34, 266.98, 36.34, 272.54)\n",
      "Text: X\n",
      "Bounding box: (16.34, 252.54, 36.34, 266.98)\n",
      "Text: r\n",
      "Bounding box: (16.34, 245.88, 36.34, 252.54)\n",
      "Text: a\n",
      "Bounding box: (16.34, 237.0, 36.34, 245.88)\n",
      "Text: Provided proper attribution is provided, Google hereby grants permission to\n",
      "Bounding box: (124.666, 707.4546768, 487.3389472, 719.4098768)\n",
      "Text: reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "Bounding box: (124.313, 693.5076768, 487.8945424, 705.4628768)\n",
      "Text: scholarly works.\n",
      "Bounding box: (267.594, 679.5596768, 346.49832000000004, 691.5148768)\n",
      "Text: Attention Is All You Need\n",
      "Bounding box: (211.488, 626.3589814, 399.89333760000005, 643.5743814)\n",
      "Text: Ashish Vaswani∗\n",
      "Bounding box: (132.908, 547.2228166, 203.88876513999998, 558.5408828)\n",
      "Text: Google Brain\n",
      "Bounding box: (139.379, 536.2440783999999, 193.3364416, 546.2066784)\n",
      "Text: avaswani@google.com\n",
      "Bounding box: (116.68099999999998, 525.2753028, 216.03900606, 535.2379027999999)\n",
      "Text: Noam Shazeer∗\n",
      "Bounding box: (239.06199999999998, 547.2228166, 304.84176514, 558.5408828)\n",
      "Text: Google Brain\n",
      "Bounding box: (242.932, 536.2440783999999, 296.88944159999994, 546.2066784)\n",
      "Text: noam@google.com\n",
      "Bounding box: (230.69199999999998, 525.2753028, 309.13253109999994, 535.2379027999999)\n",
      "Text: Niki Parmar∗\n",
      "Bounding box: (338.692, 547.2228166, 396.63076514, 558.5408828)\n",
      "Text: Google Research\n",
      "Bounding box: (331.45399999999995, 536.2440783999999, 399.7874733999999, 546.2066784)\n",
      "Text: nikip@google.com\n",
      "Bounding box: (323.787, 525.2753028, 407.45689983999995, 535.2379027999999)\n",
      "Text: Jakob Uszkoreit∗\n",
      "Bounding box: (424.29999999999995, 547.2228166, 497.21276514, 558.5408828)\n",
      "Text: Google Research\n",
      "Bounding box: (424.549, 536.2440783999999, 492.8824733999999, 546.2066784)\n",
      "Text: usz@google.com\n",
      "Bounding box: (422.11199999999997, 525.2753028, 495.3231623599999, 535.2379027999999)\n",
      "Text: Llion Jones∗\n",
      "Bounding box: (144.29199999999997, 497.2258166, 197.21976513999996, 508.5438828)\n",
      "Text: Google Research\n",
      "Bounding box: (134.54799999999997, 486.2470784, 202.88147339999998, 496.20967840000003)\n",
      "Text: llion@google.com\n",
      "Bounding box: (126.88199999999998, 475.2773028, 210.55189983999998, 485.2399028)\n",
      "Text: Aidan N. Gomez∗ †\n",
      "Bounding box: (248.76099999999997, 497.2258166, 330.28951098, 508.5438828)\n",
      "Text: University of Toronto\n",
      "Bounding box: (244.57499999999996, 486.2470784, 330.82122819999995, 496.20967840000003)\n",
      "Text: aidan@cs.toronto.edu\n",
      "Bounding box: (235.40699999999995, 475.2773028, 339.9943747999999, 485.2399028)\n",
      "Text: Łukasz Kaiser∗\n",
      "Bounding box: (394.12499999999994, 497.2258166, 459.92376513999994, 508.5438828)\n",
      "Text: Google Brain\n",
      "Bounding box: (398.00499999999994, 486.2470784, 451.9624415999999, 496.20967840000003)\n",
      "Text: lukaszkaiser@google.com\n",
      "Bounding box: (364.84899999999993, 475.2783028, 485.1145184199999, 485.2409028)\n",
      "Text: Illia Polosukhin∗ ‡\n",
      "Bounding box: (268.8069999999999, 447.2278166, 346.8475109799999, 458.5468828)\n",
      "Text: illia.polosukhin@gmail.com\n",
      "Bounding box: (238.0219999999999, 436.1893028, 373.97562463999986, 446.1519028)\n",
      "Text: Abstract\n",
      "Bounding box: (283.7579999999999, 394.7203632, 328.2432991999999, 406.6755632)\n",
      "Text: The dominant sequence transduction models are based on complex recurrent or\n",
      "Bounding box: (143.557, 368.88307840000004, 468.299304364, 378.8456784)\n",
      "Text: convolutional neural networks that include an encoder and a decoder. The best\n",
      "Bounding box: (143.866, 357.9740784, 468.1306973200002, 367.9366784)\n",
      "Text: performing models also connect the encoder and decoder through an attention\n",
      "Bounding box: (143.866, 347.0650784, 468.13069731999985, 357.0276784)\n",
      "Text: mechanism. We propose a new simple network architecture, the Transformer,\n",
      "Bounding box: (143.866, 336.1560784, 469.38060511600025, 346.1186784)\n",
      "Text: based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "Bounding box: (143.866, 325.2470784, 468.1360771239999, 335.20967840000003)\n",
      "Text: entirely. Experiments on two machine translation tasks show these models to\n",
      "Bounding box: (143.866, 314.33807840000003, 468.13069731999985, 324.30067840000004)\n",
      "Text: be superior in quality while being more parallelizable and requiring significantly\n",
      "Bounding box: (143.866, 303.42907840000004, 468.48102218639985, 313.39167840000005)\n",
      "Text: less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "Bounding box: (143.866, 292.52007840000005, 469.7870791959999, 302.48267840000005)\n",
      "Text: to-German translation task, improving over the existing best results, including\n",
      "Bounding box: (143.866, 281.6100784, 468.13069731999974, 291.5726784)\n",
      "Text: ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "Bounding box: (143.866, 270.70107840000003, 469.37602231999983, 280.6636784)\n",
      "Text: our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "Bounding box: (143.866, 259.79207840000004, 468.30205404000003, 269.7546784)\n",
      "Text: training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "Bounding box: (143.866, 248.8830784, 468.13069731999985, 258.8456784)\n",
      "Text: best models from the literature. We show that the Transformer generalizes well to\n",
      "Bounding box: (143.866, 237.9740784, 468.13153417840005, 247.9366784)\n",
      "Text: other tasks by applying it successfully to English constituency parsing both with\n",
      "Bounding box: (143.866, 227.0650784, 468.13886665199976, 237.0276784)\n",
      "Text: large and limited training data.\n",
      "Bounding box: (143.866, 216.15607839999998, 266.2665036000001, 226.1186784)\n",
      "Text: ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "Bounding box: (119.822, 184.20825760000002, 503.9969183039999, 194.7719456)\n",
      "Text: the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "Bounding box: (108.0, 174.24625759999998, 503.99668039679995, 183.2126576)\n",
      "Text: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "Bounding box: (108.0, 164.2832576, 503.99818675200004, 173.2496576)\n",
      "Text: attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "Bounding box: (108.0, 154.3202576, 504.31406405760015, 163.2866576)\n",
      "Text: detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "Bounding box: (108.0, 144.3582576, 503.9965190016, 153.3246576)\n",
      "Text: tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "Bounding box: (108.0, 134.39525759999998, 503.99954964479986, 143.3616576)\n",
      "Text: efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "Bounding box: (108.0, 124.4322576, 503.9981867520001, 133.3986576)\n",
      "Text: implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "Bounding box: (108.0, 114.4702576, 503.9981867519998, 123.43665759999999)\n",
      "Text: our research.\n",
      "Bounding box: (108.0, 104.5072576, 154.3024896, 113.4736576)\n",
      "Text: †Work performed while at Google Brain.\n",
      "Bounding box: (120.21000000000001, 93.5582576, 267.35134080000006, 104.1219456)\n",
      "Text: ‡Work performed while at Google Research.\n",
      "Bounding box: (120.21000000000001, 82.6092576, 280.2898560000001, 93.1729456)\n",
      "Text: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "Bounding box: (108.00000000000001, 50.186257600000005, 460.13742720000016, 59.152657600000005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 1\n",
      "Bounding box: (108.0, 707.5383632, 113.9776, 719.4935632)\n",
      "Text: Introduction\n",
      "Bounding box: (125.9328, 707.5383632, 190.8136704, 719.4935632)\n",
      "Text: Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "Bounding box: (108.0, 683.2020784, 503.9985536319999, 693.1646784000001)\n",
      "Text: in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "Bounding box: (108.0, 672.2930784, 503.99768879279975, 682.2556784000001)\n",
      "Text: transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      "Bounding box: (108.0, 661.3840783999999, 504.002497436, 671.3466784)\n",
      "Text: efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "Bounding box: (108.0, 650.4750783999999, 504.1673717959999, 660.4376784)\n",
      "Text: architectures [38, 24, 15].\n",
      "Bounding box: (108.0, 639.5660783999999, 210.36571500000002, 649.5286784)\n",
      "Text: Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "Bounding box: (108.0, 623.1770783999999, 503.99721058799946, 633.1396784)\n",
      "Text: sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "Bounding box: (108.0, 612.2680783999999, 503.9970113359999, 622.2306784)\n",
      "Text: states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "Bounding box: (108.0, 600.6640828, 504.35091412599996, 611.5408556)\n",
      "Text: sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "Bounding box: (108.0, 590.4500783999999, 504.16737179599977, 600.4126784)\n",
      "Text: sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "Bounding box: (108.0, 579.5410784, 503.9973699896, 589.5036784)\n",
      "Text: significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "Bounding box: (108.0, 568.6320784, 503.99714878, 578.5946784)\n",
      "Text: computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "Bounding box: (108.0, 557.7230784, 503.99949209799996, 567.6856784)\n",
      "Text: constraint of sequential computation, however, remains.\n",
      "Bounding box: (108.0, 546.8140784, 330.5246336000001, 556.7766784)\n",
      "Text: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "Bounding box: (107.641, 530.4250784, 505.6553182847998, 540.3876784)\n",
      "Text: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "Bounding box: (108.0, 519.5160784, 503.99701133599984, 529.4786784)\n",
      "Text: the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "Bounding box: (108.0, 508.60707840000003, 503.99916959600006, 518.5696784)\n",
      "Text: are used in conjunction with a recurrent network.\n",
      "Bounding box: (108.0, 497.69807840000004, 303.78501520000015, 507.66067840000005)\n",
      "Text: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "Bounding box: (108.0, 481.30907840000003, 503.99721058799986, 491.27167840000004)\n",
      "Text: relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "Bounding box: (108.0, 470.40007840000004, 505.7389520207997, 480.36267840000005)\n",
      "Text: The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "Bounding box: (107.691, 459.4910784, 504.0032280000001, 469.45367839999994)\n",
      "Text: translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Bounding box: (108.0, 448.5820784, 452.35726899999986, 458.54467839999995)\n",
      "Text: 2 Background\n",
      "Bounding box: (108.0, 417.2253632, 188.82910720000004, 429.1805632)\n",
      "Text: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "Bounding box: (107.691, 392.8890784, 504.0016738343998, 402.85167839999997)\n",
      "Text: [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "Bounding box: (108.0, 381.9800784, 504.0028835880001, 391.9426784)\n",
      "Text: block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "Bounding box: (108.0, 371.07107840000003, 505.2413400759996, 381.0336784)\n",
      "Text: the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "Bounding box: (108.0, 360.1610784, 504.0013948799995, 370.1236784)\n",
      "Text: in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "Bounding box: (108.0, 349.2520784, 503.9992828087997, 359.2146784)\n",
      "Text: it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "Bounding box: (108.0, 338.3430784, 503.997436504, 348.30567840000003)\n",
      "Text: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "Bounding box: (108.0, 327.43407840000003, 503.9972105879998, 337.39667840000004)\n",
      "Text: to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "Bounding box: (108.0, 316.52507840000004, 503.99721058799975, 326.48767840000005)\n",
      "Text: described in section 3.2.\n",
      "Bounding box: (108.0, 305.6160784, 204.56748180000002, 315.57867839999994)\n",
      "Text: Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "Bounding box: (108.0, 289.22707840000004, 503.99958168679973, 299.18967840000005)\n",
      "Text: of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "Bounding box: (108.0, 278.31807840000005, 503.997210588, 288.28067840000006)\n",
      "Text: used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "Bounding box: (108.0, 267.4090784, 505.24956918359993, 277.37167839999995)\n",
      "Text: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "Bounding box: (108.0, 256.5000784, 463.116877, 266.46267839999996)\n",
      "Text: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "Bounding box: (108.0, 240.1120784, 505.653592464, 250.0746784)\n",
      "Text: aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "Bounding box: (108.0, 229.20307839999998, 503.9949988907996, 239.1656784)\n",
      "Text: language modeling tasks [34].\n",
      "Bounding box: (108.0, 218.29307839999998, 228.08918040000003, 228.2556784)\n",
      "Text: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "Bounding box: (107.691, 201.90507839999998, 504.0032279999997, 211.8676784)\n",
      "Text: entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "Bounding box: (108.0, 190.9960784, 505.6571789999996, 200.9586784)\n",
      "Text: aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "Bounding box: (108.0, 180.0870784, 503.99736998960003, 190.0496784)\n",
      "Text: self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "Bounding box: (108.0, 169.1780784, 417.3486926000001, 179.1406784)\n",
      "Text: 3 Model Architecture\n",
      "Bounding box: (108.0, 137.8213632, 226.0934656, 149.77656320000003)\n",
      "Text: Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Bounding box: (108.0, 113.4840784, 505.743154821, 123.4466784)\n",
      "Text: Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "Bounding box: (108.0, 101.88008280000001, 504.002764156, 112.75685560000001)\n",
      "Text: of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "Bounding box: (108.0, 90.9710828, 504.00038308800004, 101.8478556)\n",
      "Text: sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "Bounding box: (108.0, 80.06208280000001, 504.0025778080001, 90.93885560000001)\n",
      "Text: [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "Bounding box: (108.0, 69.8480784, 490.5638399999998, 79.8106784)\n",
      "Text: 2\n",
      "Bounding box: (303.509, 39.96007839999999, 308.4903, 49.922678399999995)\n",
      "Text: Figure 1: The Transformer - model architecture.\n",
      "Bounding box: (210.011, 377.54707840000003, 401.9903020000001, 387.5096784)\n",
      "Text: The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "Bounding box: (107.691, 345.4950784, 504.35307466159963, 355.45767839999996)\n",
      "Text: connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "Bounding box: (108.0, 334.5860784, 505.2471183839997, 344.5486784)\n",
      "Text: respectively.\n",
      "Bounding box: (108.0, 323.6770784, 157.58386019999998, 333.6396784)\n",
      "Text: 3.1 Encoder and Decoder Stacks\n",
      "Bounding box: (108.0, 299.2798166, 253.00564300000005, 309.2424166)\n",
      "Text: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "Bounding box: (108.0, 279.23207840000003, 504.00004513600004, 289.41385560000003)\n",
      "Text: sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "Bounding box: (108.0, 268.32307840000004, 505.65748784059974, 278.28567840000005)\n",
      "Text: wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "Bounding box: (107.641, 257.4140784, 504.00184042399997, 267.37667839999995)\n",
      "Text: the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "Bounding box: (108.0, 246.5050784, 503.99867434400005, 256.4676784)\n",
      "Text: LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "Bounding box: (108.0, 235.59607839999998, 504.16604712000003, 245.77785559999998)\n",
      "Text: itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "Bounding box: (108.0, 224.6870784, 504.0033873999999, 234.6496784)\n",
      "Text: layers, produce outputs of dimension dmodel = 512.\n",
      "Bounding box: (108.0, 212.9286592, 311.81265, 223.9588556)\n",
      "Text: Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "Bounding box: (108.0, 190.65607839999998, 503.99623303600004, 200.83785559999998)\n",
      "Text: sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "Bounding box: (108.0, 179.7470784, 503.99721058800003, 189.7096784)\n",
      "Text: attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "Bounding box: (108.0, 168.8380784, 504.00139487999974, 178.8006784)\n",
      "Text: around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "Bounding box: (108.0, 157.92907839999998, 503.9972105879998, 167.8916784)\n",
      "Text: sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "Bounding box: (108.0, 147.0200784, 503.9972105879995, 156.9826784)\n",
      "Text: masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "Bounding box: (108.0, 136.1110784, 503.99685193439973, 146.0736784)\n",
      "Text: predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Bounding box: (108.0, 125.2020784, 456.64965, 135.3838556)\n",
      "Text: 3.2 Attention\n",
      "Bounding box: (108.0, 100.8048166, 170.814193, 110.7674166)\n",
      "Text: An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "Bounding box: (107.641, 80.75707840000001, 505.24856525199993, 90.7196784)\n",
      "Text: where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "Bounding box: (107.641, 69.8480784, 504.0037881949998, 79.8106784)\n",
      "Text: 3\n",
      "Bounding box: (303.509, 39.9600784, 308.4903, 49.9226784)\n",
      "Text: Scaled Dot-Product Attention\n",
      "Bounding box: (147.783, 711.0930784, 266.2183888, 721.0556784)\n",
      "Text: Multi-Head Attention\n",
      "Bounding box: (363.586, 711.0930784, 450.2008444, 721.0556784)\n",
      "Text: Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "Bounding box: (108.0, 507.8290784, 503.9972105879998, 517.7916784)\n",
      "Text: attention layers running in parallel.\n",
      "Bounding box: (108.0, 496.9200784, 247.73542760000007, 506.88267840000003)\n",
      "Text: of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "Bounding box: (108.0, 465.11407840000004, 503.99499889079954, 475.0766784)\n",
      "Text: query with the corresponding key.\n",
      "Bounding box: (108.0, 454.20507840000005, 243.51128520000006, 464.1676784)\n",
      "Text: 3.2.1 Scaled Dot-Product Attention\n",
      "Bounding box: (108.0, 431.2148166, 263.8848022000001, 441.1774166)\n",
      "Text: We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "Bounding box: (107.532, 412.5150784, 503.9966557799998, 422.47767839999995)\n",
      "Text: queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "Bounding box: (108.0, 400.9110828, 504.00025063640004, 411.7878556)\n",
      "Text: dk, and apply a softmax function to obtain the weights on the\n",
      "Bounding box: (258.351, 390.0020828, 503.9973009360001, 400.8788556)\n",
      "Text: query with all keys, divide each by\n",
      "Bounding box: (108.0, 390.6970784, 247.55985670020004, 400.65967839999996)\n",
      "Text: values.\n",
      "Bounding box: (107.751, 379.7880784, 135.447028, 389.75067839999997)\n",
      "Text: √\n",
      "Bounding box: (250.049, 398.6592556, 258.35083458, 408.6218556)\n",
      "Text: In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "Bounding box: (108.0, 363.3990784, 504.16996207199975, 373.36167839999996)\n",
      "Text: into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "Bounding box: (108.0, 352.4900784, 503.998232168, 362.6718556)\n",
      "Text: the matrix of outputs as:\n",
      "Bounding box: (108.0, 341.5810784, 204.85639720000003, 351.5436784)\n",
      "Text: Attention(Q, K, V ) = softmax(\n",
      "Bounding box: (219.97, 309.2952556, 354.41799616000003, 319.2578556)\n",
      "Text: QK T\n",
      "Bounding box: (355.609, 316.0342556, 377.36492024, 327.2038828)\n",
      "Text: √\n",
      "Bounding box: (358.077, 309.5942556, 366.37883458, 319.5568556)\n",
      "Text: dk\n",
      "Bounding box: (366.379, 300.9360828, 375.80018874, 311.81285560000003)\n",
      "Text: )V\n",
      "Bounding box: (380.13, 309.2952556, 389.81618457999997, 319.2578556)\n",
      "Text: (1)\n",
      "Bounding box: (493.051, 309.07607840000003, 504.6673916, 319.0386784)\n",
      "Text: 1√\n",
      "Bounding box: (120.056, 262.97108280000003, 129.5665791, 272.6518828)\n",
      "Text: The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "Bounding box: (107.691, 282.7740784, 505.652786382, 292.73667839999996)\n",
      "Text: plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "Bounding box: (108.0, 271.8650784, 504.1664950871999, 281.82767839999997)\n",
      "Text: . Additive attention computes the compatibility function using a feed-forward network with\n",
      "Bounding box: (136.299, 260.9560784, 504.0007429085999, 270.9186784)\n",
      "Text: of\n",
      "Bounding box: (108.0, 260.9560784, 116.3735354122, 270.9186784)\n",
      "Text: a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "Bounding box: (108.0, 247.78607839999998, 503.9972105879997, 257.7486784)\n",
      "Text: much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "Bounding box: (108.0, 236.8770784, 504.0002989939997, 246.8396784)\n",
      "Text: matrix multiplication code.\n",
      "Bounding box: (108.0, 225.9680784, 216.20379860000006, 235.9306784)\n",
      "Text: dk\n",
      "Bounding box: (126.601, 256.87762779999997, 134.52532353, 264.5228828)\n",
      "Text: While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "Bounding box: (107.532, 208.8840828, 503.9988162544, 219.76085559999999)\n",
      "Text: dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "Bounding box: (108.0, 197.9750828, 503.99988768900005, 208.8518556)\n",
      "Text: dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "Bounding box: (108.0, 187.0650828, 503.9982337495996, 197.9428556)\n",
      "Text: extremely small gradients 4. To counteract this effect, we scale the dot products by 1√\n",
      "Bounding box: (108.0, 176.85207839999998, 450.23857910000004, 188.5468828)\n",
      "Text: dk\n",
      "Bounding box: (447.274, 172.7736278, 455.19732353, 180.4178828)\n",
      "Text: .\n",
      "Bounding box: (456.972, 176.8520784, 459.46265, 186.81467840000002)\n",
      "Text: 3.2.2 Multi-Head Attention\n",
      "Bounding box: (108.0, 151.49081660000002, 230.58979300000004, 161.4534166)\n",
      "Text: Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "Bounding box: (108.0, 131.9426592, 505.2413755470001, 142.9728556)\n",
      "Text: we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "Bounding box: (107.641, 121.88207840000001, 503.995565807, 132.0638556)\n",
      "Text: linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "Bounding box: (108.0, 110.2780828, 503.9958454088001, 121.1548556)\n",
      "Text: queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "Bounding box: (108.0, 99.3680828, 504.00062310839996, 110.2458556)\n",
      "Text: 4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "Bounding box: (120.653, 81.4132576, 504.00192160000006, 91.8454384)\n",
      "Text: i=1 qiki, has mean 0 and variance dk.\n",
      "Bounding box: (368.583, 68.1003456, 503.87787519999995, 79.2269184)\n",
      "Text: variables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\n",
      "Bounding box: (107.776, 70.0632576, 376.19932353, 82.31156)\n",
      "Text: 4\n",
      "Bounding box: (303.509, 39.9600784, 308.4903, 49.9226784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "Bounding box: (108.0, 707.8850784, 503.9972105879998, 717.8476784000001)\n",
      "Text: depicted in Figure 2.\n",
      "Bounding box: (108.0, 696.9760784, 190.46044020000002, 706.9386784000001)\n",
      "Text: Multi-head attention allows the model to jointly attend to information from different representation\n",
      "Bounding box: (108.0, 680.5880784, 503.9970113359999, 690.5506784)\n",
      "Text: subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "Bounding box: (108.0, 669.6790784, 445.19415959999975, 679.6416784)\n",
      "Text: MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
      "Bounding box: (186.94, 633.7700828000001, 410.92153482, 646.3518828000001)\n",
      "Text: where headi = Attention(QW Q\n",
      "Bounding box: (224.49699999999996, 616.9590828000001, 358.44732786, 630.2148828000002)\n",
      "Text: i , KW K\n",
      "Bounding box: (350.78899999999993, 615.6770828000002, 390.03415061999993, 629.5408828000003)\n",
      "Text: i\n",
      "Bounding box: (381.9559999999999, 615.9910828000003, 384.7748099599999, 622.9648828000003)\n",
      "Text: , V W V\n",
      "Bounding box: (391.0209999999999, 617.8742556000002, 418.97152547999985, 629.5408828000003)\n",
      "Text: i )\n",
      "Bounding box: (412.88299999999987, 615.9910828000003, 425.0604551399999, 627.8368556000003)\n",
      "Text: Where the projections are parameter matrices W Q\n",
      "Bounding box: (107.532, 576.5400784, 302.96132786, 589.0998828)\n",
      "Text: and W O ∈ Rhdv×dmodel.\n",
      "Bounding box: (108.00000000000006, 564.3080784, 201.21365000000003, 576.4226)\n",
      "Text: i ∈ Rdmodel×dk , W K\n",
      "Bounding box: (295.304, 574.5620828000001, 377.31615062000003, 588.6546000000001)\n",
      "Text: i ∈ Rdmodel×dk , W V\n",
      "Bounding box: (369.238, 574.7460828000001, 450.16952548, 588.6546)\n",
      "Text: i ∈ Rdmodel×dv\n",
      "Bounding box: (444.081, 574.7460828000001, 502.82449486, 588.6546)\n",
      "Text: In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "Bounding box: (108.0, 547.9200784, 504.00182671200014, 558.1018556)\n",
      "Text: dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      "Bounding box: (108.0, 536.1616592, 503.9972804664001, 547.1928556)\n",
      "Text: is similar to that of single-head attention with full dimensionality.\n",
      "Bounding box: (108.0, 526.1010784, 369.1496338000002, 536.0636784000001)\n",
      "Text: 3.2.3 Applications of Attention in our Model\n",
      "Bounding box: (108.0, 501.9988166, 302.8584934, 511.9614166)\n",
      "Text: The Transformer uses multi-head attention in three different ways:\n",
      "Bounding box: (107.691, 482.87507840000006, 372.60649660000007, 492.8376784000001)\n",
      "Text: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "Bounding box: (135.397, 462.0310784, 505.24178082399993, 471.9936784)\n",
      "Text: and the memory keys and values come from the output of the encoder. This allows every\n",
      "Bounding box: (143.866, 451.12107840000004, 504.3475378480001, 461.08367840000005)\n",
      "Text: position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "Bounding box: (143.866, 440.2120784, 503.99492158359993, 450.17467839999995)\n",
      "Text: typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "Bounding box: (143.866, 429.3030784, 504.0020348799999, 439.26567839999996)\n",
      "Text: [38, 2, 9].\n",
      "Bounding box: (143.866, 418.3940784, 182.8795416, 428.35667839999996)\n",
      "Text: • The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "Bounding box: (135.39700000000002, 402.5580784, 503.99789044, 412.52067839999995)\n",
      "Text: and queries come from the same place, in this case, the output of the previous layer in the\n",
      "Bounding box: (143.866, 391.6490784, 504.0004408639998, 401.61167839999996)\n",
      "Text: encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "Bounding box: (143.866, 380.7400784, 504.00341968139986, 390.70267839999997)\n",
      "Text: encoder.\n",
      "Bounding box: (143.866, 369.8310784, 177.340336, 379.7936784)\n",
      "Text: • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "Bounding box: (135.39700000000002, 353.9950784, 503.99919553899986, 363.95767839999996)\n",
      "Text: all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "Bounding box: (143.866, 343.0860784, 504.003130766, 353.0486784)\n",
      "Text: information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "Bounding box: (143.866, 332.1770784, 504.0024533091998, 342.1396784)\n",
      "Text: inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "Bounding box: (143.866, 321.26807840000004, 503.995694816, 331.44985560000003)\n",
      "Text: of the softmax which correspond to illegal connections. See Figure 2.\n",
      "Bounding box: (143.866, 310.35907840000004, 420.9657564000001, 320.3216784)\n",
      "Text: 3.3 Position-wise Feed-Forward Networks\n",
      "Bounding box: (108.00000000000001, 284.91081660000003, 292.86600560000005, 294.87341660000004)\n",
      "Text: In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "Bounding box: (108.0, 264.4420784, 504.34271355599975, 274.40467839999997)\n",
      "Text: connected feed-forward network, which is applied to each position separately and identically. This\n",
      "Bounding box: (108.0, 253.5330784, 503.99886437959975, 263.4956784)\n",
      "Text: consists of two linear transformations with a ReLU activation in between.\n",
      "Bounding box: (108.0, 242.6240784, 401.53804640000004, 252.5866784)\n",
      "Text: FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "Bounding box: (226.901, 213.2190828, 384.6015791000001, 224.0958556)\n",
      "Text: (2)\n",
      "Bounding box: (493.0510000000001, 213.9140784, 504.6673916000001, 223.8766784)\n",
      "Text: While the linear transformations are the same across different positions, they use different parameters\n",
      "Bounding box: (107.532, 192.1280784, 504.0020355839999, 202.0906784)\n",
      "Text: from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "Bounding box: (108.0, 181.2190784, 505.7450491319996, 191.1816784)\n",
      "Text: The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "Bounding box: (107.691, 169.46165919999999, 504.34890426000004, 180.49185559999998)\n",
      "Text: df f = 2048.\n",
      "Bounding box: (108.0, 158.70508279999999, 158.71564999999998, 169.5828556)\n",
      "Text: 3.4 Embeddings and Softmax\n",
      "Bounding box: (108.0, 133.9528166, 240.02437520000004, 143.9154166)\n",
      "Text: Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "Bounding box: (108.0, 113.4840784, 503.9972105879998, 123.4466784)\n",
      "Text: tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "Bounding box: (108.0, 101.7266592, 505.64902170000016, 112.75685560000001)\n",
      "Text: mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "Bounding box: (108.0, 91.6660784, 503.99701133599973, 101.6286784)\n",
      "Text: our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "Bounding box: (108.0, 80.75707840000001, 504.2503104409998, 90.7196784)\n",
      "Text: dmodel.\n",
      "Bounding box: (480.185, 68.9996592, 505.743837, 80.0298556)\n",
      "Text: linear transformation, similar to [30]. In the embedding layers, we multiply those weights by\n",
      "Bounding box: (108.0, 69.8480784, 469.5317701720001, 79.8106784)\n",
      "Text: √\n",
      "Bounding box: (471.882, 77.7752556, 480.18383458, 87.7378556)\n",
      "Text: 5\n",
      "Bounding box: (303.509, 39.9600784, 308.4903, 49.9226784)\n",
      "Text: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "Bounding box: (107.691, 711.0930784, 503.9950586679997, 721.0556784)\n",
      "Text: for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "Bounding box: (108.0, 700.1840784, 504.003525788, 710.3658556)\n",
      "Text: size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "Bounding box: (108.0, 689.2750784, 435.0811038000001, 699.4568556)\n",
      "Text: Layer Type\n",
      "Bounding box: (124.547, 665.6540784, 169.9465682, 675.6166784000001)\n",
      "Text: Complexity per Layer\n",
      "Bounding box: (239.7046934, 665.6540784, 327.5449376, 675.6166784000001)\n",
      "Text: Self-Attention\n",
      "Bounding box: (124.547, 642.1080784, 181.55299720000002, 652.0706784)\n",
      "Text: Recurrent\n",
      "Bounding box: (124.54699999999991, 630.7250783999999, 163.8295317999999, 640.6876784)\n",
      "Text: Convolutional\n",
      "Bounding box: (124.54699999999991, 619.3430784, 180.9652037999999, 629.3056784)\n",
      "Text: Self-Attention (restricted)\n",
      "Bounding box: (124.54700000000003, 608.4340784, 227.7495734000001, 618.3966784)\n",
      "Text: O(n2 · d)\n",
      "Bounding box: (264.39599999999996, 642.3272556, 302.8514551399999, 653.4958828)\n",
      "Text: O(n · d2)\n",
      "Bounding box: (264.3959999999999, 630.9442555999999, 302.8514551399999, 642.1138827999999)\n",
      "Text: O(k · n · d2)\n",
      "Bounding box: (258.0489999999999, 619.5622556, 309.19845513999996, 630.7308828)\n",
      "Text: O(r · n · d)\n",
      "Bounding box: (260.648, 608.6532556, 306.59945514000003, 618.6158556)\n",
      "Text: Sequential Maximum Path Length\n",
      "Bounding box: (340.32695340000004, 665.6540784, 487.45463019999994, 675.6166784000001)\n",
      "Text: Operations\n",
      "Bounding box: (339.499, 654.7450784, 383.2148888, 664.7076784000001)\n",
      "Text: O(1)\n",
      "Bounding box: (351.0539999999999, 642.3272556, 371.6602102799999, 652.2898556)\n",
      "Text: O(n)\n",
      "Bounding box: (350.5539999999999, 630.9442555999999, 372.1594551399999, 640.9068556)\n",
      "Text: O(1)\n",
      "Bounding box: (351.054, 619.5622556, 371.66021027999994, 629.5248556)\n",
      "Text: O(1)\n",
      "Bounding box: (351.05400000000003, 608.6532556, 371.66021028, 618.6158556)\n",
      "Text: O(1)\n",
      "Bounding box: (431.0079999999999, 642.3272556, 451.6142102799999, 652.2898556)\n",
      "Text: O(n)\n",
      "Bounding box: (430.5089999999999, 630.9442555999999, 452.1134551399999, 640.9068556)\n",
      "Text: O(logk(n))\n",
      "Bounding box: (417.80899999999997, 618.6470828, 464.81391028, 629.5248556)\n",
      "Text: O(n/r)\n",
      "Bounding box: (425.63300000000004, 608.6532556, 456.99045514000005, 618.6158556)\n",
      "Text: 3.5 Positional Encoding\n",
      "Bounding box: (108.0, 568.4808166, 215.19757600000003, 578.4434166)\n",
      "Text: Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "Bounding box: (108.0, 548.4360783999999, 504.0013948799996, 558.3986784)\n",
      "Text: order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "Bounding box: (108.0, 537.5270783999999, 503.9984060999996, 547.4896784)\n",
      "Text: tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "Bounding box: (108.0, 526.6180784, 503.9972105879996, 536.5806784)\n",
      "Text: bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "Bounding box: (108.0, 514.8596592, 503.50149999999996, 525.8908556)\n",
      "Text: as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "Bounding box: (108.0, 504.8000784, 505.24786557899967, 514.7626784)\n",
      "Text: learned and fixed [9].\n",
      "Bounding box: (108.0, 493.8910784, 193.0606788, 503.85367840000004)\n",
      "Text: In this work, we use sine and cosine functions of different frequencies:\n",
      "Bounding box: (108.0, 477.5020784, 389.86187920000003, 487.4646784)\n",
      "Text: P E(pos,2i) = sin(pos/100002i/dmodel)\n",
      "Bounding box: (235.50900000000001, 443.1180828, 386.30645514, 456.0038828)\n",
      "Text: P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
      "Bounding box: (225.69400000000002, 426.24708280000004, 386.3064551400001, 439.1328828)\n",
      "Text: where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
      "Bounding box: (107.641, 404.7710784, 503.99713772240005, 414.9528556)\n",
      "Text: corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
      "Bounding box: (108.0, 393.86207840000003, 503.99946076, 404.04385560000003)\n",
      "Text: chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "Bounding box: (108.0, 382.95307840000004, 504.3427135559997, 392.91567840000005)\n",
      "Text: relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
      "Bounding box: (108.0, 371.3490828, 504.00100873200006, 382.22585560000005)\n",
      "Text: P Epos.\n",
      "Bounding box: (108.0, 360.44008279999997, 137.92264999999998, 371.3168556)\n",
      "Text: We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "Bounding box: (107.532, 344.74607840000004, 503.99586547819996, 354.70867840000005)\n",
      "Text: versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "Bounding box: (107.751, 333.8370784, 504.0022568879998, 343.79967839999995)\n",
      "Text: because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "Bounding box: (108.0, 322.9280784, 503.9973699895998, 332.89067839999996)\n",
      "Text: during training.\n",
      "Bounding box: (108.0, 312.0190784, 169.98729719999997, 321.98167839999996)\n",
      "Text: 4 Why Self-Attention\n",
      "Bounding box: (108.0, 283.4383632, 225.041408, 295.3935632)\n",
      "Text: In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "Bounding box: (108.0, 260.8320784, 505.6535924639997, 270.79467839999995)\n",
      "Text: tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "Bounding box: (108.0, 249.92307839999998, 504.0030685967998, 259.88567839999996)\n",
      "Text: (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "Bounding box: (106.834, 238.31808279999998, 503.997884064, 251.1286)\n",
      "Text: layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "Bounding box: (108.0, 228.1050784, 503.9968519343998, 238.0676784)\n",
      "Text: consider three desiderata.\n",
      "Bounding box: (108.0, 217.1950784, 209.5288566, 227.1576784)\n",
      "Text: One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "Bounding box: (108.0, 200.8070784, 504.0004882833997, 210.7696784)\n",
      "Text: be parallelized, as measured by the minimum number of sequential operations required.\n",
      "Bounding box: (108.0, 189.8980784, 457.4581201999999, 199.8606784)\n",
      "Text: The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "Bounding box: (107.691, 173.5090784, 504.00140484419984, 183.4716784)\n",
      "Text: dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "Bounding box: (108.0, 162.6000784, 503.9966526823998, 172.5626784)\n",
      "Text: ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "Bounding box: (108.0, 151.69107839999998, 503.9972105879999, 161.6536784)\n",
      "Text: traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "Bounding box: (108.0, 140.7820784, 503.99755927899946, 150.7446784)\n",
      "Text: and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
      "Bounding box: (108.0, 129.8730784, 503.9994911184, 139.8356784)\n",
      "Text: the maximum path length between any two input and output positions in networks composed of the\n",
      "Bounding box: (108.0, 118.9640784, 504.00048828339965, 128.92667840000001)\n",
      "Text: different layer types.\n",
      "Bounding box: (108.0, 108.0550784, 190.47040280000002, 118.0176784)\n",
      "Text: As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "Bounding box: (107.641, 91.6660784, 504.34535593599963, 101.6286784)\n",
      "Text: executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "Bounding box: (108.0, 80.75707840000001, 504.00030908400004, 90.93885560000001)\n",
      "Text: computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "Bounding box: (108.0, 69.8480784, 503.9972105879998, 79.8106784)\n",
      "Text: 6\n",
      "Bounding box: (303.509, 39.9600784, 308.4903, 49.9226784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "Bounding box: (108.0, 707.8850784, 504.00358807199996, 718.0668556)\n",
      "Text: sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "Bounding box: (108.0, 696.9760784, 504.0034870259998, 706.9386784000001)\n",
      "Text: [38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "Bounding box: (108.0, 686.0670784, 504.00255743679986, 696.0296784000001)\n",
      "Text: very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "Bounding box: (107.751, 675.1580783999999, 503.999884744, 685.3398556)\n",
      "Text: the input sequence centered around the respective output position. This would increase the maximum\n",
      "Bounding box: (108.0, 664.2490783999999, 504.00139487999974, 674.2116784)\n",
      "Text: path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "Bounding box: (108.0, 653.3400783999999, 437.5675558000001, 663.5218556)\n",
      "Text: A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "Bounding box: (107.641, 636.9510783999999, 503.99988086800005, 647.1328556)\n",
      "Text: positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "Bounding box: (108.0, 626.0420783999999, 505.2424466680001, 636.2238556)\n",
      "Text: or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
      "Bounding box: (108.0, 614.4380828, 504.0038955560001, 625.3148556)\n",
      "Text: between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "Bounding box: (108.0, 604.2240783999999, 503.99890422999994, 614.1866784)\n",
      "Text: recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
      "Bounding box: (108.0, 593.3150784, 504.350698084, 603.4968556)\n",
      "Text: considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      "Bounding box: (108.0, 582.4060784, 504.00195649600005, 593.7938828)\n",
      "Text: convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "Bounding box: (108.0, 571.4970784, 505.24786557899995, 581.4596784)\n",
      "Text: the approach we take in our model.\n",
      "Bounding box: (108.0, 560.5880784, 248.17378200000005, 570.5506784)\n",
      "Text: As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "Bounding box: (107.641, 544.1990784, 504.00363875599965, 554.1616784)\n",
      "Text: from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "Bounding box: (108.0, 533.2900784, 503.99840609999967, 543.2526784)\n",
      "Text: heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "Bounding box: (108.0, 522.3810784, 504.0013948799997, 532.3436784)\n",
      "Text: and semantic structure of the sentences.\n",
      "Bounding box: (108.0, 511.47207840000004, 266.2559010000001, 521.4346784)\n",
      "Text: 5 Training\n",
      "Bounding box: (108.0, 480.41136320000004, 170.22681599999999, 492.36656320000003)\n",
      "Text: This section describes the training regime for our models.\n",
      "Bounding box: (107.691, 456.25307840000005, 337.4783690000001, 466.2156784)\n",
      "Text: 5.1 Training Data and Batching\n",
      "Bounding box: (108.0, 429.5438166, 249.52869560000005, 439.5064166)\n",
      "Text: We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "Bounding box: (107.532, 408.57107840000003, 503.9966557799995, 418.5336784)\n",
      "Text: sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
      "Bounding box: (108.0, 397.66207840000004, 505.6543158080001, 407.6246784)\n",
      "Text: target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "Bounding box: (108.0, 386.7530784, 504.3042579199998, 396.7156784)\n",
      "Text: 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "Bounding box: (108.0, 375.8430784, 504.0009465629997, 385.80567840000003)\n",
      "Text: vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "Bounding box: (107.751, 364.93407840000003, 504.0001842359998, 374.89667840000004)\n",
      "Text: batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "Bounding box: (108.0, 354.02507840000004, 503.9972105879997, 363.98767840000005)\n",
      "Text: target tokens.\n",
      "Bounding box: (108.0, 343.1160784, 161.39953599999998, 353.07867839999994)\n",
      "Text: 5.2 Hardware and Schedule\n",
      "Bounding box: (108.0, 316.40781659999993, 233.04059260000002, 326.37041659999994)\n",
      "Text: We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "Bounding box: (107.532, 295.43407840000003, 503.99665577999997, 305.39667840000004)\n",
      "Text: the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "Bounding box: (108.0, 284.52507840000004, 503.99701133599973, 294.48767840000005)\n",
      "Text: trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "Bounding box: (108.0, 273.6160784, 504.00139487999974, 283.57867839999994)\n",
      "Text: bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "Bounding box: (108.0, 262.7070784, 503.9972105879998, 272.66967839999995)\n",
      "Text: (3.5 days).\n",
      "Bounding box: (107.671, 251.79807839999998, 150.0020874, 261.76067839999996)\n",
      "Text: 5.3 Optimizer\n",
      "Bounding box: (108.0, 225.0888166, 174.1317388, 235.05141659999998)\n",
      "Text: We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "Bounding box: (107.532, 203.4210828, 504.00292131680004, 215.5038828)\n",
      "Text: rate over the course of training, according to the formula:\n",
      "Bounding box: (108.0, 193.2070784, 336.2332034000001, 203.1696784)\n",
      "Text: lrate = d−0.5\n",
      "Bounding box: (162.892, 163.03525560000003, 219.33957910000004, 174.84988280000002)\n",
      "Text: model · min(step_num−0.5, step_num · warmup_steps−1.5)\n",
      "Bounding box: (202.80400000000003, 160.57065920000002, 449.10745514, 174.70188280000002)\n",
      "Text: (3)\n",
      "Bounding box: (493.051, 162.8160784, 504.6673916, 172.77867840000002)\n",
      "Text: This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "Bounding box: (107.691, 139.3480784, 505.2438336688, 149.5298556)\n",
      "Text: and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "Bounding box: (108.0, 128.4390784, 503.9972105879999, 138.4016784)\n",
      "Text: warmup_steps = 4000.\n",
      "Bounding box: (108.0, 117.53007840000001, 208.17065, 127.7118556)\n",
      "Text: 5.4 Regularization\n",
      "Bounding box: (108.0, 90.8208166, 193.5089958, 100.78341660000001)\n",
      "Text: We employ three types of regularization during training:\n",
      "Bounding box: (107.532, 69.8480784, 331.9893780000001, 79.8106784)\n",
      "Text: 7\n",
      "Bounding box: (303.509, 39.96007839999999, 308.4903, 49.922678399999995)\n",
      "Text: Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "Bounding box: (107.691, 711.0930784, 504.0032279999998, 721.0556784)\n",
      "Text: English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Bounding box: (108.0, 700.1840784, 483.51031919999986, 710.1466784)\n",
      "Text: Model\n",
      "Bounding box: (136.671, 676.0900783999999, 162.6833486, 686.0526784)\n",
      "Text: ByteNet [18]\n",
      "Bounding box: (136.671, 657.1570783999999, 188.9646874, 667.1196784)\n",
      "Text: Deep-Att + PosUnk [39]\n",
      "Bounding box: (136.671, 645.7740783999999, 234.98193680000003, 655.7366784)\n",
      "Text: GNMT + RL [38]\n",
      "Bounding box: (136.67100000000005, 634.3920783999998, 208.42164520000006, 644.3546783999999)\n",
      "Text: ConvS2S [9]\n",
      "Bounding box: (136.67100000000005, 623.0090783999998, 188.02820300000005, 632.9716783999999)\n",
      "Text: MoE [32]\n",
      "Bounding box: (136.67100000000005, 611.6270783999997, 175.68454160000005, 621.5896783999998)\n",
      "Text: Deep-Att + PosUnk Ensemble [39]\n",
      "Bounding box: (136.671, 598.9890783999999, 276.76508120000005, 608.9516784)\n",
      "Text: GNMT + RL Ensemble [38]\n",
      "Bounding box: (136.67100000000005, 587.6070783999999, 250.2047896000001, 597.5696783999999)\n",
      "Text: ConvS2S Ensemble [9]\n",
      "Bounding box: (136.67100000000005, 576.2250783999999, 229.8113474000001, 586.1876784)\n",
      "Text: Transformer (base model)\n",
      "Bounding box: (136.671, 563.0890784, 240.34181560000002, 573.0516784)\n",
      "Text: Transformer (big)\n",
      "Bounding box: (136.671, 551.7070784, 207.9733282, 561.6696784000001)\n",
      "Text: BLEU\n",
      "Bounding box: (311.02099999999996, 684.3790783999999, 337.03334859999995, 694.3416784)\n",
      "Text: Training Cost (FLOPs)\n",
      "Bounding box: (383.2498499999999, 684.3790783999999, 475.3241992, 694.3416784)\n",
      "Text: EN-DE EN-FR\n",
      "Bounding box: (288.72, 668.4640784, 359.33490880000005, 678.4266784)\n",
      "Text: 23.75\n",
      "Bounding box: (292.44621359999996, 657.1570783999999, 314.8620636, 667.1196784)\n",
      "Text: 24.6\n",
      "Bounding box: (294.93686360000004, 634.3920783999998, 312.37141360000004, 644.3546783999999)\n",
      "Text: 25.16\n",
      "Bounding box: (292.4462136000001, 623.0090783999998, 314.8620636, 632.9716783999999)\n",
      "Text: 26.03\n",
      "Bounding box: (292.4462136000001, 611.6270783999997, 314.8620636, 621.5896783999998)\n",
      "Text: 26.30\n",
      "Bounding box: (292.4462136000001, 587.6070783999999, 314.8620636000001, 597.5696783999999)\n",
      "Text: 26.36\n",
      "Bounding box: (292.4462136000001, 576.2250783999999, 314.86206360000006, 586.1876784)\n",
      "Text: 27.3\n",
      "Bounding box: (294.93686360000004, 563.0890784, 312.37141360000004, 573.0516784)\n",
      "Text: 28.4\n",
      "Bounding box: (294.94100000000003, 551.7768166000001, 312.37555000000003, 561.7394166)\n",
      "Text: 39.2\n",
      "Bounding box: (336.22187800000006, 645.7740783999999, 353.656428, 655.7366784)\n",
      "Text: 39.92\n",
      "Bounding box: (333.7312280000001, 634.3920783999998, 356.147078, 644.3546783999999)\n",
      "Text: 40.46\n",
      "Bounding box: (333.73122800000004, 623.0090783999998, 356.147078, 632.9716783999999)\n",
      "Text: 40.56\n",
      "Bounding box: (333.73122800000004, 611.6270783999997, 356.147078, 621.5896783999998)\n",
      "Text: 40.4\n",
      "Bounding box: (336.22187800000006, 598.9890783999999, 353.65642800000006, 608.9516784)\n",
      "Text: 41.16\n",
      "Bounding box: (333.7312280000001, 587.6070783999999, 356.1470780000001, 597.5696783999999)\n",
      "Text: 41.29\n",
      "Bounding box: (333.73600000000005, 576.2948166, 356.15185, 586.2574165999999)\n",
      "Text: 38.1\n",
      "Bounding box: (336.22187800000006, 563.0890784, 353.656428, 573.0516784)\n",
      "Text: 41.8\n",
      "Bounding box: (336.22601440000005, 551.7768166000001, 353.6605644, 561.7394166)\n",
      "Text: EN-DE\n",
      "Bounding box: (387.46929120000004, 668.4640784, 417.3471286, 678.4266784)\n",
      "Text: EN-FR\n",
      "Bounding box: (440.0419314, 668.4640784, 468.82388280000004, 678.4266784)\n",
      "Text: 2.3 · 1019\n",
      "Bounding box: (383.24500000000006, 634.6112555999998, 421.0761582, 645.7798827999999)\n",
      "Text: 9.6 · 1018\n",
      "Bounding box: (383.24500000000006, 623.2282555999998, 421.0761582, 634.3978827999998)\n",
      "Text: 2.0 · 1019\n",
      "Bounding box: (383.24500000000006, 611.8462555999997, 421.0761582, 623.0148827999998)\n",
      "Text: 1.8 · 1020\n",
      "Bounding box: (383.24500000000006, 587.8262555999999, 421.0761582, 598.9948827999999)\n",
      "Text: 7.7 · 1019\n",
      "Bounding box: (383.24500000000006, 576.4442555999999, 421.0761582, 587.6128828)\n",
      "Text: 1.0 · 1020\n",
      "Bounding box: (435.264, 645.9932555999999, 473.0951582, 657.1628827999999)\n",
      "Text: 1.4 · 1020\n",
      "Bounding box: (435.264, 634.6112555999998, 473.0951582, 645.7798827999999)\n",
      "Text: 1.5 · 1020\n",
      "Bounding box: (435.264, 623.2282555999998, 473.0951582, 634.3978827999998)\n",
      "Text: 1.2 · 1020\n",
      "Bounding box: (435.26500000000004, 611.8462555999997, 473.0951582, 623.0148827999998)\n",
      "Text: 8.0 · 1020\n",
      "Bounding box: (435.264, 599.2082555999999, 473.0951582, 610.3778828)\n",
      "Text: 1.1 · 1021\n",
      "Bounding box: (435.264, 587.8262555999999, 473.0951582, 598.9948827999999)\n",
      "Text: 1.2 · 1021\n",
      "Bounding box: (435.264, 576.4442555999999, 473.0951582, 587.6128828)\n",
      "Text: 3.3 · 1018\n",
      "Bounding box: (407.339, 563.3082555999999, 450.73688672, 574.4768828)\n",
      "Text: 2.3 · 1019\n",
      "Bounding box: (410.12300000000005, 551.9262556, 447.95415820000005, 563.0948828)\n",
      "Text: Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
      "Bounding box: (108.0, 508.9910784, 504.00125174320016, 519.0234166)\n",
      "Text: sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "Bounding box: (108.0, 498.0820784, 504.0013948799996, 508.04467839999995)\n",
      "Text: positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Bounding box: (108.0, 487.1730784, 503.997210588, 497.13567839999996)\n",
      "Text: Pdrop = 0.1.\n",
      "Bounding box: (108.0, 475.5690828, 159.49865, 486.4458556)\n",
      "Text: Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\n",
      "Bounding box: (108.0, 450.2240828, 503.99579818399997, 461.1018556)\n",
      "Text: hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "Bounding box: (108.0, 440.01107840000003, 491.6995764, 449.97367840000004)\n",
      "Text: 6 Results\n",
      "Bounding box: (108.0, 409.0393632, 163.1254272, 420.9945632)\n",
      "Text: 6.1 Machine Translation\n",
      "Bounding box: (108.0, 385.0038166, 219.07302740000006, 394.9664166)\n",
      "Text: On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "Bounding box: (108.0, 364.0670784, 504.6653025439999, 374.02967839999997)\n",
      "Text: in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "Bounding box: (108.0, 353.1580784, 504.24929999999995, 363.3398556)\n",
      "Text: BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "Bounding box: (108.0, 342.24907840000003, 503.99929678200004, 352.43085560000003)\n",
      "Text: listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "Bounding box: (108.0, 331.34007840000004, 503.99791379040005, 341.52185560000004)\n",
      "Text: surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "Bounding box: (108.0, 320.43107840000005, 504.0033873999998, 330.3936784)\n",
      "Text: the competitive models.\n",
      "Bounding box: (108.0, 309.5210784, 203.07309180000001, 319.48367840000003)\n",
      "Text: On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "Bounding box: (108.0, 293.13307840000004, 505.24532765000004, 303.31485560000004)\n",
      "Text: outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "Bounding box: (108.0, 282.2240784, 503.99659189920004, 292.4058556)\n",
      "Text: previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "Bounding box: (108.0, 271.3150784, 503.9972105879998, 281.2776784)\n",
      "Text: dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "Bounding box: (108.0, 259.7100828, 269.62264999999996, 270.5878556)\n",
      "Text: For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "Bounding box: (108.0, 244.0170784, 503.99721058799975, 253.9796784)\n",
      "Text: were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "Bounding box: (107.641, 233.10807839999998, 504.0040372599996, 243.0706784)\n",
      "Text: used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "Bounding box: (108.0, 222.1990784, 504.00079226, 232.3808556)\n",
      "Text: were chosen after experimentation on the development set. We set the maximum output length during\n",
      "Bounding box: (107.641, 211.2900784, 504.0036387559996, 221.2526784)\n",
      "Text: inference to input length + 50, but terminate early when possible [38].\n",
      "Bounding box: (108.0, 200.38107839999998, 387.07221440000006, 210.56285559999998)\n",
      "Text: Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "Bounding box: (107.691, 183.9920784, 503.99505866799956, 193.9546784)\n",
      "Text: architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "Bounding box: (108.0, 173.0830784, 503.99736998959963, 183.0456784)\n",
      "Text: model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "Bounding box: (108.0, 162.17407839999998, 503.99701133599984, 172.1366784)\n",
      "Text: single-precision floating-point capacity of each GPU 5.\n",
      "Bounding box: (108.0, 151.2650784, 326.97665, 162.49945920000002)\n",
      "Text: 6.2 Model Variations\n",
      "Bounding box: (108.0, 124.6458166, 203.93983800000004, 134.6084166)\n",
      "Text: To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "Bounding box: (107.691, 103.7080784, 504.0004583971999, 113.6706784)\n",
      "Text: in different ways, measuring the change in performance on English-to-German translation on the\n",
      "Bounding box: (108.0, 92.7990784, 503.9972105879998, 102.7616784)\n",
      "Text: 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "Bounding box: (120.653, 70.0632576, 453.5559904000002, 80.4954384)\n",
      "Text: 8\n",
      "Bounding box: (303.509, 39.96007839999999, 308.4903, 49.922678399999995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "Bounding box: (107.691, 711.0930784, 503.9950985183997, 721.0556784)\n",
      "Text: model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "Bounding box: (108.0, 700.1840784, 503.99709103679976, 710.1466784)\n",
      "Text: perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "Bounding box: (108.0, 689.2750784, 504.00338739999984, 699.2376784)\n",
      "Text: per-word perplexities.\n",
      "Bounding box: (108.0, 678.3660784, 195.5314036, 688.3286784000001)\n",
      "Text: N dmodel\n",
      "Bounding box: (146.127, 644.5066592000001, 189.79250000000002, 655.5378556000001)\n",
      "Text: dff\n",
      "Bounding box: (207.132, 644.5066592000001, 216.7872058, 655.5378556000001)\n",
      "Text: base\n",
      "Bounding box: (116.468, 626.7900784, 134.1715402, 636.7526784)\n",
      "Text: 6\n",
      "Bounding box: (148.182, 626.7900784, 153.1633, 636.7526784)\n",
      "Text: 512\n",
      "Bounding box: (171.2553816, 626.7900784, 186.19928159999998, 636.7526784)\n",
      "Text: 2048\n",
      "Bounding box: (202.2490302, 626.7900784, 222.17423019999998, 636.7526784)\n",
      "Text: ϵls\n",
      "Bounding box: (345.588, 644.6600828, 355.9537497, 655.5378556000001)\n",
      "Text: 0.1\n",
      "Bounding box: (344.793911, 626.7900784, 357.24716099999995, 636.7526784)\n",
      "Text: dv\n",
      "Bounding box: (285.456, 644.6600828, 294.62582932000004, 655.5378556000001)\n",
      "Text: Pdrop\n",
      "Bounding box: (309.843, 644.6600828, 332.34080682, 655.5378556000001)\n",
      "Text: 0.1\n",
      "Bounding box: (315.1153256, 626.7900784, 327.5685756, 636.7526784)\n",
      "Text: 64\n",
      "Bounding box: (285.43674020000003, 626.7900784, 295.3993402, 636.7526784)\n",
      "Text: 512\n",
      "Bounding box: (282.94309, 614.1530783999999, 297.88698999999997, 624.1156784)\n",
      "Text: 128\n",
      "Bounding box: (282.94309, 603.2430784, 297.88698999999997, 613.2056784)\n",
      "Text: 32\n",
      "Bounding box: (285.43439, 592.3340784, 295.39698999999996, 602.2966784)\n",
      "Text: 16\n",
      "Bounding box: (285.43439, 581.4250784, 295.39698999999996, 591.3876784)\n",
      "Text: h\n",
      "Bounding box: (236.238, 645.5752556, 241.97845012, 655.5378556000001)\n",
      "Text: 8\n",
      "Bounding box: (236.6200002, 626.7900784, 241.6013002, 636.7526784)\n",
      "Text: 1\n",
      "Bounding box: (236.617, 614.1530783999999, 241.5983, 624.1156784)\n",
      "Text: 4\n",
      "Bounding box: (236.617, 603.2430784, 241.5983, 613.2056784)\n",
      "Text: 16\n",
      "Bounding box: (234.127, 592.3340784, 244.08960000000002, 602.2966784)\n",
      "Text: 32\n",
      "Bounding box: (234.127, 581.4250784, 244.08960000000002, 591.3876784)\n",
      "Text: dk\n",
      "Bounding box: (258.47651572, 644.6600828, 267.89318874, 655.5378556000001)\n",
      "Text: 64\n",
      "Bounding box: (258.53772019999997, 626.7900784, 268.5003202, 636.7526784)\n",
      "Text: 512\n",
      "Bounding box: (256.04407, 614.1530783999999, 270.98796999999996, 624.1156784)\n",
      "Text: 128\n",
      "Bounding box: (256.04407, 603.2430784, 270.98796999999996, 613.2056784)\n",
      "Text: 32\n",
      "Bounding box: (258.53537, 592.3340784, 268.49797, 602.2966784)\n",
      "Text: 16\n",
      "Bounding box: (258.53537, 581.4250784, 268.49797, 591.3876784)\n",
      "Text: 16\n",
      "Bounding box: (258.535, 568.7880784, 268.4976, 578.7506784000001)\n",
      "Text: 32\n",
      "Bounding box: (258.535, 557.8790783999999, 268.4976, 567.8416784)\n",
      "Text: 2\n",
      "Bounding box: (148.182, 545.2410784, 153.1633, 555.2036784000001)\n",
      "Text: 4\n",
      "Bounding box: (148.182, 534.3320784, 153.1633, 544.2946784000001)\n",
      "Text: 8\n",
      "Bounding box: (148.182, 523.4230784, 153.1633, 533.3856784000001)\n",
      "Text: 256\n",
      "Bounding box: (171.26, 512.5140784, 186.2039, 522.4766784000001)\n",
      "Text: 1024\n",
      "Bounding box: (168.769, 501.6050784, 188.6942, 511.5676784)\n",
      "Text: 32\n",
      "Bounding box: (258.532376, 512.5140784, 268.494976, 522.4766784000001)\n",
      "Text: 128\n",
      "Bounding box: (256.041376, 501.6050784, 270.985276, 511.5676784)\n",
      "Text: 32\n",
      "Bounding box: (285.431396, 512.5140784, 295.39399599999996, 522.4766784000001)\n",
      "Text: 128\n",
      "Bounding box: (282.940396, 501.6050784, 297.884296, 511.5676784)\n",
      "Text: 1024\n",
      "Bounding box: (202.246, 490.69607840000003, 222.17120000000003, 500.6586784)\n",
      "Text: 4096\n",
      "Bounding box: (202.246, 479.78707840000004, 222.17120000000003, 489.7496784)\n",
      "Text: 0.0\n",
      "Bounding box: (315.113, 467.1490784, 327.56624999999997, 477.11167839999996)\n",
      "Text: 0.2\n",
      "Bounding box: (315.113, 456.2400784, 327.56624999999997, 466.20267839999997)\n",
      "Text: 0.0\n",
      "Bounding box: (344.792, 445.3310784, 357.24524999999994, 455.2936784)\n",
      "Text: 0.2\n",
      "Bounding box: (344.792, 434.42207840000003, 357.24524999999994, 444.3846784)\n",
      "Text: positional embedding instead of sinusoids\n",
      "Bounding box: (178.634, 421.7840784, 345.78650280000005, 431.74667839999995)\n",
      "Text: 6\n",
      "Bounding box: (148.182, 409.1470784, 153.1633, 419.1096784)\n",
      "Text: 1024\n",
      "Bounding box: (168.76473159999998, 409.1470784, 188.6899316, 419.1096784)\n",
      "Text: 4096\n",
      "Bounding box: (202.2490302, 409.1470784, 222.17423019999998, 419.1096784)\n",
      "Text: 16\n",
      "Bounding box: (234.1293502, 409.1470784, 244.0919502, 419.1096784)\n",
      "Text: 0.3\n",
      "Bounding box: (315.1153256, 409.1470784, 327.5685756, 419.1096784)\n",
      "Text: (A)\n",
      "Bounding box: (118.406, 597.7890784, 132.23408880000002, 607.7516784000001)\n",
      "Text: (B)\n",
      "Bounding box: (118.68, 563.3330784, 131.96014580000002, 573.2956784)\n",
      "Text: (C)\n",
      "Bounding box: (118.68, 512.5140784, 131.96014580000002, 522.4766784000001)\n",
      "Text: (D)\n",
      "Bounding box: (118.406, 450.78507840000003, 132.23408880000002, 460.74767840000004)\n",
      "Text: (E)\n",
      "Bounding box: (118.959, 421.7840784, 131.68124020000002, 431.74667839999995)\n",
      "Text: big\n",
      "Bounding box: (118.954, 409.1470784, 131.6862028, 419.1096784)\n",
      "Text: PPL\n",
      "Bounding box: (405.096, 650.8100784, 422.2615598, 660.7726784)\n",
      "Text: train\n",
      "Bounding box: (371.139, 650.8100784, 389.4004458, 660.7726784)\n",
      "Text: steps\n",
      "Bounding box: (370.307, 639.4280784, 390.2322, 649.3906784000001)\n",
      "Text: (dev)\n",
      "Bounding box: (403.293, 639.4280784, 424.065021, 649.3906784000001)\n",
      "Text: 100K 4.92\n",
      "Bounding box: (369.20228099999997, 626.7900784, 422.39655, 636.7526784)\n",
      "Text: 5.29\n",
      "Bounding box: (404.962, 614.1530783999999, 422.39655, 624.1156784)\n",
      "Text: 5.00\n",
      "Bounding box: (404.962, 603.2430784, 422.39655, 613.2056784)\n",
      "Text: 4.91\n",
      "Bounding box: (404.962, 592.3340784, 422.39655, 602.2966784)\n",
      "Text: 5.01\n",
      "Bounding box: (404.962, 581.4250784, 422.39655, 591.3876784)\n",
      "Text: 5.16\n",
      "Bounding box: (404.962, 568.7880784, 422.39655, 578.7506784000001)\n",
      "Text: 5.01\n",
      "Bounding box: (404.962, 557.8790783999999, 422.39655, 567.8416784)\n",
      "Text: 6.11\n",
      "Bounding box: (404.962, 545.2410784, 422.39655, 555.2036784000001)\n",
      "Text: 5.19\n",
      "Bounding box: (404.962, 534.3320784, 422.39655, 544.2946784000001)\n",
      "Text: 4.88\n",
      "Bounding box: (404.962, 523.4230784, 422.39655, 533.3856784000001)\n",
      "Text: 5.75\n",
      "Bounding box: (404.962, 512.5140784, 422.39655, 522.4766784000001)\n",
      "Text: 4.66\n",
      "Bounding box: (404.962, 501.6050784, 422.39655, 511.5676784)\n",
      "Text: 5.12\n",
      "Bounding box: (404.962, 490.69607840000003, 422.39655, 500.6586784)\n",
      "Text: 4.75\n",
      "Bounding box: (404.962, 479.78707840000004, 422.39655, 489.7496784)\n",
      "Text: 5.77\n",
      "Bounding box: (404.962, 467.1490784, 422.39655, 477.11167839999996)\n",
      "Text: 4.95\n",
      "Bounding box: (404.962, 456.2400784, 422.39655, 466.20267839999997)\n",
      "Text: 4.67\n",
      "Bounding box: (404.962, 445.3310784, 422.39655, 455.2936784)\n",
      "Text: 5.47\n",
      "Bounding box: (404.962, 434.42207840000003, 422.39655, 444.3846784)\n",
      "Text: 4.92\n",
      "Bounding box: (404.962, 421.7840784, 422.39655, 431.74667839999995)\n",
      "Text: 300K 4.33\n",
      "Bounding box: (369.202281, 409.1470784, 422.39655, 419.17941659999997)\n",
      "Text: BLEU params\n",
      "Bounding box: (436.0199104, 650.8100784, 502.7593678, 660.7726784)\n",
      "Text: ×106\n",
      "Bounding box: (477.283, 639.6472556, 498.96657910000005, 650.8158828)\n",
      "Text: (dev)\n",
      "Bounding box: (438.6403048, 639.4280784, 459.4123258, 649.3906784000001)\n",
      "Text: 25.8\n",
      "Bounding box: (440.3093048, 626.7900784, 457.74385479999995, 636.7526784)\n",
      "Text: 65\n",
      "Bounding box: (483.3875872, 626.7900784, 493.3501872, 636.7526784)\n",
      "Text: 24.9\n",
      "Bounding box: (440.3093048, 614.1530783999999, 457.74385479999995, 624.1156784)\n",
      "Text: 25.5\n",
      "Bounding box: (440.3093048, 603.2430784, 457.74385479999995, 613.2056784)\n",
      "Text: 25.8\n",
      "Bounding box: (440.3093048, 592.3340784, 457.74385479999995, 602.2966784)\n",
      "Text: 25.4\n",
      "Bounding box: (440.3093048, 581.4250784, 457.74385479999995, 591.3876784)\n",
      "Text: 25.1\n",
      "Bounding box: (440.3093048, 568.7880784, 457.74385479999995, 578.7506784000001)\n",
      "Text: 25.4\n",
      "Bounding box: (440.3093048, 557.8790783999999, 457.74385479999995, 567.8416784)\n",
      "Text: 23.7\n",
      "Bounding box: (440.3093048, 545.2410784, 457.74385479999995, 555.2036784000001)\n",
      "Text: 25.3\n",
      "Bounding box: (440.3093048, 534.3320784, 457.74385479999995, 544.2946784000001)\n",
      "Text: 25.5\n",
      "Bounding box: (440.3093048, 523.4230784, 457.74385479999995, 533.3856784000001)\n",
      "Text: 24.5\n",
      "Bounding box: (440.3093048, 512.5140784, 457.74385479999995, 522.4766784000001)\n",
      "Text: 26.0\n",
      "Bounding box: (440.3093048, 501.6050784, 457.74385479999995, 511.5676784)\n",
      "Text: 25.4\n",
      "Bounding box: (440.3093048, 490.69607840000003, 457.74385479999995, 500.6586784)\n",
      "Text: 26.2\n",
      "Bounding box: (440.3093048, 479.78707840000004, 457.74385479999995, 489.7496784)\n",
      "Text: 24.6\n",
      "Bounding box: (440.3093048, 467.1490784, 457.74385479999995, 477.11167839999996)\n",
      "Text: 25.5\n",
      "Bounding box: (440.3093048, 456.2400784, 457.74385479999995, 466.20267839999997)\n",
      "Text: 25.3\n",
      "Bounding box: (440.3093048, 445.3310784, 457.74385479999995, 455.2936784)\n",
      "Text: 25.7\n",
      "Bounding box: (440.3093048, 434.42207840000003, 457.74385479999995, 444.3846784)\n",
      "Text: 25.7\n",
      "Bounding box: (440.3093048, 421.7840784, 457.74385479999995, 431.74667839999995)\n",
      "Text: 26.4\n",
      "Bounding box: (440.3093048, 409.21681659999996, 457.74385479999995, 419.17941659999997)\n",
      "Text: 58\n",
      "Bounding box: (483.3875872, 568.7880784, 493.3501872, 578.7506784000001)\n",
      "Text: 60\n",
      "Bounding box: (483.3875872, 557.8790783999999, 493.3501872, 567.8416784)\n",
      "Text: 36\n",
      "Bounding box: (483.3875872, 545.2410784, 493.3501872, 555.2036784000001)\n",
      "Text: 50\n",
      "Bounding box: (483.3875872, 534.3320784, 493.3501872, 544.2946784000001)\n",
      "Text: 80\n",
      "Bounding box: (483.3875872, 523.4230784, 493.3501872, 533.3856784000001)\n",
      "Text: 28\n",
      "Bounding box: (483.3875872, 512.5140784, 493.3501872, 522.4766784000001)\n",
      "Text: 168\n",
      "Bounding box: (480.8969372, 501.6050784, 495.84083719999995, 511.5676784)\n",
      "Text: 53\n",
      "Bounding box: (483.3875872, 490.69607840000003, 493.3501872, 500.6586784)\n",
      "Text: 90\n",
      "Bounding box: (483.3875872, 479.78707840000004, 493.3501872, 489.7496784)\n",
      "Text: 213\n",
      "Bounding box: (480.902, 409.1470784, 495.8459, 419.1096784)\n",
      "Text: development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "Bounding box: (108.0, 368.54207840000004, 503.9972105879997, 378.5046784)\n",
      "Text: checkpoint averaging. We present these results in Table 3.\n",
      "Bounding box: (108.0, 357.63307840000004, 339.0625818000001, 367.5956784)\n",
      "Text: In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "Bounding box: (108.0, 341.24407840000003, 505.24134007599963, 351.2066784)\n",
      "Text: keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "Bounding box: (108.0, 330.33507840000004, 503.99721058799975, 340.2976784)\n",
      "Text: attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "Bounding box: (108.0, 319.4260784, 480.80049199999985, 329.3886784)\n",
      "Text: In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "Bounding box: (108.0, 302.3420828, 504.003520396, 313.21885560000004)\n",
      "Text: suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "Bounding box: (108.0, 292.1280784, 504.34271355599975, 302.0906784)\n",
      "Text: function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "Bounding box: (108.0, 281.2190784, 505.241340076, 291.1816784)\n",
      "Text: bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "Bounding box: (108.0, 270.3100784, 504.167371796, 280.2726784)\n",
      "Text: sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "Bounding box: (108.0, 259.4010784, 504.00008483600004, 269.3636784)\n",
      "Text: results to the base model.\n",
      "Bounding box: (108.0, 248.4920784, 209.00083880000003, 258.45467840000003)\n",
      "Text: 6.3 English Constituency Parsing\n",
      "Bounding box: (108.0, 221.7808166, 255.97449780000008, 231.7434166)\n",
      "Text: To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "Bounding box: (107.691, 200.8070784, 503.99772864479985, 210.7696784)\n",
      "Text: constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "Bounding box: (108.0, 189.8980784, 504.00238117739957, 199.8606784)\n",
      "Text: constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "Bounding box: (108.0, 178.98907839999998, 503.9972105879999, 188.9516784)\n",
      "Text: models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "Bounding box: (108.0, 168.0800784, 448.1032388, 178.0426784)\n",
      "Text: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Bounding box: (107.532, 150.99608279999998, 504.00032336800007, 161.87285559999998)\n",
      "Text: Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "Bounding box: (108.0, 140.7820784, 505.24372541599996, 150.7446784)\n",
      "Text: using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "Bounding box: (108.0, 129.8730784, 503.9980474463998, 139.8356784)\n",
      "Text: [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "Bounding box: (108.0, 118.9640784, 503.9957185795999, 128.92667840000001)\n",
      "Text: for the semi-supervised setting.\n",
      "Bounding box: (108.0, 108.0550784, 233.07048040000004, 118.0176784)\n",
      "Text: We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "Bounding box: (107.532, 91.6660784, 503.9962074629997, 101.6286784)\n",
      "Text: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "Bounding box: (107.671, 80.75707840000001, 503.9979726479998, 90.7196784)\n",
      "Text: remained unchanged from the English-to-German base translation model. During inference, we\n",
      "Bounding box: (108.0, 69.8480784, 503.99721058799986, 79.8106784)\n",
      "Text: 9\n",
      "Bounding box: (303.509, 39.9600784, 308.4903, 49.9226784)\n",
      "Text: Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "Bounding box: (107.691, 711.0930784, 503.99505866799973, 721.0556784)\n",
      "Text: of WSJ)\n",
      "Bounding box: (108.0, 700.1840784, 140.92639300000002, 710.1466784)\n",
      "Text: Parser\n",
      "Bounding box: (206.758, 688.1348166, 234.8724572, 698.0974166)\n",
      "Text: Training\n",
      "Bounding box: (334.084, 688.1348166, 370.995433, 698.0974166)\n",
      "Text: Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative\n",
      "Bounding box: (151.027, 676.7580783999999, 402.5227284, 686.7206784)\n",
      "Text: WSJ only, discriminative\n",
      "Bounding box: (302.558, 665.8490783999999, 402.5227284, 675.8116784)\n",
      "Text: WSJ only, discriminative\n",
      "Bounding box: (302.558, 654.9400784, 402.5227284, 664.9026784)\n",
      "Text: WSJ only, discriminative\n",
      "Bounding box: (302.558, 644.0310784, 402.5227284, 653.9936784)\n",
      "Text: WSJ only, discriminative\n",
      "Bounding box: (302.558, 633.1220784, 402.5227284, 643.0846784)\n",
      "Text: semi-supervised\n",
      "Bounding box: (320.167, 622.2130784, 384.91393739999995, 632.1756784)\n",
      "Text: semi-supervised\n",
      "Bounding box: (320.167, 611.3030784, 384.91393739999995, 621.2656784000001)\n",
      "Text: semi-supervised\n",
      "Bounding box: (320.167, 600.3940784, 384.91393739999995, 610.3566784000001)\n",
      "Text: semi-supervised\n",
      "Bounding box: (320.167, 589.4850783999999, 384.91393739999995, 599.4476784)\n",
      "Text: semi-supervised\n",
      "Bounding box: (320.167, 578.5760783999999, 384.91393739999995, 588.5386784)\n",
      "Text: multi-task\n",
      "Bounding box: (332.336, 567.6670783999999, 372.7443056, 577.6296784)\n",
      "Text: generative\n",
      "Bounding box: (331.992, 556.7580783999999, 373.08772500000003, 566.7206784)\n",
      "Text: Petrov et al. (2006) [29]\n",
      "Bounding box: (172.58599999999996, 665.8490783999999, 269.0438932, 675.8116784)\n",
      "Text: Zhu et al. (2013) [40]\n",
      "Bounding box: (177.493, 654.9400784, 264.1377322, 664.9026784)\n",
      "Text: Dyer et al. (2016) [8]\n",
      "Bounding box: (178.051, 644.0310784, 263.579921, 653.9936784)\n",
      "Text: Transformer (4 layers)\n",
      "Bounding box: (175.899, 633.1220784, 265.73176420000004, 643.0846784)\n",
      "Text: Zhu et al. (2013) [40]\n",
      "Bounding box: (177.493, 622.2130784, 264.1377322, 632.1756784)\n",
      "Text: Huang & Harper (2009) [14]\n",
      "Bounding box: (163.27099999999996, 611.3030784, 278.3589552, 621.2656784000001)\n",
      "Text: McClosky et al. (2006) [26]\n",
      "Bounding box: (164.83499999999998, 600.3940784, 276.79469880000005, 610.3566784000001)\n",
      "Text: Vinyals & Kaiser el al. (2014) [37]\n",
      "Bounding box: (151.027, 589.4850784, 290.60302600000006, 599.4476784000001)\n",
      "Text: Transformer (4 layers)\n",
      "Bounding box: (175.899, 578.5760783999999, 265.73176420000004, 588.5386784)\n",
      "Text: Luong et al. (2015) [23]\n",
      "Bounding box: (172.511, 567.6670783999999, 269.11833220000005, 577.6296784)\n",
      "Text: Dyer et al. (2016) [8]\n",
      "Bounding box: (178.051, 556.7580783999999, 263.579921, 566.7206784)\n",
      "Text: WSJ 23 F1\n",
      "Bounding box: (414.477, 688.1348166, 460.97245419999996, 698.0974166)\n",
      "Text: 88.3\n",
      "Bounding box: (429.008, 676.7580783999999, 446.44255, 686.7206784)\n",
      "Text: 90.4\n",
      "Bounding box: (429.008, 665.8490783999999, 446.44255, 675.8116784)\n",
      "Text: 90.4\n",
      "Bounding box: (429.008, 654.9400784, 446.44255, 664.9026784)\n",
      "Text: 91.7\n",
      "Bounding box: (429.008, 644.0310784, 446.44255, 653.9936784)\n",
      "Text: 91.3\n",
      "Bounding box: (429.008, 633.1220784, 446.44255, 643.0846784)\n",
      "Text: 91.3\n",
      "Bounding box: (429.008, 622.2130784, 446.44255, 632.1756784)\n",
      "Text: 91.3\n",
      "Bounding box: (429.008, 611.3030784, 446.44255, 621.2656784000001)\n",
      "Text: 92.1\n",
      "Bounding box: (429.008, 600.3940784, 446.44255, 610.3566784000001)\n",
      "Text: 92.1\n",
      "Bounding box: (429.008, 589.4850783999999, 446.44255, 599.4476784)\n",
      "Text: 92.7\n",
      "Bounding box: (429.008, 578.5760783999999, 446.44255, 588.5386784)\n",
      "Text: 93.0\n",
      "Bounding box: (429.008, 567.6670783999999, 446.44255, 577.6296784)\n",
      "Text: 93.3\n",
      "Bounding box: (429.008, 556.7580783999999, 446.44255, 566.7206784)\n",
      "Text: increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\n",
      "Bounding box: (108.0, 520.2100783999999, 504.24929999999995, 530.3918556)\n",
      "Text: for both WSJ only and the semi-supervised setting.\n",
      "Bounding box: (108.0, 509.3010784, 311.66543180000014, 519.2636784)\n",
      "Text: Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "Bounding box: (108.0, 492.91207840000004, 505.65359246399987, 502.8746784)\n",
      "Text: prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Bounding box: (108.0, 482.0030784, 504.0013948799997, 491.9656784)\n",
      "Text: Recurrent Neural Network Grammar [8].\n",
      "Bounding box: (108.0, 471.0940784, 271.39660260000005, 481.0566784)\n",
      "Text: In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
      "Bounding box: (108.0, 454.70507840000005, 505.65672295720015, 464.6676784)\n",
      "Text: Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "Bounding box: (108.0, 443.7960784, 419.56038979999994, 453.7586784)\n",
      "Text: 7 Conclusion\n",
      "Bounding box: (108.0, 415.5043632, 183.0667008, 427.4595632)\n",
      "Text: In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "Bounding box: (108.0, 393.18607840000004, 503.9967921587998, 403.1486784)\n",
      "Text: attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "Bounding box: (108.0, 382.2770784, 503.99553687119993, 392.2396784)\n",
      "Text: multi-headed self-attention.\n",
      "Bounding box: (108.0, 371.3680784, 218.12658040000005, 381.3306784)\n",
      "Text: For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "Bounding box: (108.0, 354.9790784, 503.99721058799986, 364.9416784)\n",
      "Text: on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "Bounding box: (108.0, 344.0700784, 503.99721058799975, 354.0326784)\n",
      "Text: English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "Bounding box: (108.0, 333.1610784, 503.997210588, 343.1236784)\n",
      "Text: model outperforms even all previously reported ensembles.\n",
      "Bounding box: (108.0, 322.2520784, 343.92433060000013, 332.2146784)\n",
      "Text: We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "Bounding box: (107.532, 305.86407840000004, 503.99546026799976, 315.8266784)\n",
      "Text: plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "Bounding box: (108.0, 294.9540784, 504.00029899399954, 304.9166784)\n",
      "Text: to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "Bounding box: (108.0, 284.0450784, 503.99721058799975, 294.00767840000003)\n",
      "Text: such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "Bounding box: (108.0, 273.13607840000003, 505.60792390559965, 283.09867840000004)\n",
      "Text: The code we used to train and evaluate our models is available at https://github.com/\n",
      "Bounding box: (107.691, 256.6883028, 505.05100605999996, 266.7106784)\n",
      "Text: tensorflow/tensor2tensor.\n",
      "Bounding box: (108.0, 245.77930279999998, 235.98964999999998, 255.8016784)\n",
      "Text: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "Bounding box: (108.0, 223.17407839999998, 504.0025651440001, 233.20641659999998)\n",
      "Text: comments, corrections and inspiration.\n",
      "Bounding box: (108.0, 212.2650784, 262.6793276000001, 222.2276784)\n",
      "Text: References\n",
      "Bounding box: (108.0, 183.97236320000002, 163.54385919999999, 195.9275632)\n",
      "Text: [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "Bounding box: (112.981, 167.1340784, 504.0027976212, 177.32581820000001)\n",
      "Text: arXiv:1607.06450, 2016.\n",
      "Bounding box: (129.579, 156.2250784, 229.75314999999998, 166.41681820000002)\n",
      "Text: [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "Bounding box: (112.981, 138.3410784, 504.3451129799997, 148.3036784)\n",
      "Text: learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "Bounding box: (129.579, 127.43207840000001, 368.65024860000005, 137.62381820000002)\n",
      "Text: [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
      "Bounding box: (112.98100000000002, 109.5490784, 504.00430239659977, 119.51167840000001)\n",
      "Text: machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "Bounding box: (129.579, 98.64007840000001, 386.9015486, 108.8318182)\n",
      "Text: [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "Bounding box: (112.98100000000002, 80.75707840000001, 504.00235968959987, 90.7196784)\n",
      "Text: reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "Bounding box: (129.579, 69.8480784, 325.11415, 80.0398182)\n",
      "Text: 10\n",
      "Bounding box: (301.019, 39.96007839999999, 310.98159999999996, 49.922678399999995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "Bounding box: (112.981, 707.8850784, 505.2423447359999, 717.8476784000001)\n",
      "Text: and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "Bounding box: (129.579, 696.9760784, 504.0035552015999, 706.9386784000001)\n",
      "Text: machine translation. CoRR, abs/1406.1078, 2014.\n",
      "Bounding box: (129.579, 686.0670784, 328.5312486, 696.2588182000001)\n",
      "Text: [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "Bounding box: (112.981, 666.2660784, 504.00063514399994, 676.4578182)\n",
      "Text: preprint arXiv:1610.02357, 2016.\n",
      "Bounding box: (129.579, 655.3570784, 264.53215, 665.5488182)\n",
      "Text: [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "Bounding box: (112.981, 635.5560783999999, 504.00378434139975, 645.5186784)\n",
      "Text: of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "Bounding box: (129.579, 624.6470783999999, 478.87624860000005, 634.8388182)\n",
      "Text: [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "Bounding box: (112.98100000000005, 604.8460783999999, 504.00259879199996, 614.8086784000001)\n",
      "Text: network grammars. In Proc. of NAACL, 2016.\n",
      "Bounding box: (129.579, 593.9370784, 313.29915000000005, 604.1288182000001)\n",
      "Text: [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "Bounding box: (112.98100000000005, 574.1360784, 505.6533219111997, 584.0986784)\n",
      "Text: tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "Bounding box: (129.579, 563.2270784, 450.72214999999994, 573.4188182)\n",
      "Text: [10] Alex Graves. Generating sequences with recurrent neural networks.\n",
      "Bounding box: (107.99999999999994, 543.4260783999999, 430.63402735200003, 553.3886784)\n",
      "Text: arXiv preprint\n",
      "Bounding box: (443.187, 543.6552181999999, 503.995522368, 553.6178182)\n",
      "Text: arXiv:1308.0850, 2013.\n",
      "Bounding box: (129.579, 532.5170784, 224.77115, 542.7088182)\n",
      "Text: [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "Bounding box: (108.00000000000003, 512.7160783999999, 505.64881881599985, 522.6786784000001)\n",
      "Text: age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Bounding box: (129.579, 501.8070784, 504.0035508960001, 511.9988182)\n",
      "Text: Recognition, pages 770–778, 2016.\n",
      "Bounding box: (129.27, 490.8970784, 269.74304020000005, 501.0888182)\n",
      "Text: [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "Bounding box: (108.0, 471.0960784, 504.0019611855999, 481.05867839999996)\n",
      "Text: recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "Bounding box: (129.579, 460.1870784, 412.1482238000001, 470.14967839999997)\n",
      "Text: [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "Bounding box: (108.0, 440.38607840000003, 505.245463, 450.5778182)\n",
      "Text: 9(8):1735–1780, 1997.\n",
      "Bounding box: (129.579, 429.47707840000004, 221.1751444, 439.43967840000005)\n",
      "Text: [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "Bounding box: (108.0, 409.6760784, 504.0025987919998, 419.6386784)\n",
      "Text: across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Bounding box: (129.579, 398.7670784, 504.0013075240001, 408.9588182)\n",
      "Text: Language Processing, pages 832–841. ACL, August 2009.\n",
      "Bounding box: (129.3, 387.8580784, 363.47063760000003, 398.0498182)\n",
      "Text: [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "Bounding box: (108.0, 368.0570784, 503.9998491143997, 378.0196784)\n",
      "Text: the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "Bounding box: (129.579, 357.14807840000003, 421.98015, 367.3398182)\n",
      "Text: [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Bounding box: (107.99999999999994, 337.34707840000004, 503.99519281600004, 347.53881820000004)\n",
      "Text: Information Processing Systems, (NIPS), 2016.\n",
      "Bounding box: (129.41, 326.4380784, 317.40315, 336.6298182)\n",
      "Text: [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "Bounding box: (107.99999999999997, 306.6370784, 504.00404771840005, 316.8288182)\n",
      "Text: on Learning Representations (ICLR), 2016.\n",
      "Bounding box: (129.579, 295.7280784, 302.99715000000003, 305.9198182)\n",
      "Text: [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "Bounding box: (108.00000000000003, 275.9270784, 505.6534016119998, 285.8896784)\n",
      "Text: ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n",
      "Bounding box: (129.579, 265.01807840000004, 505.24483699999996, 275.20981820000003)\n",
      "Text: 2017.\n",
      "Bounding box: (129.579, 254.10907840000002, 151.99484999999999, 264.0716784)\n",
      "Text: [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "Bounding box: (108.0, 234.3080784, 505.7473290047997, 244.27067840000004)\n",
      "Text: In International Conference on Learning Representations, 2017.\n",
      "Bounding box: (129.579, 223.39907839999998, 386.19415000000004, 233.5908182)\n",
      "Text: [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "Bounding box: (108.00000000000003, 203.5970784, 505.7453403842, 213.78881819999998)\n",
      "Text: [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "Bounding box: (108.0, 183.7960784, 504.0007056056, 193.9878182)\n",
      "Text: arXiv:1703.10722, 2017.\n",
      "Bounding box: (129.579, 172.88707839999998, 229.75314999999998, 183.0788182)\n",
      "Text: [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Bounding box: (108.0, 153.0860784, 504.00259879199984, 163.0486784)\n",
      "Text: Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "Bounding box: (129.579, 142.1770784, 503.996591224, 152.36881820000002)\n",
      "Text: arXiv:1703.03130, 2017.\n",
      "Bounding box: (129.579, 131.26807839999998, 229.75314999999998, 141.45981819999997)\n",
      "Text: [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "Bounding box: (108.0, 111.46707839999999, 504.2508368961998, 121.4296784)\n",
      "Text: sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "Bounding box: (129.579, 100.5580784, 416.13215, 110.74981819999999)\n",
      "Text: [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "Bounding box: (108.0, 80.7570784, 505.6534016119997, 90.7196784)\n",
      "Text: based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "Bounding box: (129.579, 69.8480784, 426.10415, 80.0398182)\n",
      "Text: 11\n",
      "Bounding box: (301.019, 39.96007839999999, 310.98159999999996, 49.922678399999995)\n",
      "Text: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "Bounding box: (108.0, 707.8850784, 504.00339579999974, 717.8476784000001)\n",
      "Text: corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
      "Bounding box: (129.579, 696.9760784, 479.1151444, 707.1678182)\n",
      "Text: [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Bounding box: (108.0, 673.4930784, 503.99751786599995, 683.4556784000001)\n",
      "Text: Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      "Bounding box: (129.27, 662.5840784, 505.24521545, 672.7758182)\n",
      "Text: pages 152–159. ACL, June 2006.\n",
      "Bounding box: (129.579, 651.6750784, 262.2808320000001, 661.6376784)\n",
      "Text: [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "Bounding box: (108.0, 628.1920784, 504.00339579999974, 638.1546784000001)\n",
      "Text: model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "Bounding box: (129.579, 617.2830783999999, 407.44515, 627.4748182)\n",
      "Text: [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "Bounding box: (108.0, 593.8000784, 503.997218988, 603.7626784)\n",
      "Text: summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "Bounding box: (129.579, 582.8910784, 354.45315, 593.0828182)\n",
      "Text: [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "Bounding box: (107.99999999999997, 559.4080783999999, 505.242344736, 569.3706784000001)\n",
      "Text: and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Bounding box: (129.579, 548.4990783999999, 504.00305504000005, 558.6908182)\n",
      "Text: Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
      "Bounding box: (129.25, 537.5900783999999, 504.346771288, 547.7818182)\n",
      "Text: 2006.\n",
      "Bounding box: (129.579, 526.6810783999999, 151.99484999999999, 536.6436784)\n",
      "Text: [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "Bounding box: (108.0, 503.1980784, 504.00063514399994, 513.3898182)\n",
      "Text: preprint arXiv:1608.05859, 2016.\n",
      "Bounding box: (129.579, 492.2890784, 264.53215, 502.4808182)\n",
      "Text: [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "Bounding box: (108.0, 468.8060784, 504.0025190911998, 478.7686784)\n",
      "Text: with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "Bounding box: (129.22, 457.8970784, 371.16115, 468.0888182)\n",
      "Text: [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "Bounding box: (108.0, 434.4140784, 505.24880050079986, 444.37667839999995)\n",
      "Text: and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "Bounding box: (129.579, 423.5050784, 504.00259879199996, 433.46767839999995)\n",
      "Text: layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "Bounding box: (129.579, 412.5960784, 314.60415, 422.7878182)\n",
      "Text: [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "Bounding box: (108.0, 389.1130784, 505.6512098399998, 399.0756784)\n",
      "Text: nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Bounding box: (129.579, 378.2040784, 503.9972643058, 388.3958182)\n",
      "Text: Learning Research, 15(1):1929–1958, 2014.\n",
      "Bounding box: (129.3, 367.2950784, 306.43474440000006, 377.4868182)\n",
      "Text: [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "Bounding box: (108.0, 343.8120784, 504.3481017599999, 353.77467839999997)\n",
      "Text: networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Bounding box: (129.579, 332.9020784, 505.2423447359999, 342.8646784)\n",
      "Text: Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Bounding box: (128.971, 321.9930784, 505.24585387900004, 332.1848182)\n",
      "Text: Inc., 2015.\n",
      "Bounding box: (129.579, 311.0840784, 172.1890402, 321.0466784)\n",
      "Text: [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "Bounding box: (108.0, 287.6010784, 504.0025987919998, 297.56367839999996)\n",
      "Text: networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "Bounding box: (129.579, 276.6920784, 494.1286402, 286.8838182)\n",
      "Text: [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Bounding box: (108.0, 253.20907839999998, 505.7402754839997, 263.1716784)\n",
      "Text: Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "Bounding box: (129.579, 242.3000784, 484.3155486, 252.4918182)\n",
      "Text: [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Bounding box: (108.0, 218.81707839999999, 504.0025987919998, 228.7796784)\n",
      "Text: Advances in Neural Information Processing Systems, 2015.\n",
      "Bounding box: (128.971, 207.9080784, 365.24315, 218.09981820000002)\n",
      "Text: [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Bounding box: (108.0, 184.4250784, 504.0025987919999, 194.3876784)\n",
      "Text: Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "Bounding box: (129.579, 173.5160784, 504.0033957999998, 183.4786784)\n",
      "Text: translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "Bounding box: (129.579, 162.60707839999998, 503.999372686, 172.79881819999997)\n",
      "Text: arXiv:1609.08144, 2016.\n",
      "Bounding box: (129.579, 151.69807839999999, 229.75314999999998, 161.88981819999998)\n",
      "Text: [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "Bounding box: (108.0, 128.21507839999998, 504.0025987919998, 138.1776784)\n",
      "Text: fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "Bounding box: (129.579, 117.3060784, 476.6145486, 127.4978182)\n",
      "Text: [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "Bounding box: (108.0, 93.8230784, 504.00259879199984, 103.7856784)\n",
      "Text: shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "Bounding box: (129.579, 82.91407840000001, 503.9968397520001, 93.1058182)\n",
      "Text: 1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "Bounding box: (128.832, 72.0050784, 342.0416376, 82.1968182)\n",
      "Text: 12\n",
      "Bounding box: (301.019, 39.96007839999999, 310.98159999999996, 49.922678399999995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Attention Visualizations\n",
      "Bounding box: (108.0, 707.5383632, 230.7679488, 719.4935632)\n",
      "Text: Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "Bounding box: (108.0, 469.9060784, 503.99721058799975, 479.8686784)\n",
      "Text: encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "Bounding box: (108.0, 458.99607840000004, 503.99679215879985, 468.95867840000005)\n",
      "Text: the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "Bounding box: (108.0, 448.0870784, 504.1711575839997, 458.04967839999995)\n",
      "Text: the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "Bounding box: (108.0, 437.1780784, 440.91024159999995, 447.14067839999996)\n",
      "Text: 13\n",
      "Bounding box: (301.019, 39.960078399999965, 310.98159999999996, 49.92267839999997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Bounding box: (108.0, 168.0110784, 505.3881887999997, 177.9736784)\n",
      "Text: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "Bounding box: (108.0, 157.10207839999998, 503.99804744639994, 167.0646784)\n",
      "Text: and 6. Note that the attentions are very sharp for this word.\n",
      "Bounding box: (108.0, 146.1930784, 343.2568364000001, 156.1556784)\n",
      "Text: 14\n",
      "Bounding box: (301.019, 39.96007839999999, 310.98159999999996, 49.922678399999995)\n",
      "Text: Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "Bounding box: (108.0, 179.8940784, 503.99721058799975, 189.85667840000002)\n",
      "Text: sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "Bounding box: (108.0, 168.98507840000002, 504.00139487999985, 178.94767840000003)\n",
      "Text: at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "Bounding box: (108.0, 158.07507840000002, 377.2591902000001, 168.03767840000003)\n",
      "Text: 15\n",
      "Bounding box: (301.019, 39.960078400000036, 310.98159999999996, 49.92267840000004)\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextBoxHorizontal, LTTextLineHorizontal\n",
    "\n",
    "for page_layout in extract_pages(pdf_path):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextBoxHorizontal):\n",
    "            for text_line in element:\n",
    "                if isinstance(text_line, LTTextLineHorizontal):\n",
    "                    text = text_line.get_text().strip()\n",
    "                    bbox = text_line.bbox  # (x0, y0, x1, y1)\n",
    "                    print(f\"Text: {text}\")\n",
    "                    print(f\"Bounding box: {bbox}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af36a49",
   "metadata": {},
   "source": [
    "## Font size based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02361bb7",
   "metadata": {},
   "source": [
    "Not useful, as the categorization happens for only one class or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cfb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextBoxHorizontal, LTTextLineHorizontal, LTChar, LTImage\n",
    "\n",
    "# 1. Collect font size stats from the whole PDF\n",
    "def collect_font_sizes(pdf_path):\n",
    "    font_sizes = []\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextBoxHorizontal):\n",
    "                for line in element:\n",
    "                    if isinstance(line, LTTextLineHorizontal):\n",
    "                        for char in line:\n",
    "                            if isinstance(char, LTChar):\n",
    "                                font_sizes.append(round(char.size, 1))\n",
    "    return font_sizes\n",
    "\n",
    "# 2. Classify text lines based on relative font size\n",
    "def classify_text(textline, thresholds):\n",
    "    font_sizes = [char.size for char in textline if isinstance(char, LTChar)]\n",
    "    if not font_sizes:\n",
    "        return \"Other\"\n",
    "\n",
    "    avg_font = sum(font_sizes) / len(font_sizes)\n",
    "    if avg_font >= thresholds[\"title\"]:\n",
    "        return \"Title\"\n",
    "    elif avg_font >= thresholds[\"paragraph\"]:\n",
    "        return \"Paragraph\"\n",
    "    else:\n",
    "        return \"Caption or Footer\"\n",
    "\n",
    "# 3. Adaptive threshold generation based on percentiles\n",
    "def compute_thresholds(font_sizes):\n",
    "    sorted_sizes = sorted(font_sizes)\n",
    "    n = len(sorted_sizes)\n",
    "    return {\n",
    "        \"title\": sorted_sizes[int(n * 0.90)] if n else 0,      # Top 10% font size\n",
    "        \"paragraph\": sorted_sizes[int(n * 0.50)] if n else 0,  # Median\n",
    "    }\n",
    "\n",
    "# 4. Full layout extraction\n",
    "def extract_layout_adaptive(pdf_path):\n",
    "    font_sizes = collect_font_sizes(pdf_path)\n",
    "    thresholds = compute_thresholds(font_sizes)\n",
    "    results = []\n",
    "\n",
    "    for page_num, page_layout in enumerate(extract_pages(pdf_path)):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextBoxHorizontal):\n",
    "                for line in element:\n",
    "                    if isinstance(line, LTTextLineHorizontal):\n",
    "                        text = line.get_text().strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        bbox = list(line.bbox)\n",
    "                        label = classify_text(line, thresholds)\n",
    "                        results.append({\n",
    "                            \"type\": label,\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"text\": text,\n",
    "                            \"bbox\": bbox\n",
    "                        })\n",
    "            elif isinstance(element, LTImage):\n",
    "                results.append({\n",
    "                    \"type\": \"Image\",\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"text\": None,\n",
    "                    \"bbox\": list(element.bbox)\n",
    "                })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01607dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Usage\n",
    "# pdf_path = \"example.pdf\"\n",
    "output = extract_layout_adaptive(pdf_path)\n",
    "\n",
    "with open(\"output_adaptive.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a698926",
   "metadata": {},
   "source": [
    "# PDFplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdcfb0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (44.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 645.7 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 699.0 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 699.0 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.0/3.0 MB 740.5 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 780.8 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.6/3.0 MB 814.6 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.8/3.0 MB 853.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 896.4 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 896.4 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.4/3.0 MB 895.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 915.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 897.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 881.1 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfplumber\n",
      "Successfully installed pdfplumber-0.11.6 pypdfium2-4.30.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3b322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providedproperattributionisprovided,Googleherebygrantspermissionto 124.666 72.5901232 487.3389472 84.54532319999998\n",
      "reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor 124.313 86.5371232 487.8945424 98.49232319999999\n",
      "scholarlyworks. 267.594 100.48512319999998 346.49832000000004 112.44032319999997\n",
      "Attention 211.488 148.4256186 281.296447 165.64101860000005\n",
      "Is 285.600297 148.4256186 298.9938782 165.64101860000005\n",
      "All 303.2977282 148.4256186 325.2990094 165.64101860000005\n",
      "You 329.6028594 148.4256186 358.30093120000004 165.64101860000005\n",
      "Need 362.60478120000005 148.4256186 399.89333760000005 165.64101860000005\n",
      "AshishVaswani∗ 132.908 233.45911720000004 203.88876513999998 244.7771834\n",
      "GoogleBrain 139.379 245.7933216 193.3364416 255.75592160000008\n",
      "avaswani@google.com 116.68099999999998 256.7620972000001 216.03900606 266.72469720000004\n",
      "NoamShazeer∗ 239.06199999999998 233.45911720000004 304.84176514 244.7771834\n",
      "GoogleBrain 242.932 245.7933216 296.88944159999994 255.75592160000008\n",
      "noam@google.com 230.69199999999998 256.7620972000001 309.13253109999994 266.72469720000004\n",
      "NikiParmar∗ 338.692 233.45911720000004 396.63076514 244.7771834\n",
      "GoogleResearch 331.45399999999995 245.7933216 399.7874733999999 255.75592160000008\n",
      "nikip@google.com 323.787 256.7620972000001 407.45689983999995 266.72469720000004\n",
      "JakobUszkoreit∗ 424.29999999999995 233.45911720000004 497.21276514 244.7771834\n",
      "GoogleResearch 424.549 245.7933216 492.8824733999999 255.75592160000008\n",
      "usz@google.com 422.11199999999997 256.7620972000001 495.3231623599999 266.72469720000004\n",
      "LlionJones∗ 144.29199999999997 283.4561172 197.21976513999996 294.7741834\n",
      "GoogleResearch 134.54799999999997 295.79032159999997 202.88147339999998 305.7529216\n",
      "llion@google.com 126.88199999999998 306.7600972 210.55189983999998 316.7226972\n",
      "AidanN.Gomez∗ 248.76099999999997 283.4561172 323.40776514 294.7741834\n",
      "† 326.63663454 283.4561172 330.28951098 290.4299172\n",
      "UniversityofToronto 244.57499999999996 295.79032159999997 330.82122819999995 305.7529216\n",
      "aidan@cs.toronto.edu 235.40699999999995 306.7600972 339.9943747999999 316.7226972\n",
      "ŁukaszKaiser∗ 394.12499999999994 283.4561172 459.92376513999994 294.7741834\n",
      "GoogleBrain 398.00499999999994 295.79032159999997 451.9624415999999 305.7529216\n",
      "lukaszkaiser@google.com 364.84899999999993 306.7590972 485.1145184199999 316.7216972\n",
      "IlliaPolosukhin∗ 268.8069999999999 333.4531172 339.9657651399999 344.7721834\n",
      "‡ 343.1946345399999 333.4531172 346.8475109799999 340.4269172\n",
      "illia.polosukhin@gmail.com 238.0219999999999 345.8480972 373.97562463999986 355.8106972\n",
      "Abstract 283.7579999999999 385.3244368 328.2432991999999 397.2796368\n",
      "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor 143.557 413.1543216 468.299304364 423.11692159999996\n",
      "convolutionalneuralnetworksthatincludeanencoderandadecoder. 143.866 424.0633216 428.5299600760001 434.0259216\n",
      "Thebest 433.01133680800024 424.0633216 468.1306973200002 434.0259216\n",
      "performing 143.866 434.9723216 189.58417214800002 444.9349216\n",
      "models 192.886774048 434.9723216 222.24436447600002 444.9349216\n",
      "also 225.546966376 434.9723216 241.91770994800004 444.9349216\n",
      "connect 245.22031184800002 434.9723216 276.823671568 444.9349216\n",
      "the 280.126273468 434.9723216 292.544056612 444.9349216\n",
      "encoder 295.8466585120001 434.9723216 328.00892009200004 444.9349216\n",
      "and 331.31152199200005 434.9723216 345.9852362800001 444.9349216\n",
      "decoder 349.2878381800001 434.9723216 381.4500997600001 444.9349216\n",
      "through 384.7527016600001 434.9723216 416.36622323200015 444.9349216\n",
      "an 419.66882513200005 434.9723216 429.2616134200001 444.9349216\n",
      "attention 432.56421532 434.9723216 468.13069731999985 444.9349216\n",
      "mechanism. 143.866 445.8813216 192.69369886 455.8439216\n",
      "We 198.77048635600002 445.8813216 212.07235062400002 455.8439216\n",
      "propose 215.55786586000005 445.8813216 247.73028929200007 455.8439216\n",
      "a 251.2158045280001 445.8813216 255.7276668160001 455.8439216\n",
      "new 259.2131820520001 445.8813216 275.89894303600016 455.8439216\n",
      "simple 279.38445827200013 445.8813216 306.4861175560001 455.8439216\n",
      "network 309.97163279200015 445.8813216 343.1704032760002 455.8439216\n",
      "architecture, 346.6559185120002 445.8813216 397.16032295200023 455.8439216\n",
      "the 400.8897226360002 445.8813216 413.30750578000027 455.8439216\n",
      "Transformer, 416.79302101600024 445.8813216 469.38060511600025 455.8439216\n",
      "basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions 143.866 456.79032159999997 468.1360771239999 466.7529216\n",
      "entirely. 143.866 467.69932159999996 176.800562332 477.66192159999997\n",
      "Experiments 182.92815908800003 467.69932159999996 234.29632094800004 477.66192159999997\n",
      "on 237.80215988800003 467.69932159999996 247.96401188800004 477.66192159999997\n",
      "two 251.46985082800003 467.69932159999996 266.611010308 477.66192159999997\n",
      "machine 270.10668739600004 467.69932159999996 304.53504197200004 477.66192159999997\n",
      "translation 308.04088091200003 467.69932159999996 350.94422005600006 477.66192159999997\n",
      "tasks 354.45005899600005 467.69932159999996 374.7737629960001 477.66192159999997\n",
      "show 378.27960193600006 467.69932159999996 399.4772252080001 477.66192159999997\n",
      "these 402.98306414800015 467.69932159999996 423.86567000800005 477.66192159999997\n",
      "models 427.37150894800004 467.69932159999996 456.7290993759999 477.66192159999997\n",
      "to 460.224776464 467.69932159999996 468.13069731999985 477.66192159999997\n",
      "besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly 143.866 478.60832159999995 468.48102218639985 488.57092159999996\n",
      "less 143.866 489.51732159999995 159.10877800000003 499.47992159999995\n",
      "time 162.147171748 489.51732159999995 180.214944604 499.47992159999995\n",
      "to 183.26350020400002 489.51732159999995 191.16942106000002 499.47992159999995\n",
      "train. 194.20781480800002 489.51732159999995 215.37495252399998 499.47992159999995\n",
      "Our 220.12053740800002 489.51732159999995 235.92221726800003 499.47992159999995\n",
      "model 238.96061101600003 489.51732159999995 264.365241016 499.47992159999995\n",
      "achieves 267.413796616 489.51732159999995 301.99457897200006 499.47992159999995\n",
      "28.4 305.0431345720001 489.51732159999995 322.8263755720001 499.47992159999995\n",
      "BLEU 325.86476932000005 489.51732159999995 352.3973648920001 499.47992159999995\n",
      "on 355.4459204920001 489.51732159999995 365.6077724920001 499.47992159999995\n",
      "the 368.64616624000007 489.51732159999995 381.06394938400007 499.47992159999995\n",
      "WMT 384.1023431320001 489.51732159999995 408.93790942000015 499.47992159999995\n",
      "2014 411.9864650200001 489.51732159999995 432.31016902 499.47992159999995\n",
      "English- 435.3485627680001 489.51732159999995 469.7870791959999 499.47992159999995\n",
      "to-German 143.866 500.4273216 187.88714286400003 510.3899216\n",
      "translation 191.19990661600002 500.4273216 234.10324576000002 510.3899216\n",
      "task, 237.40584766 500.4273216 256.31705423200003 510.3899216\n",
      "improving 259.83305502400003 500.4273216 302.02506452800003 510.3899216\n",
      "over 305.33782828000005 500.4273216 323.10074557600007 510.3899216\n",
      "the 326.4033474760001 500.4273216 338.8211306200001 510.3899216\n",
      "existing 342.1338943720001 500.4273216 374.1640518760001 510.3899216\n",
      "best 377.47681562800005 500.4273216 393.8475592000001 510.3899216\n",
      "results, 397.1603229520001 500.4273216 426.22321967199997 510.3899216\n",
      "including 429.739220464 500.4273216 468.13069731999974 510.3899216\n",
      "ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, 143.866 511.3363216 469.37602231999983 521.2989216\n",
      "ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after 143.866 522.2453216 468.30205404000003 532.2079216\n",
      "trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe 143.866 533.1543216 468.13069731999985 543.1169216000001\n",
      "bestmodelsfromtheliterature. 143.866 544.0633216 268.20797523760007 554.0259216\n",
      "WeshowthattheTransformergeneralizeswellto 271.31379586240007 544.0633216 468.13153417840005 554.0259216\n",
      "othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith 143.866 554.9723216 468.13886665199976 564.9349216\n",
      "largeandlimitedtrainingdata. 143.866 565.8813216 266.2665036000001 575.8439216\n",
      "∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted 119.822 597.2280544 503.9969183039999 607.7917424\n",
      "theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand 108.0 608.7873423999999 503.99668039679995 617.7537424\n",
      "hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head 108.0 618.7503424 503.99818675200004 627.7167423999999\n",
      "attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery 108.0 628.7133424 504.31406405760015 637.6797424\n",
      "detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand 108.0 638.6753424 503.9965190016 647.6417424\n",
      "tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and 108.0 648.6383424 503.99954964479986 657.6047424000001\n",
      "efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand 108.0 658.6013424 503.9981867520001 667.5677424\n",
      "implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating 108.0 668.5633424 503.9981867519998 677.5297424\n",
      "ourresearch. 108.0 678.5263424 154.3024896 687.4927424\n",
      "†WorkperformedwhileatGoogleBrain. 120.21000000000001 687.8780544 267.35134080000006 698.4417424\n",
      "‡WorkperformedwhileatGoogleResearch. 120.21000000000001 698.8270544 280.2898560000001 709.3907424\n",
      "31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 108.00000000000001 732.8473424 460.13742720000016 741.8137424\n",
      "a 16.34 546.12 36.34 555.0\n",
      "r 16.34 539.46 36.34 546.12\n",
      "X 16.34 525.02 36.34 539.46\n",
      "i 16.34 519.46 36.34 525.02\n",
      "v 16.34 509.46 36.34 519.46\n",
      ": 16.34 503.9 36.34 509.46\n",
      "1 16.34 493.9 36.34 503.9\n",
      "7 16.34 483.9 36.34 493.9\n",
      "0 16.34 473.9 36.34 483.9\n",
      "6 16.34 463.9 36.34 473.9\n",
      ". 16.34 458.9 36.34 463.9\n",
      "0 16.34 448.9 36.34 458.9\n",
      "3 16.34 438.9 36.34 448.9\n",
      "7 16.34 428.9 36.34 438.9\n",
      "6 16.34 418.9 36.34 428.9\n",
      "2 16.34 408.9 36.34 418.9\n",
      "v 16.34 398.9 36.34 408.9\n",
      "7 16.34 388.9 36.34 398.9\n",
      "  16.34 383.9 36.34 388.9\n",
      "  16.34 378.9 36.34 383.9\n",
      "[ 16.34 372.23999999999995 36.34 378.9\n",
      "c 16.34 363.36 36.34 372.24\n",
      "s 16.34 355.58000000000004 36.34 363.36\n",
      ". 16.34 350.58000000000004 36.34 355.58000000000004\n",
      "C 16.34 337.24000000000007 36.34 350.58000000000004\n",
      "L 16.34 325.02 36.34 337.24\n",
      "] 16.34 318.35999999999996 36.34 325.02\n",
      "  16.34 313.36 36.34 318.36\n",
      "  16.34 308.36 36.34 313.36\n",
      "2 16.34 298.36 36.34 308.36\n",
      "  16.34 293.36 36.34 298.36\n",
      "A 16.34 278.91999999999996 36.34 293.36\n",
      "u 16.34 268.9200000000001 36.34 278.9200000000001\n",
      "g 16.34 258.9200000000001 36.34 268.9200000000001\n",
      "  16.34 253.92000000000007 36.34 258.9200000000001\n",
      "2 16.34 243.92000000000007 36.34 253.92000000000007\n",
      "0 16.34 233.92000000000007 36.34 243.92000000000007\n",
      "2 16.34 223.92000000000007 36.34 233.92000000000007\n",
      "3 16.34 213.92000000000007 36.34 223.92000000000007\n",
      "1 108.0 72.50643679999996 113.9776 84.46163679999995\n",
      "Introduction 125.9328 72.50643679999996 190.8136704 84.46163679999995\n",
      "Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks 108.0 98.83532159999993 503.9985536319999 108.7979216\n",
      "inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand 108.0 109.74432159999992 503.99768879279975 119.70692159999999\n",
      "transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. 108.0 120.65332160000003 457.156359716 130.6159216000001\n",
      "Numerous 461.66822200400003 120.65332160000003 504.002497436 130.6159216000001\n",
      "effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder 108.0 131.56232160000002 504.1673717959999 141.52492160000008\n",
      "architectures[38,24,15]. 108.0 142.4713216 210.36571500000002 152.43392160000008\n",
      "Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput 108.0 158.86032160000002 503.99721058799946 168.8229216000001\n",
      "sequences. 108.0 169.7693216 151.0505465216 179.73192160000008\n",
      "Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden 154.15130614560002 169.7693216 503.9970113359999 179.73192160000008\n",
      "statesh 108.0 180.4591444 138.14645012 190.64092160000007\n",
      "t 138.14600000000002 184.36211720000006 141.15589208 191.33591720000004\n",
      ",asafunctionoftheprevioushiddenstateh 141.654 180.4591444 316.66145012 190.64092160000007\n",
      "t−1 316.661 184.36211720000006 329.8685791 191.33591720000004\n",
      "andtheinputforpositiont. 332.857 180.4591444 441.23774349999997 190.64092160000007\n",
      "Thisinherently 444.324854362 180.6783216 504.35091412599996 190.64092160000007\n",
      "sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger 108.0 191.5873216 504.16737179599977 201.54992160000006\n",
      "sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. 108.0 202.4963216 397.4491662202 212.45892160000005\n",
      "Recentworkhasachieved 400.53823971440005 202.4963216 503.9973699896 212.45892160000005\n",
      "significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional 108.0 213.40532159999998 503.99714878 223.36792160000005\n",
      "computation[32],whilealsoimprovingmodelperformanceincaseofthelatter. 108.0 224.31432159999997 431.95124411080013 234.27692160000004\n",
      "Thefundamental 435.04453178480014 224.31432159999997 503.99949209799996 234.27692160000004\n",
      "constraintofsequentialcomputation,however,remains. 108.0 235.22332159999996 330.5246336000001 245.18592160000003\n",
      "Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- 107.641 251.61232159999997 505.6553182847998 261.57492160000004\n",
      "tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein 108.0 262.52132159999996 503.99701133599984 272.48392160000003\n",
      "theinputoroutputsequences[2,19]. 108.0 273.43032159999996 253.30903188399998 283.39292159999997\n",
      "Inallbutafewcases[27],however,suchattentionmechanisms 256.364959808 273.43032159999996 503.99916959600006 283.39292159999997\n",
      "areusedinconjunctionwitharecurrentnetwork. 108.0 284.33932159999995 303.78501520000015 294.30192159999996\n",
      "InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead 108.0 300.72832159999996 503.99721058799986 310.69092159999997\n",
      "relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput. 108.0 311.63732159999995 505.7389520207997 321.59992159999996\n",
      "TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin 107.691 322.54632160000006 504.0032280000001 332.5089216\n",
      "translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs. 108.0 333.45532160000005 452.35726899999986 343.4179216\n",
      "2 108.0 362.8194368 113.9776 374.7746368\n",
      "Background 125.9328 362.8194368 188.82910720000004 374.7746368\n",
      "ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU 107.691 389.14832160000003 504.0016738343998 399.1109216\n",
      "[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding 108.0 400.0573216 504.0028835880001 410.0199216\n",
      "block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, 108.0 410.9663216 505.2413400759996 420.92892159999997\n",
      "thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows 108.0 421.8763216 504.0013948799995 431.8389216\n",
      "inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. 108.0 432.7853216 456.04180709619976 442.7479216\n",
      "Thismakes 459.16584931639983 432.7853216 503.9992828087997 442.7479216\n",
      "it 108.0 443.69432159999997 113.649989712 453.6569216\n",
      "more 116.729030868 443.69432159999997 137.611636728 453.6569216\n",
      "difficult 140.69067788400002 443.69432159999997 172.619216868 453.6569216\n",
      "to 175.69825802399998 443.69432159999997 183.60417888 453.6569216\n",
      "learn 186.673058184 443.69432159999997 206.98660033200002 453.6569216\n",
      "dependencies 210.06564148800004 443.69432159999997 264.80753821200005 453.6569216\n",
      "between 267.88657936800007 443.69432159999997 301.74587023200013 453.6569216\n",
      "distant 304.82491138800015 443.69432159999997 331.92657067200014 453.6569216\n",
      "positions 335.00561182800016 443.69432159999997 371.71022125200017 453.6569216\n",
      "[12]. 374.7892624080002 443.69432159999997 394.056359716 453.6569216\n",
      "In 398.90356312 443.69432159999997 407.36838583599996 453.6569216\n",
      "the 410.44742699200003 443.69432159999997 422.865210136 453.6569216\n",
      "Transformer 425.944251292 443.69432159999997 476.38768462 453.6569216\n",
      "this 479.456563924 443.69432159999997 494.140440064 453.6569216\n",
      "is 497.21948122000003 443.69432159999997 503.997436504 453.6569216\n",
      "reducedtoaconstantnumberofoperations, 108.0 454.60332159999996 288.24076892400006 464.56592159999997\n",
      "albeitatthecostofreducedeffectiveresolutiondue 291.24867711600007 454.60332159999996 503.9972105879998 464.56592159999997\n",
      "to 108.0 465.51232159999995 115.90592085600001 475.47492159999996\n",
      "averaging 118.97480016 465.51232159999995 158.697479628 475.47492159999996\n",
      "attention-weighted 161.766358932 465.51232159999995 237.97008708000007 475.47492159999996\n",
      "positions, 241.04912823600006 465.51232159999995 280.29420066000006 475.47492159999996\n",
      "an 283.51550774400005 465.51232159999995 293.10829603200006 475.47492159999996\n",
      "effect 296.1771753360001 465.51232159999995 319.0515041880001 475.47492159999996\n",
      "we 322.12038349200014 465.51232159999995 333.96910292400014 475.47492159999996\n",
      "counteract 337.04814408000016 465.51232159999995 379.3722576600002 475.47492159999996\n",
      "with 382.4411369640002 465.51232159999995 400.50890982000004 475.47492159999996\n",
      "Multi-Head 403.57778912400005 465.51232159999995 450.99299055599994 475.47492159999996\n",
      "Attention 454.07203171199995 465.51232159999995 492.4635085679997 475.47492159999996\n",
      "as 495.53238787199973 465.51232159999995 503.99721058799975 475.47492159999996\n",
      "describedinsection3.2. 108.0 476.42132160000006 204.56748180000002 486.3839216\n",
      "Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions 108.0 492.81032159999995 503.99958168679973 502.77292159999996\n",
      "ofasinglesequenceinordertocomputearepresentationofthesequence. 108.0 503.71932159999994 407.7339865920001 513.6819215999999\n",
      "Self-attentionhasbeen 410.9044844160002 503.71932159999994 503.997210588 513.6819215999999\n",
      "usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, 108.0 514.6283216 505.24956918359993 524.5909216\n",
      "textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22]. 108.0 525.5373216 463.116877 535.4999216\n",
      "End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- 108.0 541.9253216 505.653592464 551.8879216\n",
      "alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand 108.0 552.8343216000001 503.9949988907996 562.7969216\n",
      "languagemodelingtasks[34]. 108.0 563.7443216 228.08918040000003 573.7069216\n",
      "To 107.691 580.1323216000001 118.18819311600001 590.0949216\n",
      "the 121.673708352 580.1323216000001 134.091491496 590.0949216\n",
      "best 137.577006732 580.1323216000001 153.947750304 590.0949216\n",
      "of 157.44342739200002 580.1323216000001 165.908250108 590.0949216\n",
      "our 169.393765344 580.1323216000001 182.93951406 590.0949216\n",
      "knowledge, 186.435191148 580.1323216000001 233.31181442400003 590.0949216\n",
      "however, 237.05137596000006 580.1323216000001 273.53242464000004 590.0949216\n",
      "the 277.27198617600004 580.1323216000001 289.68976932000004 590.0949216\n",
      "Transformer 293.1854464080001 580.1323216000001 343.61871788400015 590.0949216\n",
      "is 347.1143949720001 580.1323216000001 353.8923502560001 590.0949216\n",
      "the 357.3778654920002 580.1323216000001 369.7956486360001 590.0949216\n",
      "first 373.2811638720002 580.1323216000001 389.09300558400014 590.0949216\n",
      "transduction 392.5886826720001 580.1323216000001 442.8288789599999 590.0949216\n",
      "model 446.31439419599997 580.1323216000001 471.71902419599985 590.0949216\n",
      "relying 475.21470128399983 580.1323216000001 504.0032279999997 590.0949216\n",
      "entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- 108.0 591.0413216 505.6571789999996 601.0039216\n",
      "alignedRNNsorconvolution. 108.0 601.9503216 227.19254640000005 611.9129216\n",
      "Inthefollowingsections,wewilldescribetheTransformer,motivate 230.30148531860007 601.9503216 503.99736998960003 611.9129216\n",
      "self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9]. 108.0 612.8593215999999 417.3486926000001 622.8219216\n",
      "3 108.0 642.2234368 113.9776 654.1786368\n",
      "ModelArchitecture 125.9328 642.2234368 226.0934656 654.1786368\n",
      "Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35]. 108.0 668.5533216 505.743154821 678.5159216\n",
      "Here, 108.0 679.4623216 130.284941436 689.4249216\n",
      "the 133.892398896 679.4623216 146.31018204 689.4249216\n",
      "encoder 149.694078756 679.4623216 181.85634033600002 689.4249216\n",
      "maps 185.240237052 679.4623216 206.69190662400004 689.4249216\n",
      "an 210.06564148800004 679.4623216 219.65842977600005 689.4249216\n",
      "input 223.04232649200003 679.4623216 243.93509420400002 689.4249216\n",
      "sequence 247.31899092000006 679.4623216 284.5621785000001 689.4249216\n",
      "of 287.9460752160001 679.4623216 296.4108979320001 689.4249216\n",
      "symbol 299.7947946480001 679.4623216 329.7214487880001 689.4249216\n",
      "representations 333.1053455040002 679.4623216 394.62519751200006 689.4249216\n",
      "(x 398.005 679.2431444 407.5726259 689.2057444\n",
      "1 407.57300000000004 683.1461171999999 411.5445791 690.1199172\n",
      ",...,x 412.04200000000003 679.2431444 434.8912231 689.2057444\n",
      "n 434.894 683.1461171999999 439.81889756 690.1199172\n",
      ") 440.317 679.2431444 444.19145514 689.2057444\n",
      "to 447.574 679.4623216 455.479920856 689.4249216\n",
      "a 458.863817572 679.4623216 463.37567986 689.4249216\n",
      "sequence 466.75957657600003 679.4623216 504.002764156 689.4249216\n",
      "of 108.0 690.3713216 116.464822716 700.3339215999999\n",
      "continuous 120.153574992 690.3713216 164.75394342 700.3339215999999\n",
      "representations 168.442695696 690.3713216 229.96254770400006 700.3339215999999\n",
      "z 233.658 690.1521444 238.74988485999998 700.1147444000001\n",
      "= 243.74099999999999 690.1521444 251.48991027999998 700.1147444000001\n",
      "(z 256.48117288 690.1521444 264.98760526 700.1147444000001\n",
      "1 264.988 694.0551172 268.9595791 701.0289172\n",
      ",...,z 269.457 690.1521444 291.24620246 700.1147444000001\n",
      "n 291.248 694.0551172 296.17289755999997 701.0289172\n",
      "). 296.671 690.1521444 303.085463 700.3339215999999\n",
      "Given 309.771961616 690.3713216 334.211215676 700.3339215999999\n",
      "z, 337.904 690.1521444 345.53646299999997 700.3339215999999\n",
      "the 349.530070836 690.3713216 361.94785397999993 700.3339215999999\n",
      "decoder 365.636606256 690.3713216 397.79886783599994 700.3339215999999\n",
      "then 401.487620112 690.3713216 418.986329256 700.3339215999999\n",
      "generates 422.675081532 690.3713216 461.046234684 700.3339215999999\n",
      "an 464.745148812 690.3713216 474.33793710000003 700.3339215999999\n",
      "output 478.02668937600004 690.3713216 504.00038308800004 700.3339215999999\n",
      "sequence(y 108.0 701.0611444 156.56066278 711.2429216\n",
      "1 156.56099999999998 704.9641172 160.5325791 711.9379172\n",
      ",...,y 161.02999999999997 701.0611444 183.07025997999997 711.0237443999999\n",
      "m 183.07299999999998 704.9641172 190.14234105999998 711.9379172\n",
      ")ofsymbolsoneelementatatime. 190.64 701.0611444 334.0760886640001 711.2429216\n",
      "Ateachstepthemodelisauto-regressive 337.36852871200006 701.2803216 504.0025778080001 711.2429216\n",
      "[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. 108.0 712.1893216 490.5638399999998 722.1519216\n",
      "2 303.509 742.0773216 308.4903 752.0399216\n",
      "Figure1: 210.011 404.4903216 246.2649014 414.45292159999997\n",
      "TheTransformer-modelarchitecture. 249.3533074 404.4903216 401.9903020000001 414.45292159999997\n",
      "TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully 107.691 436.54232160000004 504.35307466159963 446.5049216\n",
      "connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, 108.0 447.4513216 505.2471183839997 457.4139216\n",
      "respectively. 108.0 458.3603216 157.58386019999998 468.3229216\n",
      "3.1 108.0 482.7575834 120.45325 492.7201834\n",
      "EncoderandDecoderStacks 130.41585 482.7575834 253.00564300000005 492.7201834\n",
      "Encoder: 108.0 502.7355834 148.078344288 512.6981834000001\n",
      "The 158.041 502.80532159999996 173.84267986 512.7679215999999\n",
      "encoder 177.033501388 502.80532159999996 209.195762968 512.7679215999999\n",
      "is 212.376422644 502.80532159999996 219.154377928 512.7679215999999\n",
      "composed 222.345199456 502.80532159999996 263.551509316 512.7679215999999\n",
      "of 266.74233084400004 502.80532159999996 275.20715356000005 512.7679215999999\n",
      "a 278.39797508800007 502.80532159999996 282.90983737600004 512.7679215999999\n",
      "stack 286.09049705200005 502.80532159999996 306.97310291200006 512.7679215999999\n",
      "of 310.1639244400001 502.80532159999996 318.62874715600003 512.7679215999999\n",
      "N 321.819 502.58614439999997 329.8239491 512.5487444\n",
      "= 334.969 502.58614439999997 342.71791028 512.5487444\n",
      "6 346.77268848 502.58614439999997 351.75398848 512.5487444\n",
      "identical 354.946 502.80532159999996 389.94341828800003 512.7679215999999\n",
      "layers. 393.13423981600005 502.80532159999996 419.94120539200003 512.7679215999999\n",
      "Each 425.12374991200005 502.80532159999996 445.43729206 512.7679215999999\n",
      "layer 448.61795173600007 502.80532159999996 468.9314938840001 512.7679215999999\n",
      "has 472.1223154120001 502.80532159999996 485.6680641280001 512.7679215999999\n",
      "two 488.8588856560001 502.80532159999996 504.00004513600004 512.7679215999999\n",
      "sub-layers. 108.0 513.7143216 151.827868424 523.6769216\n",
      "Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 154.9239655512 513.7143216 505.65748784059974 523.6769216\n",
      "wisefullyconnectedfeed-forwardnetwork. 107.641 524.6233216 284.6646859824001 534.5859216\n",
      "Weemployaresidualconnection[11]aroundeachof 287.77214047360013 524.6233216 504.00184042399997 534.5859216\n",
      "the 108.0 535.5323215999999 120.417783144 545.4949216\n",
      "two 123.537471708 535.5323215999999 138.68879304 545.4949216\n",
      "sub-layers, 141.808481604 535.5323215999999 186.114156324 545.4949216\n",
      "followed 189.39643452 535.5323215999999 225.26777208000004 545.4949216\n",
      "by 228.39762249600005 535.5323215999999 238.55947449600004 545.4949216\n",
      "layer 241.67916306000004 535.5323215999999 261.9927052080001 545.4949216\n",
      "normalization 265.1225556240001 535.5323215999999 321.5716434840001 545.4949216\n",
      "[1]. 324.69133204800016 535.5323215999999 338.98035971599995 545.4949216\n",
      "That 343.969829048 535.5323215999999 362.596503764 545.4949216\n",
      "is, 365.716192328 535.5323215999999 375.034610612 545.4949216\n",
      "the 378.316888808 535.5323215999999 390.734671952 545.4949216\n",
      "output 393.864522368 535.5323215999999 419.83821608 545.4949216\n",
      "of 422.957904644 535.5323215999999 431.42272735999995 545.4949216\n",
      "each 434.542415924 535.5323215999999 453.158928788 545.4949216\n",
      "sub-layer 456.28877920400004 535.5323215999999 494.101030496 545.4949216\n",
      "is 497.2207190600001 535.5323215999999 503.99867434400005 545.4949216\n",
      "LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer 108.0 546.2221444 504.16604712000003 556.4039216\n",
      "itself. 108.0 557.3503216 130.41585 567.3129216\n",
      "Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding 133.5142186 557.3503216 504.0033873999999 567.3129216\n",
      "layers,produceoutputsofdimensiond 108.0 568.0411444 263.1615333 578.2229216000001\n",
      "model 263.162 572.0975407999999 280.59649999999993 579.0713408\n",
      "=512. 283.861 568.0411444 311.81265 578.2229216000001\n",
      "Decoder: 108.0 591.3115834 145.950133676 601.2741834\n",
      "ThedecoderisalsocomposedofastackofN 155.913 591.1621444 333.80594909999996 601.3439216\n",
      "=6identicallayers. 337.659 591.1621444 417.26841275199996 601.3439216\n",
      "Inadditiontothetwo 420.32434067599996 591.3813216 503.99623303600004 601.3439216\n",
      "sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head 108.0 602.2903216 503.99721058800003 612.2529216\n",
      "attentionovertheoutputoftheencoderstack. 108.0 613.1993216 285.58553677200007 623.1619216\n",
      "Similartotheencoder,weemployresidualconnections 288.6024113040001 613.1993216 504.00139487999974 623.1619216\n",
      "aroundeachofthesub-layers,followedbylayernormalization. 108.0 624.1083216 365.6029482000001 634.0709216\n",
      "Wealsomodifytheself-attention 368.80393158000015 624.1083216 503.9972105879998 634.0709216\n",
      "sub-layer 108.0 635.0173216000001 145.812251292 644.9799216\n",
      "in 148.91161615200002 635.0173216000001 156.81753700800002 644.9799216\n",
      "the 159.92706372 635.0173216000001 172.344846864 644.9799216\n",
      "decoder 175.444211724 635.0173216000001 207.60647330400005 644.9799216\n",
      "stack 210.71600001600004 635.0173216000001 231.59860587600005 644.9799216\n",
      "to 234.69797073600006 635.0173216000001 242.60389159200008 644.9799216\n",
      "prevent 245.7134183040001 635.0173216000001 275.7925002240001 644.9799216\n",
      "positions 278.8918650840001 635.0173216000001 315.59647450800014 644.9799216\n",
      "from 318.6958393680001 635.0173216000001 338.45047965600014 644.9799216\n",
      "attending 341.56000636800013 635.0173216000001 379.38241951200007 644.9799216\n",
      "to 382.48178437200005 635.0173216000001 390.387705228 644.9799216\n",
      "subsequent 393.49723194 635.0173216000001 438.65650222799985 644.9799216\n",
      "positions. 441.75586708799983 635.0173216000001 481.00093951199955 644.9799216\n",
      "This 485.9294377319996 635.0173216000001 503.9972105879995 644.9799216\n",
      "masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe 108.0 645.9263215999999 503.99685193439973 655.8889216\n",
      "predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 108.0 656.6161443999999 456.64965 666.7979216\n",
      "3.2 108.0 681.2325834 120.45325 691.1951834\n",
      "Attention 130.41585 681.2325834 170.814193 691.1951834\n",
      "Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, 107.641 701.2803216 505.24856525199993 711.2429216\n",
      "wherethequery,keys,values,andoutputareallvectors. 107.641 712.1893216 331.43207931200016 722.1519216\n",
      "Theoutputiscomputedasaweightedsum 334.53478164300014 712.1893216 504.0037881949998 722.1519216\n",
      "3 303.509 742.0773216 308.4903 752.0399216\n",
      "ScaledDot-ProductAttention 147.783 70.94432159999997 266.2183888 80.90692160000003\n",
      "Multi-HeadAttention 363.586 70.94432159999997 450.2008444 80.90692160000003\n",
      "Figure 108.0 274.2083216 134.53259557200002 284.1709216\n",
      "2: 137.997787104 274.2083216 145.90370796 284.1709216\n",
      "(left) 150.9338247 274.2083216 171.247366848 284.1709216\n",
      "Scaled 174.702396528 274.2083216 201.79389396000002 284.1709216\n",
      "Dot-Product 205.25908549200003 274.2083216 255.49928178000005 284.1709216\n",
      "Attention. 258.964473312 274.2083216 299.89641316800004 284.1709216\n",
      "(right) 305.8919058480001 274.2083216 331.85543770800007 284.1709216\n",
      "Multi-Head 335.3206292400001 274.2083216 382.73583067200013 284.1709216\n",
      "Attention 386.20102220400014 274.2083216 424.58233720799996 284.1709216\n",
      "consists 428.04752873999996 274.2083216 460.23011402399976 284.1709216\n",
      "of 463.69530555599977 274.2083216 472.1601282719997 284.1709216\n",
      "several 475.6151579519998 274.2083216 503.9972105879998 284.1709216\n",
      "attentionlayersrunninginparallel. 108.0 285.11732159999997 247.73542760000007 295.0799216\n",
      "ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe 108.0 316.9233216 503.99499889079954 326.88592159999996\n",
      "querywiththecorrespondingkey. 108.0 327.8323216 243.51128520000006 337.79492159999995\n",
      "3.2.1 108.0 350.8225834 127.9252 360.7851834\n",
      "ScaledDot-ProductAttention 137.8878 350.8225834 263.8848022000001 360.7851834\n",
      "Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). 107.532 369.52232160000005 414.02361817200006 379.4849216\n",
      "Theinputconsistsof 418.1493300840001 369.52232160000005 503.9966557799998 379.4849216\n",
      "queriesandkeysofdimensiond 108.0 380.2121444 236.8405333 390.3939216\n",
      "k 236.84 384.11511720000004 241.07518874000002 391.0889172\n",
      ",andvaluesofdimensiond 241.742 380.2121444 351.45553329999996 390.3939216\n",
      "v 351.455 384.11511720000004 355.43982932 391.0889172\n",
      ". 356.189 380.43132160000005 358.68463130000004 390.3939216\n",
      "Wecomputethedotproductsofthe 361.7692315868 380.43132160000005 504.00025063640004 390.3939216\n",
      "querywithallkeys,divideeachby 108.0 391.34032160000004 247.55985670020004 401.3029216\n",
      "√ 250.049 383.3781444 258.35083458 393.3407444\n",
      "d 258.351 391.1211444 263.5365333 401.0837444\n",
      "k 263.536 395.02411720000003 267.77118874 401.9979172\n",
      ",andapplyasoftmaxfunctiontoobtaintheweightsonthe 268.438 391.34032160000004 503.9973009360001 401.3029216\n",
      "values. 107.751 402.24932160000003 135.447028 412.2119216\n",
      "Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether 108.0 418.63832160000004 504.16996207199975 428.6009216\n",
      "intoamatrixQ. 108.0 429.3281444 171.91212195 439.5099216\n",
      "ThekeysandvaluesarealsopackedtogetherintomatricesK 174.9998006802 429.3281444 420.29823618 439.5099216\n",
      "andV. 423.498 429.3281444 450.93812195 439.5099216\n",
      "Wecompute 454.0258006802 429.54732160000003 503.998232168 439.5099216\n",
      "thematrixofoutputsas: 108.0 440.4563216 204.85639720000003 450.4189216\n",
      "Attention(Q,K,V)=softmax( 219.97 472.7421444 354.41799616000003 482.7047444\n",
      "QKT 355.609 464.7961172 377.36492024 475.9657444\n",
      "√ 358.077 472.4431444 366.37883458 482.4057444\n",
      "d 366.379 480.18714439999997 371.5645333 490.1497444\n",
      "k 371.565 484.0901172 375.80018874 491.0639172\n",
      ")V 380.13 472.7421444 389.81618457999997 482.7047444\n",
      "(1) 493.051 472.9613216 504.6673916 482.92392159999997\n",
      "Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- 107.691 499.26332160000004 505.652786382 509.2259216\n",
      "plicative)attention. 108.0 510.17232160000003 185.23479747240003 520.1349216\n",
      "Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor 188.3393628096 510.17232160000003 504.1664950871999 520.1349216\n",
      "of 108.0 521.0813216 116.3735354122 531.0439216\n",
      "1 125.595 519.3481171999999 129.5665791 526.3219171999999\n",
      "√ 120.056 522.0551172 126.60091129999999 529.0289172\n",
      "dk 126.601 527.4771172000001 134.52532353 535.1223722\n",
      ". 136.299 521.0813216 138.81206585 531.0439216\n",
      "Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith 141.8981107138 521.0813216 504.0007429085999 531.0439216\n",
      "asinglehiddenlayer. 108.0 534.2513216 194.355418296 544.2139216\n",
      "Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis 197.57672538000003 534.2513216 503.9972105879997 544.2139216\n",
      "muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized 108.0 545.1603216 504.0002989939997 555.1229216\n",
      "matrixmultiplicationcode. 108.0 556.0693216 216.20379860000006 566.0319216\n",
      "Whileforsmallvaluesofd 107.532 572.2391444 216.74753330000001 582.4209216\n",
      "k 216.747 576.1421172 220.98218874000003 583.1159172\n",
      "thetwomechanismsperformsimilarly,additiveattentionoutperforms 224.134 572.4583216 503.9988162544 582.4209216\n",
      "dotproductattentionwithoutscalingforlargervaluesofd 108.0 583.1481444 341.5725333 593.3299216\n",
      "k 341.573 587.0511172 345.80818874 594.0249172\n",
      "[3]. 348.972 583.3673216 363.125236779 593.3299216\n",
      "Wesuspectthatforlargevaluesof 366.239097222 583.3673216 503.99988768900005 593.3299216\n",
      "d 108.0 594.0571444 113.1855333 604.0197444\n",
      "k 113.185 597.9611172 117.42018874 604.9349172\n",
      ",thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas 118.087 594.2763216 503.9982337495996 604.2389216\n",
      "extremelysmallgradients4. 108.0 603.9135408 219.74965 615.1479216\n",
      "Tocounteractthiseffect,wescalethedotproductsby 222.83805600000002 605.1853216 437.04391860000004 615.1479216\n",
      "1 446.26700000000005 603.4531172 450.23857910000004 610.4269172\n",
      "√ 440.729 606.1601172000001 447.2739113 613.1339172\n",
      "dk 447.274 611.5821172 455.19732353 619.2263722\n",
      ". 456.972 605.1853216 459.46265 615.1479216\n",
      "3.2.2 108.0 630.5465834 127.9252 640.5091834\n",
      "Multi-HeadAttention 137.8878 630.5465834 230.58979300000004 640.5091834\n",
      "Insteadofperformingasingleattentionfunctionwithd 108.0 649.0271444 331.85653329999997 659.2089215999999\n",
      "model 331.85699999999997 653.0835408 349.2914999999999 660.0573408\n",
      "-dimensionalkeys,valuesandqueries, 349.789 649.2463216 505.2413755470001 659.2089215999999\n",
      "wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned 107.641 659.9361444 503.995565807 670.1179216\n",
      "linearprojectionstod 108.0 670.8451444 195.6575333 681.0269216\n",
      "k 195.65800000000002 674.7481172 199.89318874000003 681.7219172\n",
      ",d 200.56 670.8451444 210.7345333 681.0269216\n",
      "k 210.734 674.7481172 214.96918874000002 681.7219172\n",
      "andd 218.124 670.8451444 240.24153330000001 681.0269216\n",
      "v 240.241 674.7481172 244.22582932 681.7219172\n",
      "dimensions,respectively. 247.463 671.0643216 347.80758241280006 681.0269216\n",
      "Oneachoftheseprojectedversionsof 350.8983395864 671.0643216 503.9958454088001 681.0269216\n",
      "queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd 108.0 681.7541444 447.3985333 691.9359216\n",
      "v 447.398 685.6581172 451.38282932000004 692.6319172\n",
      "-dimensional 452.132 681.9733216 504.00062310839996 691.9359216\n",
      "4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom 120.653 700.1545616 504.00192160000006 710.5867424\n",
      "variableswithmean0andvariance1.Thentheirdotproduct,q·k= 107.776 712.7730816 356.29574016000004 721.9367424\n",
      "(cid:80)dk 358.85600000000005 709.68844 376.19932353 718.65484\n",
      "i=1 368.583 717.9220544 380.49016288 723.8996544\n",
      "q 382.524 712.7730816000001 386.64137088 721.7394816\n",
      "i 386.641 716.1780544000001 389.30461856 722.1556544\n",
      "k 389.803 712.7730816000001 394.60092063999997 721.7394816\n",
      "i 394.601 716.1780544000001 397.26461856 722.1556544\n",
      ",hasmean0andvarianced 397.763 712.7730816 497.070608 721.9367424\n",
      "k 497.07 716.2450544 501.02059584 722.2226544\n",
      ". 501.643 712.9703423999999 503.87787519999995 721.9367424\n",
      "4 303.509 742.0773216 308.4903 752.0399216\n",
      "output 108.0 74.15232159999994 133.973693712 84.1149216\n",
      "values. 137.05273486800002 74.15232159999994 165.31284527999998 84.1149216\n",
      "These 170.170210536 74.15232159999994 194.43671311200004 84.1149216\n",
      "are 197.51575426800002 74.15232159999994 209.92337556000004 84.1149216\n",
      "concatenated 213.00241671600006 74.15232159999994 266.04728415600005 84.1149216\n",
      "and 269.12632531200006 74.15232159999994 283.8000396000001 84.1149216\n",
      "once 286.8790807560001 74.15232159999994 306.06465733200014 84.1149216\n",
      "again 309.15386034000016 74.15232159999994 331.1136225120001 84.1149216\n",
      "projected, 334.19266366800014 74.15232159999994 374.54537796000017 84.1149216\n",
      "resulting 377.76668504400016 74.15232159999994 413.333167044 84.1149216\n",
      "in 416.422370052 74.15232159999994 424.328290908 84.1149216\n",
      "the 427.40733206399995 74.15232159999994 439.8251152079999 84.1149216\n",
      "final 442.9041563639999 74.15232159999994 460.9719292199999 84.1149216\n",
      "values, 464.0509703759999 74.15232159999994 492.3110807879998 84.1149216\n",
      "as 495.5323878719998 74.15232159999994 503.9972105879998 84.1149216\n",
      "depictedinFigure2. 108.0 85.06132159999993 190.46044020000002 95.0239216\n",
      "Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation 108.0 101.44932159999996 503.9970113359999 111.41192160000003\n",
      "subspacesatdifferentpositions. 108.0 112.35832159999995 235.03311260000007 122.32092160000002\n",
      "Withasingleattentionhead,averaginginhibitsthis. 238.12151860000006 112.35832159999995 445.19415959999975 122.32092160000002\n",
      "MultiHead(Q,K,V)=Concat(head 186.94 147.35214439999993 343.19479158 157.3147444\n",
      "1 343.18999999999994 151.25611719999995 347.1615790999999 158.22991719999993\n",
      ",...,head 347.65999999999997 147.35214439999993 385.29612055999996 157.3147444\n",
      "h 385.29599999999994 151.25611719999995 389.69646779999994 158.22991719999993\n",
      ")WO 390.19399999999996 145.6481171999999 410.92153482 157.3147444\n",
      "wherehead 224.49699999999996 164.16314439999985 271.80512056 174.3449215999999\n",
      "i 271.80499999999995 168.06711719999987 274.06032691999997 175.04091719999985\n",
      "=Attention(QWQ 277.32599999999996 161.78511719999983 358.44732786 174.12574439999992\n",
      "i 350.78899999999993 169.3491171999998 353.60780995999994 176.32291719999978\n",
      ",KWK 358.94499999999994 162.4591171999997 390.03415061999993 174.1257443999998\n",
      "i 381.9559999999999 169.03511719999972 384.7748099599999 176.0089171999997\n",
      ",VWV 391.0209999999999 162.4591171999997 418.97152547999985 174.1257443999998\n",
      "i 412.88299999999987 169.03511719999972 415.7018099599999 176.0089171999997\n",
      ") 421.18599999999986 164.16314439999974 425.0604551399999 174.1257443999998\n",
      "WheretheprojectionsareparametermatricesWQ 107.532 202.90011719999995 302.96132786 215.45992160000003\n",
      "i 295.304 210.46411719999992 298.12280996 217.4379171999999\n",
      "∈Rdmodel×dk,WK 306.227 203.34539999999993 377.31615062000003 215.45992160000003\n",
      "i 369.238 210.28011719999995 372.05680996 217.25391719999993\n",
      "∈Rdmodel×dk,WV 381.07 203.34540000000004 450.16952548 215.45992160000003\n",
      "i 444.081 210.28011719999995 446.89980996 217.25391719999993\n",
      "∈Rdmodel×dv 455.15000000000003 203.34540000000004 502.82449486 215.24074440000004\n",
      "andWO 108.00000000000006 216.30311719999997 141.72953482000005 227.6919216\n",
      "∈Rhdv×dmodel. 145.20900000000003 215.5774 201.21365000000003 227.6919216\n",
      "In 108.0 234.11732159999997 116.464822716 244.07992160000003\n",
      "this 120.366973884 234.11732159999997 135.05085002400003 244.07992160000003\n",
      "work 138.963163044 234.11732159999997 159.744150384 244.07992160000003\n",
      "we 163.646301552 234.11732159999997 175.495020984 244.07992160000003\n",
      "employ 179.407334004 234.11732159999997 209.79127148400002 244.07992160000003\n",
      "h 213.695 233.89814439999998 219.43545011999998 243.86074440000004\n",
      "= 224.822 233.89814439999998 232.57091028 243.86074440000004\n",
      "8 237.96067688 233.89814439999998 242.94197688 243.86074440000004\n",
      "parallel 246.846 234.11732159999997 277.32139414799997 244.07992160000003\n",
      "attention 281.223545316 234.11732159999997 316.79002731599996 244.07992160000003\n",
      "layers, 320.702340336 234.11732159999997 347.509305912 244.07992160000003\n",
      "or 351.76712190000006 234.11732159999997 360.231944616 244.07992160000003\n",
      "heads. 364.13409578400007 234.11732159999997 389.81309578800006 244.07992160000003\n",
      "For 397.1499529320001 234.11732159999997 411.11233758000003 244.07992160000003\n",
      "each 415.0144887480001 234.11732159999997 433.6310016120001 244.07992160000003\n",
      "of 437.5433146320001 234.11732159999997 446.0081373480001 244.07992160000003\n",
      "these 449.91028851600015 234.11732159999997 470.7928943760001 244.07992160000003\n",
      "we 474.69504554400015 234.11732159999997 486.54376497600015 244.07992160000003\n",
      "use 490.45607799600015 234.11732159999997 504.00182671200014 244.07992160000003\n",
      "d 108.0 244.80714439999997 113.1855333 254.76974440000004\n",
      "k 113.185 248.7111172 117.42018874 255.68491719999997\n",
      "=d 120.85900000000001 244.80714439999997 136.56453330000002 254.76974440000004\n",
      "v 136.56400000000002 248.7111172 140.54882932 255.68491719999997\n",
      "=d 144.06900000000002 244.80714439999997 159.77453330000003 254.76974440000004\n",
      "model 159.77500000000003 248.8645408 177.20950000000005 255.83834079999997\n",
      "/h=64. 177.70700000000002 244.80714439999997 214.1636874 254.98892160000003\n",
      "Duetothereduceddimensionofeachhead,thetotalcomputationalcost 217.25958527519998 245.02632159999996 503.9972804664001 254.98892160000003\n",
      "issimilartothatofsingle-headattentionwithfulldimensionality. 108.0 255.93632159999993 369.1496338000002 265.8989216\n",
      "3.2.3 108.0 280.0385834 127.9252 290.0011834\n",
      "ApplicationsofAttentioninourModel 137.8878 280.0385834 302.8584934 290.0011834\n",
      "TheTransformerusesmulti-headattentioninthreedifferentways: 107.691 299.1623215999999 372.60649660000007 309.12492159999994\n",
      "• 135.397 320.0063216 138.88391 329.96892159999993\n",
      "In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, 143.866 320.0063216 505.24178082399993 329.9689216\n",
      "andthememorykeysandvaluescomefromtheoutputoftheencoder. 143.866 330.91632159999995 429.43436490400023 340.87892159999996\n",
      "Thisallowsevery 432.71664310000017 330.91632159999995 504.3475378480001 340.87892159999996\n",
      "positioninthedecodertoattendoverallpositionsintheinputsequence. 143.866 341.82532160000005 435.85641626560016 351.7879216\n",
      "Thismimicsthe 438.9577537204001 341.82532160000005 503.99492158359993 351.7879216\n",
      "typical 143.866 352.73432160000004 171.526561144 362.6969216\n",
      "encoder-decoder 174.72754452400002 352.73432160000004 242.24288921200005 362.6969216\n",
      "attention 245.44387259200005 352.73432160000004 281.01035459200006 362.6969216\n",
      "mechanisms 284.22149982400003 352.73432160000004 334.4616961120001 362.6969216\n",
      "in 337.6626794920001 352.73432160000004 345.5686003480001 362.6969216\n",
      "sequence-to-sequence 348.7695837280001 352.73432160000004 437.92967317600005 362.6969216\n",
      "models 441.1408184080001 352.73432160000004 470.49840883600007 362.6969216\n",
      "such 473.699392216 352.73432160000004 492.3260669319999 362.6969216\n",
      "as 495.5372121639999 352.73432160000004 504.0020348799999 362.6969216\n",
      "[38,2,9]. 143.866 363.64332160000004 182.8795416 373.6059216\n",
      "• 135.39700000000002 379.47932160000005 138.88391000000001 389.4419216\n",
      "Theencodercontainsself-attentionlayers. 143.557 379.47932160000005 315.94265732800005 389.4419216\n",
      "Inaself-attentionlayerallofthekeys,values 319.15380256000003 379.47932160000005 503.99789044 389.4419216\n",
      "andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe 143.866 390.38832160000004 504.0004408639998 400.3509216\n",
      "encoder. 143.866 401.29732160000003 176.96226529260002 411.2599216\n",
      "Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe 180.0659638836 401.29732160000003 504.00341968139986 411.2599216\n",
      "encoder. 143.866 412.2063216 177.340336 422.1689216\n",
      "• 135.39700000000002 428.04232160000004 138.88391000000001 438.0049216\n",
      "Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto 143.866 428.04232160000004 503.99919553899986 438.0049216\n",
      "allpositionsinthedecoderuptoandincludingthatposition. 143.866 438.9513216 387.6938604320002 448.9139216\n",
      "Weneedtopreventleftward 390.80308826600015 438.9513216 504.003130766 448.9139216\n",
      "informationflowinthedecodertopreservetheauto-regressiveproperty. 143.866 449.8603216 427.0961725718001 459.8229216\n",
      "Weimplementthis 430.1810418488001 449.8603216 504.0024533091998 459.8229216\n",
      "insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput 143.866 460.55014439999997 503.995694816 470.73192159999996\n",
      "ofthesoftmaxwhichcorrespondtoillegalconnections. 143.866 471.6783216 365.02575740000015 481.64092159999996\n",
      "SeeFigure2. 368.11416340000005 471.6783216 420.9657564000001 481.64092159999996\n",
      "3.3 108.00000000000001 497.12658339999996 120.45325000000003 507.08918339999997\n",
      "Position-wiseFeed-ForwardNetworks 130.41585 497.12658339999996 292.86600560000005 507.08918339999997\n",
      "Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully 108.0 517.5953216 504.34271355599975 527.5579216\n",
      "connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. 108.0 528.5043216 483.01979066379977 538.4669216\n",
      "This 486.12594005439985 528.5043216 503.99886437959975 538.4669216\n",
      "consistsoftwolineartransformationswithaReLUactivationinbetween. 108.0 539.4133216 401.53804640000004 549.3759216\n",
      "FFN(x)=max(0,xW 226.901 567.9041444 321.03666982000004 577.8667444\n",
      "1 321.03400000000005 571.8071172 325.00557910000003 578.7809172\n",
      "+b 327.71700000000004 567.9041444 341.9559479200001 577.8667444\n",
      "1 341.95500000000004 571.8071172 345.9265791 578.7809172\n",
      ")W 346.42500000000007 567.9041444 359.7076794400001 577.8667444\n",
      "2 359.7080000000001 571.8071172 363.67957910000007 578.7809172\n",
      "+b 366.3920000000001 567.9041444 380.6299479200001 577.8667444\n",
      "2 380.6300000000001 571.8071172 384.6015791000001 578.7809172\n",
      "(2) 493.0510000000001 568.1233216 504.6673916000001 578.0859216\n",
      "Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters 107.532 589.9093216 504.0020355839999 599.8719216\n",
      "from 108.0 600.8183216 127.75464028799999 610.7809216\n",
      "layer 131.484039972 600.8183216 151.79758212 610.7809216\n",
      "to 155.526981804 600.8183216 163.43290266 610.7809216\n",
      "layer. 167.16230234399998 600.8183216 189.467567484 610.7809216\n",
      "Another 196.286170176 600.8183216 229.57639732800004 610.7809216\n",
      "way 233.31595886400004 600.8183216 250.14398577600005 610.7809216\n",
      "of 253.87338546000007 600.8183216 262.3382081760001 610.7809216\n",
      "describing 266.06760786000007 600.8183216 308.4018832920001 610.7809216\n",
      "this 312.13128297600014 600.8183216 326.81515911600013 610.7809216\n",
      "is 330.5445588000002 600.8183216 337.32251408400015 610.7809216\n",
      "as 341.0519137680002 600.8183216 349.51673648400015 610.7809216\n",
      "two 353.2562980200002 600.8183216 368.3974575000002 610.7809216\n",
      "convolutions 372.12685718400024 600.8183216 424.0335972 610.7809216\n",
      "with 427.76299688399996 600.8183216 445.8307697399998 610.7809216\n",
      "kernel 449.5601694239998 600.8183216 474.86318090399976 610.7809216\n",
      "size 478.59258058799975 600.8183216 494.39426044799967 610.7809216\n",
      "1. 498.12366013199966 600.8183216 505.7450491319996 610.7809216\n",
      "The 107.691 611.7273216 123.49267986000001 621.6899216\n",
      "dimensionality 126.886738428 611.7273216 187.29894856800001 621.6899216\n",
      "of 190.70316898800002 611.7273216 199.167991704 621.6899216\n",
      "input 202.56205027200002 611.7273216 223.45481798400002 621.6899216\n",
      "and 226.84887655200004 611.7273216 241.52259084000005 621.6899216\n",
      "output 244.91664940800004 611.7273216 270.89034312000007 621.6899216\n",
      "is 274.29456354000007 611.7273216 281.07251882400004 621.6899216\n",
      "d 284.466 611.5081444 289.6515333 621.4707444000001\n",
      "model 289.651 615.5645408 307.08549999999997 622.5383408\n",
      "= 312.029 611.5081444 319.77791028 621.4707444000001\n",
      "512, 324.22122988 611.5081444 341.708463 621.6899216\n",
      "and 345.336244164 611.7273216 360.009958452 621.6899216\n",
      "the 363.40401702 611.7273216 375.82180016399997 621.6899216\n",
      "inner-layer 379.215858732 611.7273216 423.592666416 621.6899216\n",
      "has 426.99688683600004 611.7273216 440.54263555200004 621.6899216\n",
      "dimensionality 443.93669412 611.7273216 504.34890426000004 621.6899216\n",
      "d 108.0 622.4171444 113.1855333 632.3797443999999\n",
      "ff 113.185 626.3211172 121.73627356 633.2949172\n",
      "=2048. 125.784 622.4171444 158.71564999999998 632.5989216\n",
      "3.4 108.0 648.0845834 120.45325 658.0471834\n",
      "EmbeddingsandSoftmax 130.41585 648.0845834 240.02437520000004 658.0471834\n",
      "Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput 108.0 668.5533216 503.9972105879998 678.5159216\n",
      "tokensandoutputtokenstovectorsofdimensiond 108.0 679.2431444 306.8505333 689.4249216\n",
      "model 306.85 683.2995408 324.2845 690.2733408\n",
      ". 324.783 679.4623216 327.223837 689.4249216\n",
      "Wealsousetheusuallearnedlineartransfor- 330.28952827200004 679.4623216 505.64902170000016 689.4249216\n",
      "mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. 108.0 690.3713216 492.5442056279997 700.3339215999999\n",
      "In 495.66497015279975 690.3713216 503.99701133599973 700.3339215999999\n",
      "ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-softmax 108.0 701.2803216 504.2503104409998 711.2429216\n",
      "lineartransformation,similarto[30]. 108.0 712.1893216 253.21503188399998 722.1519216\n",
      "Intheembeddinglayers,wemultiplythoseweightsby 256.26119646 712.1893216 469.5317701720001 722.1519216\n",
      "√ 471.882 704.2621444 480.18383458 714.2247444\n",
      "d 480.185 711.9701444 485.3705333 721.9327444\n",
      "model 485.37 716.0265408 502.80449999999996 723.0003408\n",
      ". 503.303 712.1893216 505.743837 722.1519216\n",
      "5 303.509 742.0773216 308.4903 752.0399216\n",
      "Table1: 107.691 70.94432159999997 138.91418690400002 80.90692160000003\n",
      "Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations 144.78195905200002 70.94432159999997 503.9950586679997 80.90692160000003\n",
      "fordifferentlayertypes. 108.0 81.85332159999996 205.02028197 91.81592160000002\n",
      "nisthesequencelength,distherepresentationdimension,kisthekernel 208.114 81.63414439999997 504.003525788 91.81592160000002\n",
      "sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. 108.0 92.54314439999996 435.0811038000001 102.72492160000002\n",
      "LayerType 124.547 116.38332159999993 169.9465682 126.3459216\n",
      "ComplexityperLayer 239.7046934 116.38332159999993 327.5449376 126.3459216\n",
      "Sequential 340.32695340000004 116.38332159999993 382.3890506000001 126.3459216\n",
      "MaximumPathLength 395.1710664000001 116.38332159999993 487.45463019999994 126.3459216\n",
      "Operations 339.499 127.29232159999992 383.2148888 137.2549216\n",
      "Self-Attention 124.547 139.92932159999998 181.55299720000002 149.89192160000005\n",
      "O(n2·d) 264.39599999999996 138.5041172 302.8514551399999 149.67274440000006\n",
      "O(1) 351.0539999999999 139.7101444 371.6602102799999 149.67274440000006\n",
      "O(1) 431.0079999999999 139.7101444 451.6142102799999 149.67274440000006\n",
      "Recurrent 124.54699999999991 151.31232160000002 163.8295317999999 161.27492160000008\n",
      "O(n·d2) 264.3959999999999 149.88611720000006 302.8514551399999 161.0557444000001\n",
      "O(n) 350.5539999999999 151.09314440000003 372.1594551399999 161.0557444000001\n",
      "O(n) 430.5089999999999 151.09314440000003 452.1134551399999 161.0557444000001\n",
      "Convolutional 124.54699999999991 162.69432159999997 180.9652037999999 172.65692160000003\n",
      "O(k·n·d2) 258.0489999999999 161.26911719999998 309.19845513999996 172.43774440000004\n",
      "O(1) 351.054 162.47514439999998 371.66021027999994 172.43774440000004\n",
      "O(log 417.80899999999997 162.47514439999998 442.31212425999996 172.43774440000004\n",
      "k 442.30899999999997 166.3791172 446.54418874 173.35291719999998\n",
      "(n)) 447.21099999999996 162.47514439999998 464.81391028 172.43774440000004\n",
      "Self-Attention(restricted) 124.54700000000003 173.60332159999996 227.7495734000001 183.56592160000002\n",
      "O(r·n·d) 260.648 173.38414439999997 306.59945514000003 183.34674440000003\n",
      "O(1) 351.05400000000003 173.38414439999997 371.66021028 183.34674440000003\n",
      "O(n/r) 425.63300000000004 173.38414439999997 456.99045514000005 183.34674440000003\n",
      "3.5 108.0 213.55658340000002 120.45325 223.51918339999997\n",
      "PositionalEncoding 130.41585 213.55658340000002 215.19757600000003 223.51918339999997\n",
      "Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe 108.0 233.6013216 504.0013948799996 243.56392160000007\n",
      "orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe 108.0 244.5103216 503.9984060999996 254.47292160000006\n",
      "tokensinthesequence. 108.0 255.4193216 202.078425816 265.38192160000006\n",
      "Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe 205.23876178800003 255.4193216 503.9972105879996 265.38192160000006\n",
      "bottomsoftheencoderanddecoderstacks. 108.0 266.3283216 276.5153864800001 276.29092160000005\n",
      "Thepositionalencodingshavethesamedimensiond 279.59084110000003 266.1091444 486.0675333 276.29092160000005\n",
      "model 486.067 270.1665408 503.50149999999996 277.1403408\n",
      "astheembeddings,sothatthetwocanbesummed. 108.0 277.2373216 310.6354882494001 287.1999216\n",
      "Therearemanychoicesofpositionalencodings, 313.7517397164001 277.2373216 505.24786557899967 287.1999216\n",
      "learnedandfixed[9]. 108.0 288.14632159999996 193.0606788 298.1089216\n",
      "Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: 108.0 304.5353216 389.86187920000003 314.4979216\n",
      "PE 235.50900000000001 337.7001444 250.64418192 347.6627444\n",
      "(pos,2i) 250.64300000000003 341.9081172 277.82510432 348.8819172\n",
      "=sin(pos/100002i/dmodel) 281.091 335.9961172 386.30645514 347.6627444\n",
      "PE 225.69400000000002 354.5711444 240.82918192 364.5337444\n",
      "(pos,2i+1) 240.82800000000003 358.7791172 278.09770602000003 365.75291719999996\n",
      "=cos(pos/100002i/dmodel) 281.36300000000006 352.8671172 386.3064551400001 364.5337444\n",
      "whereposisthepositionandiisthedimension. 107.641 377.0471444 300.25669750699996 387.2289216\n",
      "Thatis,eachdimensionofthepositionalencoding 303.342015101 377.26632159999997 503.99713772240005 387.2289216\n",
      "correspondstoasinusoid. 108.0 388.17532159999996 209.34355224 398.13792159999997\n",
      "Thewavelengthsformageometricprogressionfrom2πto10000·2π. 212.40924351200005 387.95614439999997 488.182837 398.13792159999997\n",
      "We 491.24852827200004 388.17532159999996 503.99946076 398.13792159999997\n",
      "chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby 108.0 399.08432159999995 504.3427135559997 409.04692159999996\n",
      "relativepositions,sinceforanyfixedoffsetk,PE 108.0 409.77414439999995 311.53018191999996 419.95592159999995\n",
      "pos+k 311.529 413.6771172 333.67918874 420.6509172\n",
      "canberepresentedasalinearfunctionof 336.93 409.99332159999994 504.00100873200006 419.95592159999995\n",
      "PE 108.0 420.6831444 123.13518192000001 430.6457444\n",
      "pos 123.134 424.58611720000005 134.9336696 431.55991720000003\n",
      ". 135.432 420.90232160000005 137.92264999999998 430.8649216\n",
      "Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo 107.532 437.29132159999995 503.99586547819996 447.25392159999996\n",
      "versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion 107.751 448.20032160000005 504.0022568879998 458.1629216\n",
      "becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered 108.0 459.10932160000004 503.9973699895998 469.0719216\n",
      "duringtraining. 108.0 470.01832160000004 169.98729719999997 479.9809216\n",
      "4 108.0 496.6064368 113.9776 508.5616368\n",
      "WhySelf-Attention 125.9328 496.6064368 225.041408 508.5616368\n",
      "In 108.0 521.2053216 116.464822716 531.1679216\n",
      "this 119.777586468 521.2053216 134.461462608 531.1679216\n",
      "section 137.764064508 521.2053216 166.552591224 531.1679216\n",
      "we 169.865354976 521.2053216 181.71407440800002 531.1679216\n",
      "compare 185.016676308 521.2053216 220.00393274400002 531.1679216\n",
      "various 223.31669649600002 521.2053216 252.97914248400005 531.1679216\n",
      "aspects 256.29190623600005 521.2053216 285.639334812 531.1679216\n",
      "of 288.94193671200003 521.2053216 297.40675942800004 531.1679216\n",
      "self-attention 300.71952318000007 521.2053216 354.3436161840001 531.1679216\n",
      "layers 357.6462180840001 521.2053216 381.9127206600001 531.1679216\n",
      "to 385.22548441200007 521.2053216 393.13140526800004 531.1679216\n",
      "the 396.43400716800005 521.2053216 408.85179031199993 531.1679216\n",
      "recurrent 412.16455406399996 521.2053216 448.83867793199994 531.1679216\n",
      "and 452.14127983199995 521.2053216 466.8149941199999 531.1679216\n",
      "convolu- 470.12775787199985 521.2053216 505.6535924639997 531.1679216\n",
      "tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations 108.0 532.1143216 504.0030685967998 542.0769216\n",
      "(x 106.834 542.8041444 116.4026259 552.7667444\n",
      "1 116.403 546.7081172000001 120.3745791 553.6819172\n",
      ",...,x 120.872 542.8041444 143.7212231 552.7667444\n",
      "n 143.724 546.7081172000001 148.64889756 553.6819172\n",
      ") 149.147 542.8041444 153.02145514 552.7667444\n",
      "to 156.396 543.0233216 164.30192085599998 552.9859216\n",
      "another 167.67565571999998 543.0233216 198.151049868 552.9859216\n",
      "sequence 201.524784732 543.0233216 238.767972312 552.9859216\n",
      "of 242.141707176 543.0233216 250.606529892 552.9859216\n",
      "equal 253.980264756 543.0233216 275.990836188 552.9859216\n",
      "length 279.37473290400004 543.0233216 304.77936290400004 552.9859216\n",
      "(z 308.15 542.8041444 316.65760526 552.7667444\n",
      "1 316.657 546.7081172000001 320.62857909999997 553.6819172\n",
      ",...,z 321.127 542.8041444 342.91620246 552.7667444\n",
      "n 342.918 546.7081172000001 347.84289756 553.6819172\n",
      "), 348.341 542.8041444 354.75546299999996 552.9859216\n",
      "with 358.352758608 543.0233216 376.42053146399996 552.9859216\n",
      "x 379.794 542.8041444 385.48762589999995 552.7667444\n",
      "i 385.488 546.7081172000001 388.30680996 553.6819172\n",
      ",z 388.805 542.8041444 397.86996974000004 552.7667444\n",
      "i 397.866 546.7081172000001 400.68480996 553.6819172\n",
      "∈ 405.58799999999997 542.8041444 412.23006541999996 552.7667444\n",
      "Rd, 416.63399999999996 540.8714 431.015463 552.9859216\n",
      "such 434.61275860800004 543.0233216 453.239433324 552.9859216\n",
      "as 456.61316818800003 543.0233216 465.077990904 552.9859216\n",
      "a 468.451725768 543.0233216 472.963588056 552.9859216\n",
      "hidden 476.33732292 543.0233216 503.997884064 552.9859216\n",
      "layerinatypicalsequencetransductionencoderordecoder. 108.0 553.9323216 345.8387238908002 563.8949216\n",
      "Motivatingouruseofself-attentionwe 348.95078110320014 553.9323216 503.9968519343998 563.8949216\n",
      "considerthreedesiderata. 108.0 564.8423216 209.5288566 574.8049216\n",
      "Oneisthetotalcomputationalcomplexityperlayer. 108.0 581.2303216 315.04037213860005 591.1929216\n",
      "Anotheristheamountofcomputationthatcan 318.14183910720004 581.2303216 504.0004882833997 591.1929216\n",
      "beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. 108.0 592.1393216 457.4581201999999 602.1019216\n",
      "Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. 107.691 608.5283216 418.4079461214 618.4909216\n",
      "Learninglong-range 421.5101802102 608.5283216 504.00140484419984 618.4909216\n",
      "dependenciesisakeychallengeinmanysequencetransductiontasks. 108.0 619.4373216 388.4955285352001 629.3999216\n",
      "Onekeyfactoraffectingthe 391.6008311048001 619.4373216 503.9966526823998 629.3999216\n",
      "abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto 108.0 630.3463216 503.9972105879999 640.3089216000001\n",
      "traverseinthenetwork. 108.0 641.2553216 202.911598054 651.2179216\n",
      "Theshorterthesepathsbetweenanycombinationofpositionsintheinput 206.005881988 641.2553216 503.99755927899946 651.2179216\n",
      "andoutputsequences,theeasieritistolearnlong-rangedependencies[12]. 108.0 652.1643216 406.8399220378 662.1269216000001\n",
      "Hencewealsocompare 409.9301511936 652.1643216 503.9994911184 662.1269216000001\n",
      "themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe 108.0 663.0733216 504.00048828339965 673.0359215999999\n",
      "differentlayertypes. 108.0 673.9823216 190.47040280000002 683.9449216\n",
      "AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially 107.641 690.3713216 504.34535593599963 700.3339215999999\n",
      "executed 108.0 701.2803216 143.8205283 711.2429216\n",
      "operations, 147.011349828 701.2803216 191.88608826 711.2429216\n",
      "whereas 195.269984976 701.2803216 228.56021212800005 711.2429216\n",
      "a 231.76119550800004 701.2803216 236.27305779600005 711.2429216\n",
      "recurrent 239.47404117600004 701.2803216 276.14816504400005 711.2429216\n",
      "layer 279.33898657200007 701.2803216 299.6525287200001 711.2429216\n",
      "requires 302.8535121000001 701.2803216 335.5848373920001 711.2429216\n",
      "O(n) 338.789 701.0611444 360.39345514 711.0237443999999\n",
      "sequential 363.594 701.2803216 404.80030985999997 711.2429216\n",
      "operations. 408.00129324 701.2803216 452.876031672 711.2429216\n",
      "In 458.089061748 701.2803216 466.553884464 711.2429216\n",
      "terms 469.75486784400005 701.2803216 492.33450298800005 711.2429216\n",
      "of 495.5354863680001 701.2803216 504.00030908400004 711.2429216\n",
      "computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence 108.0 712.1893216 503.9972105879998 722.1519216\n",
      "6 303.509 742.0773216 308.4903 752.0399216\n",
      "length 108.0 74.15232159999994 133.40463 84.1149216\n",
      "n 136.86 73.93314439999995 142.83955252 83.89574440000001\n",
      "is 146.295 74.15232159999994 153.072955284 84.1149216\n",
      "smaller 156.52798496399998 74.15232159999994 186.44447725199998 84.1149216\n",
      "than 189.89950693199998 74.15232159999994 207.39821607599998 84.1149216\n",
      "the 210.85324575599998 74.15232159999994 223.2710289 84.1149216\n",
      "representation 226.72605857999997 74.15232159999994 284.29295016000003 84.1149216\n",
      "dimensionality 287.74797984 74.15232159999994 348.16018998000004 84.1149216\n",
      "d, 351.613 73.93314439999995 359.33946299999997 84.1149216\n",
      "which 363.038377128 74.15232159999994 387.873943416 84.1149216\n",
      "is 391.32897309599997 74.15232159999994 398.10692837999994 84.1149216\n",
      "most 401.56195806 74.15232159999994 421.32676019999997 84.1149216\n",
      "often 424.78178987999996 74.15232159999994 445.66439574 84.1149216\n",
      "the 449.11942541999997 74.15232159999994 461.53720856399997 84.1149216\n",
      "case 464.992238244 74.15232159999994 482.480785536 84.1149216\n",
      "with 485.93581521600004 74.15232159999994 504.00358807199996 84.1149216\n",
      "sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece 108.0 85.06132159999993 504.0034870259998 95.0239216\n",
      "[38]andbyte-pair[31]representations. 108.0 95.97032159999992 265.0290336304 105.93292159999999\n",
      "Toimprovecomputationalperformancefortasksinvolving 268.1159253152 95.97032159999992 504.00255743679986 105.93292159999999\n",
      "verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin 107.751 106.66014440000004 503.999884744 116.84192160000009\n",
      "theinputsequencecenteredaroundtherespectiveoutputposition. 108.0 117.78832160000002 365.80120394000016 127.75092160000008\n",
      "Thiswouldincreasethemaximum 368.88642190800016 117.78832160000002 504.00139487999974 127.75092160000008\n",
      "pathlengthtoO(n/r). 108.0 128.47814440000002 199.13264999999998 138.65992160000008\n",
      "Weplantoinvestigatethisapproachfurtherinfuturework. 202.221056 128.6973216 437.5675558000001 138.65992160000008\n",
      "Asingleconvolutionallayerwithkernelwidthk 107.641 144.86714440000003 301.35752956 155.04892160000009\n",
      "<ndoesnotconnectallpairsofinputandoutput 304.43597296 144.86714440000003 503.99988086800005 155.04892160000009\n",
      "positions. 108.0 155.9953216 145.706049976 165.95792160000008\n",
      "DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, 148.752214552 155.77614440000002 505.2424466680001 165.95792160000008\n",
      "orO(log 108.0 166.6851444 143.90312426 176.86692160000007\n",
      "k 143.9 170.58811720000006 148.13518874000002 177.56191720000004\n",
      "(n))inthecaseofdilatedconvolutions[18], 148.802 166.6851444 332.065359716 176.86692160000007\n",
      "increasingthelengthofthelongestpaths 335.11391531600003 166.9043216 504.0038955560001 176.86692160000007\n",
      "betweenanytwopositionsinthenetwork. 108.0 177.8133216 277.6189436820001 187.77592160000006\n",
      "Convolutionallayersaregenerallymoreexpensivethan 280.6979848380001 177.8133216 503.99890422999994 187.77592160000006\n",
      "recurrent 108.0 188.7223216 144.67412386799998 198.68492160000005\n",
      "layers, 147.763326876 188.7223216 174.570292452 198.68492160000005\n",
      "by 177.801761388 188.7223216 187.96361338800003 198.68492160000005\n",
      "a 191.05281639600003 188.7223216 195.56467868400003 198.68492160000005\n",
      "factor 198.65388169200003 188.7223216 222.24970203600006 198.68492160000005\n",
      "of 225.33890504400006 188.7223216 233.80372776000004 198.68492160000005\n",
      "k. 236.889 188.5031444 244.929463 198.68492160000005\n",
      "Separable 249.80715196 188.7223216 289.87533439599997 198.68492160000005\n",
      "convolutions 292.964537404 188.7223216 344.87127742 198.68492160000005\n",
      "[6], 347.96048042800004 188.7223216 362.24735971599995 198.68492160000005\n",
      "however, 365.478828652 188.7223216 401.970039184 198.68492160000005\n",
      "decrease 405.20150811999997 188.7223216 440.17860270399996 198.68492160000005\n",
      "the 443.267805712 188.7223216 455.685588856 198.68492160000005\n",
      "complexity 458.774791864 188.7223216 504.350698084 198.68492160000005\n",
      "considerably, 108.0 199.63132159999998 161.817168192 209.59392160000004\n",
      "toO(k·n·d+n·d2). 164.85556194 198.2061172 262.473463 209.59392160000004\n",
      "Evenwithk 266.863383064 199.4121444 316.69552956 209.59392160000004\n",
      "= 320.57800000000003 199.4121444 328.32691028000005 209.37474440000005\n",
      "n, 331.896 199.4121444 340.41646299999996 209.59392160000004\n",
      "however, 343.444694896 199.63132159999998 379.92574357599995 209.59392160000004\n",
      "thecomplexityofaseparable 382.964137324 199.63132159999998 504.00195649600005 209.59392160000004\n",
      "convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, 108.0 210.54032159999997 505.24786557899995 220.50292160000004\n",
      "theapproachwetakeinourmodel. 108.0 221.44932159999996 248.17378200000005 231.41192160000003\n",
      "Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions 107.641 237.83832159999997 504.00363875599965 247.80092160000004\n",
      "fromourmodelsandpresentanddiscussexamplesintheappendix. 108.0 248.74732159999996 374.52714640200014 258.70992160000003\n",
      "Notonlydoindividualattention 377.6142572640002 248.74732159999996 503.99840609999967 258.70992160000003\n",
      "headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic 108.0 259.65632159999996 504.0013948799997 269.6189216\n",
      "andsemanticstructureofthesentences. 108.0 270.56532159999995 266.2559010000001 280.52792159999996\n",
      "5 108.0 299.63343679999997 113.9776 311.58863679999996\n",
      "Training 125.9328 299.63343679999997 170.22681599999999 311.58863679999996\n",
      "Thissectiondescribesthetrainingregimeforourmodels. 107.691 325.7843216 337.4783690000001 335.74692159999995\n",
      "5.1 108.0 352.4935834 120.45325 362.4561834\n",
      "TrainingDataandBatching 130.41585 352.4935834 249.52869560000005 362.4561834\n",
      "We 107.532 373.4663216 120.84402612 383.42892159999997\n",
      "trained 124.14662802 373.4663216 152.366091024 383.42892159999997\n",
      "on 155.678854776 373.4663216 165.84070677600002 383.42892159999997\n",
      "the 169.153470528 373.4663216 181.57125367199998 383.42892159999997\n",
      "standard 184.884017424 373.4663216 219.31237200000004 383.42892159999997\n",
      "WMT 222.62513575200006 373.4663216 247.46070204000003 383.42892159999997\n",
      "2014 250.77346579200002 373.4663216 271.09716979200005 383.42892159999997\n",
      "English-German 274.40993354400007 373.4663216 341.5797752640001 383.42892159999997\n",
      "dataset 344.8925390160001 373.4663216 373.1120020200001 383.42892159999997\n",
      "consisting 376.42476577200006 373.4663216 417.6412374839999 383.42892159999997\n",
      "of 420.95400123599984 373.4663216 429.4188239519998 383.42892159999997\n",
      "about 432.7214258519998 373.4663216 455.30106099599976 383.42892159999997\n",
      "4.5 458.61382474799973 373.4663216 471.31613974799967 383.42892159999997\n",
      "million 474.62890349999964 373.4663216 503.9966557799995 383.42892159999997\n",
      "sentencepairs. 108.0 384.3753216 168.00573606 394.33792159999996\n",
      "Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- 171.806268708 384.3753216 505.6543158080001 394.33792159999996\n",
      "targetvocabularyofabout37000tokens. 108.0 395.2843216 269.28891494400006 405.2469216\n",
      "ForEnglish-French,weusedthesignificantlylargerWMT 272.37237949440004 395.2843216 504.3042579199998 405.2469216\n",
      "2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece 108.0 406.19432159999997 504.0009465629997 416.1569216\n",
      "vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining 107.751 417.10332159999996 504.0001842359998 427.06592159999997\n",
      "batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 108.0 428.01232159999995 503.9972105879997 437.97492159999996\n",
      "targettokens. 108.0 438.92132160000006 161.39953599999998 448.8839216\n",
      "5.2 108.0 465.62958340000006 120.45325 475.59218340000007\n",
      "HardwareandSchedule 130.41585 465.62958340000006 233.04059260000002 475.59218340000007\n",
      "Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. 107.532 486.60332159999996 390.73265338800013 496.56592159999997\n",
      "Forourbasemodelsusing 395.2241919720002 486.60332159999996 503.99665577999997 496.56592159999997\n",
      "thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. 108.0 497.51232159999995 487.80304413839974 507.47492159999996\n",
      "We 490.9138062127998 497.51232159999995 503.99701133599973 507.47492159999996\n",
      "trainedthebasemodelsforatotalof100,000stepsor12hours. 108.0 508.42132160000006 356.68223690800016 518.3839216\n",
      "Forourbigmodels,(describedonthe 359.76745487600016 508.42132160000006 504.00139487999974 518.3839216\n",
      "bottomlineoftable3),steptimewas1.0seconds. 108.0 519.3303216 311.68416148800003 529.2929216\n",
      "Thebigmodelsweretrainedfor300,000steps 314.915630424 519.3303216 503.9972105879998 529.2929216\n",
      "(3.5days). 107.671 530.2393216 150.0020874 540.2019216\n",
      "5.3 108.0 556.9485834 120.45325 566.9111834\n",
      "Optimizer 130.41585 556.9485834 174.1317388 566.9111834\n",
      "WeusedtheAdamoptimizer[20]withβ 107.532 577.7021444 272.94484656 587.8839216\n",
      "1 272.945 581.6051172 276.9165791 588.5789172\n",
      "=0.9,β 280.182 577.7021444 314.07584655999995 587.8839216\n",
      "2 314.07599999999996 581.6051172 318.04757909999995 588.5789172\n",
      "=0.98andϵ=10−9. 321.313 576.4961172 409.6355004 587.8839216\n",
      "Wevariedthelearning 412.722710888 577.9213216 504.00292131680004 587.8839216\n",
      "rateoverthecourseoftraining,accordingtotheformula: 108.0 588.8303215999999 336.2332034000001 598.7929216\n",
      "lrate=d−0.5 162.892 617.1501172 219.33957910000004 628.9647444\n",
      "model 202.80400000000003 624.4555408 220.23850000000004 631.4293408\n",
      "·min(step_num−0.5,step_num·warmup_steps−1.5) 222.95100000000002 617.2981172 449.10745514 629.1839216\n",
      "(3) 493.051 619.2213216 504.6673916 629.1839216\n",
      "Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, 107.691 642.4701444 505.2438336688 652.6519216\n",
      "anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. 108.0 653.5983216 465.788647068 663.5609216\n",
      "Weused 469.39610452799997 653.5983216 503.9972105879999 663.5609216\n",
      "warmup_steps=4000. 108.0 664.2881444 208.17065 674.4699216\n",
      "5.4 108.0 691.2165834 120.45325 701.1791834\n",
      "Regularization 130.41585 691.2165834 193.5089958 701.1791834\n",
      "Weemploythreetypesofregularizationduringtraining: 107.532 712.1893216 331.9893780000001 722.1519216\n",
      "7 303.509 742.0773216 308.4903 752.0399216\n",
      "Table2: 107.691 70.94432159999997 139.81042240000002 80.90692160000003\n",
      "TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe 142.8988284 70.94432159999997 504.0032279999998 80.90692160000003\n",
      "English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. 108.0 81.85332159999996 483.51031919999986 91.81592160000002\n",
      "Model 136.671 105.94732160000001 162.6833486 115.90992160000008\n",
      "BLEU 311.02099999999996 97.65832160000002 337.03334859999995 107.62092160000009\n",
      "TrainingCost(FLOPs) 383.2498499999999 97.65832160000002 475.3241992 107.62092160000009\n",
      "EN-DE 288.72 113.57332159999999 318.5978374 123.53592160000005\n",
      "EN-FR 330.5529574 113.57332159999999 359.33490880000005 123.53592160000005\n",
      "EN-DE 387.46929120000004 113.57332159999999 417.3471286 123.53592160000005\n",
      "EN-FR 440.0419314 113.57332159999999 468.82388280000004 123.53592160000005\n",
      "ByteNet[18] 136.671 124.8803216 188.9646874 134.84292160000007\n",
      "23.75 292.44621359999996 124.8803216 314.8620636 134.84292160000007\n",
      "Deep-Att+PosUnk[39] 136.671 136.26332160000004 234.98193680000003 146.2259216000001\n",
      "39.2 336.22187800000006 136.26332160000004 353.656428 146.2259216000001\n",
      "1.0·1020 435.264 134.83711720000008 473.0951582 146.00674440000012\n",
      "GNMT+RL[38] 136.67100000000005 147.6453216000001 208.42164520000006 157.60792160000017\n",
      "24.6 294.93686360000004 147.6453216000001 312.37141360000004 157.60792160000017\n",
      "39.92 333.7312280000001 147.6453216000001 356.147078 157.60792160000017\n",
      "2.3·1019 383.24500000000006 146.22011720000012 421.0761582 157.38874440000018\n",
      "1.4·1020 435.264 146.22011720000012 473.0951582 157.38874440000018\n",
      "ConvS2S[9] 136.67100000000005 159.02832160000014 188.02820300000005 168.9909216000002\n",
      "25.16 292.4462136000001 159.02832160000014 314.8620636 168.9909216000002\n",
      "40.46 333.73122800000004 159.02832160000014 356.147078 168.9909216000002\n",
      "9.6·1018 383.24500000000006 157.60211720000018 421.0761582 168.77174440000022\n",
      "1.5·1020 435.264 157.60211720000018 473.0951582 168.77174440000022\n",
      "MoE[32] 136.67100000000005 170.4103216000002 175.68454160000005 180.37292160000027\n",
      "26.03 292.4462136000001 170.4103216000002 314.8620636 180.37292160000027\n",
      "40.56 333.73122800000004 170.4103216000002 356.147078 180.37292160000027\n",
      "2.0·1019 383.24500000000006 168.98511720000022 421.0761582 180.15374440000028\n",
      "1.2·1020 435.26500000000004 168.98511720000022 473.0951582 180.15374440000028\n",
      "Deep-Att+PosUnkEnsemble[39] 136.671 183.0483216 276.76508120000005 193.01092160000007\n",
      "40.4 336.22187800000006 183.0483216 353.65642800000006 193.01092160000007\n",
      "8.0·1020 435.264 181.62211720000005 473.0951582 192.79174440000008\n",
      "GNMT+RLEnsemble[38] 136.67100000000005 194.43032160000007 250.2047896000001 204.39292160000014\n",
      "26.30 292.4462136000001 194.43032160000007 314.8620636000001 204.39292160000014\n",
      "41.16 333.7312280000001 194.43032160000007 356.1470780000001 204.39292160000014\n",
      "1.8·1020 383.24500000000006 193.0051172000001 421.0761582 204.17374440000015\n",
      "1.1·1021 435.264 193.0051172000001 473.0951582 204.17374440000015\n",
      "ConvS2SEnsemble[9] 136.67100000000005 205.81232160000002 229.8113474000001 215.77492160000008\n",
      "26.36 292.4462136000001 205.81232160000002 314.86206360000006 215.77492160000008\n",
      "41.29 333.73600000000005 205.74258340000006 356.15185 215.7051834\n",
      "7.7·1019 383.24500000000006 204.38711720000003 421.0761582 215.5557444000001\n",
      "1.2·1021 435.264 204.38711720000003 473.0951582 215.5557444000001\n",
      "Transformer(basemodel) 136.671 218.94832159999999 240.34181560000002 228.91092160000005\n",
      "27.3 294.93686360000004 218.94832159999999 312.37141360000004 228.91092160000005\n",
      "38.1 336.22187800000006 218.94832159999999 353.656428 228.91092160000005\n",
      "3.3·1018 407.339 217.5231172 450.73688672 228.69174440000006\n",
      "Transformer(big) 136.671 230.33032159999993 207.9733282 240.2929216\n",
      "28.4 294.94100000000003 230.26058339999997 312.37555000000003 240.22318339999993\n",
      "41.8 336.22601440000005 230.26058339999997 353.6605644 240.22318339999993\n",
      "2.3·1019 410.12300000000005 228.90511719999995 447.95415820000005 240.0737444\n",
      "ResidualDropout 108.0 272.9765834 182.94457505920002 282.93918340000005\n",
      "Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe 192.909 273.04632160000006 504.00125174320016 283.0089216\n",
      "sub-layerinputandnormalized. 108.0 283.95532160000005 232.29718338800004 293.9179216\n",
      "Inaddition,weapplydropouttothesumsoftheembeddingsandthe 235.38240135600006 283.95532160000005 504.0013948799996 293.9179216\n",
      "positionalencodingsinboththeencoderanddecoderstacks. 108.0 294.86432160000004 354.4045872960001 304.8269216\n",
      "Forthebasemodel,weusearateof 357.6462180840001 294.86432160000004 503.997210588 304.8269216\n",
      "P 108.0 305.5541444 114.3959892 315.5167444\n",
      "drop 114.396 309.4571172 130.49780682 316.4309172\n",
      "=0.1. 133.762 305.5541444 159.49865 315.7359216\n",
      "LabelSmoothing 108.0 331.0475834 182.46605145600003 341.0101834\n",
      "Duringtraining,weemployedlabelsmoothingofvalueϵ 192.428 330.8981444 426.22681934 341.0799216\n",
      "ls 426.227 334.8021172 432.5487497 341.7759172\n",
      "= 436.299 330.8981444 444.04791028 340.8607444\n",
      "0.1[36]. 447.30568048 330.8981444 482.056359716 341.0799216\n",
      "This 485.928025328 331.11732159999997 503.99579818399997 341.0799216\n",
      "hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore. 108.0 342.02632159999996 491.6995764 351.98892159999997\n",
      "6 108.0 371.0054368 113.9776 382.9606368\n",
      "Results 125.9328 371.0054368 163.1254272 382.9606368\n",
      "6.1 108.0 397.0335834 120.45325 406.9961834\n",
      "MachineTranslation 130.41585 397.0335834 219.07302740000006 406.9961834\n",
      "OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big) 108.0 417.97032160000003 504.6653025439999 427.9329216\n",
      "inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0 108.0 428.6601444 504.24929999999995 438.8419216\n",
      "BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. 108.0 439.56914439999997 363.3814817 449.75092159999997\n",
      "Theconfigurationofthismodelis 366.474769374 439.7883216 503.99929678200004 449.75092159999997\n",
      "listedinthebottomlineofTable3. 108.0 450.6973216 249.48968205840006 460.65992159999996\n",
      "Trainingtook3.5dayson8P100GPUs. 252.5809174368001 450.47814439999996 416.1603594924 460.65992159999996\n",
      "Evenourbasemodel 419.2515948708 450.6973216 503.99791379040005 460.65992159999996\n",
      "surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof 108.0 461.6063216 504.0033873999998 471.56892159999995\n",
      "thecompetitivemodels. 108.0 472.51632159999997 203.07309180000001 482.4789216\n",
      "OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0, 108.0 488.68514439999996 505.24532765000004 498.86692159999996\n",
      "outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe 108.0 499.5941444 503.99659189920004 509.7759216\n",
      "previousstate-of-the-artmodel. 108.0 510.7223216 236.61856076400005 520.6849216\n",
      "TheTransformer(big)modeltrainedforEnglish-to-Frenchused 240.55119748800007 510.7223216 503.9972105879998 520.6849216\n",
      "dropoutrateP 108.0 521.4121444 165.3049892 531.5939215999999\n",
      "drop 165.30499999999998 525.3161172 181.40680681999999 532.2899172\n",
      "=0.1,insteadof0.3. 184.671 521.4121444 269.62264999999996 531.5939215999999\n",
      "Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which 108.0 538.0203216 503.99721058799975 547.9829216\n",
      "werewrittenat10-minuteintervals. 107.641 548.9293216 253.02661656400005 558.8919216\n",
      "Forthebigmodels,weaveragedthelast20checkpoints. 256.48164624400005 548.9293216 487.23698145999964 558.8919216\n",
      "We 490.6920111399997 548.9293216 504.0040372599996 558.8919216\n",
      "usedbeamsearchwithabeamsizeof4andlengthpenaltyα 108.0 559.6191444 356.99607521999997 569.8009216\n",
      "= 360.158 559.6191444 367.90691028000003 569.5817443999999\n",
      "0.6[38]. 371.03516668000003 559.6191444 405.71535971599997 569.8009216\n",
      "Thesehyperparameters 409.383788288 559.8383216 504.00079226 569.8009216\n",
      "werechosenafterexperimentationonthedevelopmentset. 107.641 570.7473216 336.6403273400001 580.7099216\n",
      "Wesetthemaximumoutputlengthduring 339.6962552640001 570.7473216 504.0036387559996 580.7099216\n",
      "inferencetoinputlength+50,butterminateearlywhenpossible[38]. 108.0 581.4371444000001 387.07221440000006 591.6189216\n",
      "Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel 107.691 598.0453216 503.99505866799956 608.0079216\n",
      "architecturesfromtheliterature. 108.0 608.9543216 235.99292940920003 618.9169216\n",
      "Weestimatethenumberoffloatingpointoperationsusedtotraina 239.08200290340002 608.9543216 503.99736998959963 618.9169216\n",
      "modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained 108.0 619.8633216000001 503.99701133599984 629.8259216\n",
      "single-precisionfloating-pointcapacityofeachGPU5. 108.0 629.5005408 326.97665 640.7349216\n",
      "6.2 108.0 657.3915834 120.45325 667.3541834\n",
      "ModelVariations 130.41585 657.3915834 203.93983800000004 667.3541834\n",
      "ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel 107.691 678.3293216 504.0004583971999 688.2919216\n",
      "indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe 108.0 689.2383216000001 503.9972105879998 699.2009216\n",
      "5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively. 120.653 711.5045616 453.5559904000002 721.9367424\n",
      "8 303.509 742.0773216 308.4903 752.0399216\n",
      "Table3: 107.691 70.94432159999997 139.33572443519998 80.90692160000003\n",
      "VariationsontheTransformerarchitecture. 142.43353512960002 70.94432159999997 311.3034307680001 80.90692160000003\n",
      "Unlistedvaluesareidenticaltothoseofthebase 314.4012414624001 70.94432159999997 503.9950985183997 80.90692160000003\n",
      "model. 108.0 81.85332159999996 135.2875614 91.81592160000002\n",
      "AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. 138.4033047744 81.85332159999996 476.07447366239984 91.81592160000002\n",
      "Listed 479.1902170367999 81.85332159999996 503.99709103679976 91.81592160000002\n",
      "perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto 108.0 92.76232159999995 504.00338739999984 102.72492160000002\n",
      "per-wordperplexities. 108.0 103.67132159999994 195.5314036 113.63392160000001\n",
      "N 146.127 136.46214439999994 154.1319491 146.4247444\n",
      "d 167.17299250000002 136.46214439999994 172.35852580000002 146.4247444\n",
      "model 172.358 140.51954079999996 189.79250000000002 147.49334079999994\n",
      "d 207.132 136.46214439999994 212.3175333 146.4247444\n",
      "ff 212.317 140.51954079999996 216.7872058 147.49334079999994\n",
      "h 236.238 136.46214439999994 241.97845012 146.4247444\n",
      "d 258.47651572 136.46214439999994 263.66204902 146.4247444\n",
      "k 263.658 140.36611719999996 267.89318874 147.33991719999995\n",
      "d 285.456 136.46214439999994 290.6415333 146.4247444\n",
      "v 290.641 140.36611719999996 294.62582932000004 147.33991719999995\n",
      "P 309.843 136.46214439999994 316.2389892 146.4247444\n",
      "drop 316.23900000000003 140.36611719999996 332.34080682 147.33991719999995\n",
      "ϵ 345.588 136.46214439999994 349.63181934000005 146.4247444\n",
      "ls 349.632 140.36611719999996 355.9537497 147.33991719999995\n",
      "train 371.139 131.22732159999998 389.4004458 141.18992160000005\n",
      "PPL 405.096 131.22732159999998 422.2615598 141.18992160000005\n",
      "BLEU 436.0199104 131.22732159999998 462.032259 141.18992160000005\n",
      "params 473.98737900000003 131.22732159999998 502.7593678 141.18992160000005\n",
      "steps 370.307 142.60932159999993 390.2322 152.5719216\n",
      "(dev) 403.293 142.60932159999993 424.065021 152.5719216\n",
      "(dev) 438.6403048 142.60932159999993 459.4123258 152.5719216\n",
      "×106 477.283 141.18411719999995 498.96657910000005 152.3527444\n",
      "base 116.468 155.24732159999996 134.1715402 165.20992160000003\n",
      "6 148.182 155.24732159999996 153.1633 165.20992160000003\n",
      "512 171.2553816 155.24732159999996 186.19928159999998 165.20992160000003\n",
      "2048 202.2490302 155.24732159999996 222.17423019999998 165.20992160000003\n",
      "8 236.6200002 155.24732159999996 241.6013002 165.20992160000003\n",
      "64 258.53772019999997 155.24732159999996 268.5003202 165.20992160000003\n",
      "64 285.43674020000003 155.24732159999996 295.3993402 165.20992160000003\n",
      "0.1 315.1153256 155.24732159999996 327.5685756 165.20992160000003\n",
      "0.1 344.793911 155.24732159999996 357.24716099999995 165.20992160000003\n",
      "100K 369.20228099999997 155.24732159999996 391.3391782 165.20992160000003\n",
      "4.92 404.962 155.24732159999996 422.39655 165.20992160000003\n",
      "25.8 440.3093048 155.24732159999996 457.74385479999995 165.20992160000003\n",
      "65 483.3875872 155.24732159999996 493.3501872 165.20992160000003\n",
      "(A) 118.406 184.24832159999994 132.23408880000002 194.2109216\n",
      "1 236.617 167.88432160000002 241.5983 177.8469216000001\n",
      "512 256.04407 167.88432160000002 270.98796999999996 177.8469216000001\n",
      "512 282.94309 167.88432160000002 297.88698999999997 177.8469216000001\n",
      "5.29 404.962 167.88432160000002 422.39655 177.8469216000001\n",
      "24.9 440.3093048 167.88432160000002 457.74385479999995 177.8469216000001\n",
      "4 236.617 178.7943216 241.5983 188.75692160000006\n",
      "128 256.04407 178.7943216 270.98796999999996 188.75692160000006\n",
      "128 282.94309 178.7943216 297.88698999999997 188.75692160000006\n",
      "5.00 404.962 178.7943216 422.39655 188.75692160000006\n",
      "25.5 440.3093048 178.7943216 457.74385479999995 188.75692160000006\n",
      "16 234.127 189.70332159999998 244.08960000000002 199.66592160000005\n",
      "32 258.53537 189.70332159999998 268.49797 199.66592160000005\n",
      "32 285.43439 189.70332159999998 295.39698999999996 199.66592160000005\n",
      "4.91 404.962 189.70332159999998 422.39655 199.66592160000005\n",
      "25.8 440.3093048 189.70332159999998 457.74385479999995 199.66592160000005\n",
      "32 234.127 200.61232159999997 244.08960000000002 210.57492160000004\n",
      "16 258.53537 200.61232159999997 268.49797 210.57492160000004\n",
      "16 285.43439 200.61232159999997 295.39698999999996 210.57492160000004\n",
      "5.01 404.962 200.61232159999997 422.39655 210.57492160000004\n",
      "25.4 440.3093048 200.61232159999997 457.74385479999995 210.57492160000004\n",
      "(B) 118.68 218.70432159999996 131.96014580000002 228.66692160000002\n",
      "16 258.535 213.24932159999992 268.4976 223.21192159999998\n",
      "5.16 404.962 213.24932159999992 422.39655 223.21192159999998\n",
      "25.1 440.3093048 213.24932159999992 457.74385479999995 223.21192159999998\n",
      "58 483.3875872 213.24932159999992 493.3501872 223.21192159999998\n",
      "32 258.535 224.15832160000002 268.4976 234.1209216000001\n",
      "5.01 404.962 224.15832160000002 422.39655 234.1209216000001\n",
      "25.4 440.3093048 224.15832160000002 457.74385479999995 234.1209216000001\n",
      "60 483.3875872 224.15832160000002 493.3501872 234.1209216000001\n",
      "(C) 118.68 269.5233215999999 131.96014580000002 279.4859216\n",
      "2 148.182 236.79632159999994 153.1633 246.7589216\n",
      "6.11 404.962 236.79632159999994 422.39655 246.7589216\n",
      "23.7 440.3093048 236.79632159999994 457.74385479999995 246.7589216\n",
      "36 483.3875872 236.79632159999994 493.3501872 246.7589216\n",
      "4 148.182 247.70532159999993 153.1633 257.6679216\n",
      "5.19 404.962 247.70532159999993 422.39655 257.6679216\n",
      "25.3 440.3093048 247.70532159999993 457.74385479999995 257.6679216\n",
      "50 483.3875872 247.70532159999993 493.3501872 257.6679216\n",
      "8 148.182 258.6143215999999 153.1633 268.5769216\n",
      "4.88 404.962 258.6143215999999 422.39655 268.5769216\n",
      "25.5 440.3093048 258.6143215999999 457.74385479999995 268.5769216\n",
      "80 483.3875872 258.6143215999999 493.3501872 268.5769216\n",
      "256 171.26 269.5233215999999 186.2039 279.4859216\n",
      "32 258.532376 269.5233215999999 268.494976 279.4859216\n",
      "32 285.431396 269.5233215999999 295.39399599999996 279.4859216\n",
      "5.75 404.962 269.5233215999999 422.39655 279.4859216\n",
      "24.5 440.3093048 269.5233215999999 457.74385479999995 279.4859216\n",
      "28 483.3875872 269.5233215999999 493.3501872 279.4859216\n",
      "1024 168.769 280.4323216 188.6942 290.3949216\n",
      "128 256.041376 280.4323216 270.985276 290.3949216\n",
      "128 282.940396 280.4323216 297.884296 290.3949216\n",
      "4.66 404.962 280.4323216 422.39655 290.3949216\n",
      "26.0 440.3093048 280.4323216 457.74385479999995 290.3949216\n",
      "168 480.8969372 280.4323216 495.84083719999995 290.3949216\n",
      "1024 202.246 291.3413216 222.17120000000003 301.30392159999997\n",
      "5.12 404.962 291.3413216 422.39655 301.30392159999997\n",
      "25.4 440.3093048 291.3413216 457.74385479999995 301.30392159999997\n",
      "53 483.3875872 291.3413216 493.3501872 301.30392159999997\n",
      "4096 202.246 302.2503216 222.17120000000003 312.21292159999996\n",
      "4.75 404.962 302.2503216 422.39655 312.21292159999996\n",
      "26.2 440.3093048 302.2503216 457.74385479999995 312.21292159999996\n",
      "90 483.3875872 302.2503216 493.3501872 312.21292159999996\n",
      "(D) 118.406 331.25232159999996 132.23408880000002 341.21492159999997\n",
      "0.0 315.113 314.88832160000004 327.56624999999997 324.8509216\n",
      "5.77 404.962 314.88832160000004 422.39655 324.8509216\n",
      "24.6 440.3093048 314.88832160000004 457.74385479999995 324.8509216\n",
      "0.2 315.113 325.79732160000003 327.56624999999997 335.7599216\n",
      "4.95 404.962 325.79732160000003 422.39655 335.7599216\n",
      "25.5 440.3093048 325.79732160000003 457.74385479999995 335.7599216\n",
      "0.0 344.792 336.7063216 357.24524999999994 346.6689216\n",
      "4.67 404.962 336.7063216 422.39655 346.6689216\n",
      "25.3 440.3093048 336.7063216 457.74385479999995 346.6689216\n",
      "0.2 344.792 347.6153216 357.24524999999994 357.57792159999997\n",
      "5.47 404.962 347.6153216 422.39655 357.57792159999997\n",
      "25.7 440.3093048 347.6153216 457.74385479999995 357.57792159999997\n",
      "(E) 118.959 360.25332160000005 131.68124020000002 370.2159216\n",
      "positionalembeddinginsteadofsinusoids 178.634 360.25332160000005 345.78650280000005 370.2159216\n",
      "4.92 404.962 360.25332160000005 422.39655 370.2159216\n",
      "25.7 440.3093048 360.25332160000005 457.74385479999995 370.2159216\n",
      "big 118.954 372.8903216 131.6862028 382.8529216\n",
      "6 148.182 372.8903216 153.1633 382.8529216\n",
      "1024 168.76473159999998 372.8903216 188.6899316 382.8529216\n",
      "4096 202.2490302 372.8903216 222.17423019999998 382.8529216\n",
      "16 234.1293502 372.8903216 244.0919502 382.8529216\n",
      "0.3 315.1153256 372.8903216 327.5685756 382.8529216\n",
      "300K 369.202281 372.8903216 391.33917820000005 382.8529216\n",
      "4.33 404.962 372.82058340000003 422.39655 382.78318340000004\n",
      "26.4 440.3093048 372.82058340000003 457.74385479999995 382.78318340000004\n",
      "213 480.902 372.8903216 495.8459 382.8529216\n",
      "developmentset,newstest2013. 108.0 413.4953216 236.56775150400003 423.45792159999996\n",
      "Weusedbeamsearchasdescribedintheprevioussection,butno 239.71792562400006 413.4953216 503.9972105879997 423.45792159999996\n",
      "checkpointaveraging. 108.0 424.4043216 195.6310296 434.36692159999996\n",
      "WepresenttheseresultsinTable3. 198.71943560000003 424.4043216 339.0625818000001 434.36692159999996\n",
      "InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions, 108.0 440.7933216 505.24134007599963 450.75592159999997\n",
      "keeping 108.0 451.7023216 140.070804912 461.66492159999996\n",
      "the 143.586805704 451.7023216 156.004588848 461.66492159999996\n",
      "amount 159.51042778800002 451.7023216 189.995983788 461.66492159999996\n",
      "of 193.501822728 451.7023216 201.96664544400002 461.66492159999996\n",
      "computation 205.472484384 451.7023216 256.28174438400004 461.66492159999996\n",
      "constant, 259.79774517600003 451.7023216 296.2076608920001 461.66492159999996\n",
      "as 299.96754613200005 451.7023216 308.432368848 461.66492159999996\n",
      "described 311.93820778800006 451.7023216 350.87842465200004 461.66492159999996\n",
      "in 354.3944254440001 451.7023216 362.30034630000006 461.66492159999996\n",
      "Section 365.8061852400001 451.7023216 396.29174124 461.66492159999996\n",
      "3.2.2. 399.79758018 451.7023216 422.66174717999996 461.66492159999996\n",
      "While 428.80966764 451.7023216 453.64523392799987 461.66492159999996\n",
      "single-head 457.15107286799986 451.7023216 503.99721058799975 461.66492159999996\n",
      "attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads. 108.0 462.6113216 480.80049199999985 472.5739216\n",
      "InTable3rows(B),weobservethatreducingtheattentionkeysized 108.0 478.78114439999996 392.95753329999997 488.96292159999996\n",
      "k 392.957 482.6841172 397.19218874 489.6579172\n",
      "hurtsmodelquality. 400.627 479.0003216 482.013272668 488.96292159999996\n",
      "This 485.93574754 479.0003216 504.003520396 488.96292159999996\n",
      "suggests 108.0 489.9093216 142.438516428 499.8719216\n",
      "that 146.015488332 489.9093216 161.25826633199998 499.8719216\n",
      "determining 164.835238236 489.9093216 213.94746895200004 499.8719216\n",
      "compatibility 217.53460270800002 489.9093216 271.73792127600007 499.8719216\n",
      "is 275.3148931800001 489.9093216 282.0928484640001 499.8719216\n",
      "not 285.6698203680001 489.9093216 298.6566672240001 499.8719216\n",
      "easy 302.23363912800016 489.9093216 320.29125013200013 499.8719216\n",
      "and 323.86822203600013 489.9093216 338.5419363240002 499.8719216\n",
      "that 342.1290700800002 489.9093216 357.3718480800002 499.8719216\n",
      "a 360.94881998400024 489.9093216 365.4606822720002 499.8719216\n",
      "more 369.0376541760002 489.9093216 389.9202600360002 499.8719216\n",
      "sophisticated 393.49723194000023 489.9093216 446.562423084 499.8719216\n",
      "compatibility 450.14955684 489.9093216 504.34271355599975 499.8719216\n",
      "functionthandotproductmaybebeneficial. 108.0 500.8183216 280.09853519600006 510.7809216\n",
      "Wefurtherobserveinrows(C)and(D)that,asexpected, 283.1349364240001 500.8183216 505.241340076 510.7809216\n",
      "biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour 108.0 511.7273216 504.167371796 521.6899215999999\n",
      "sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical 108.0 522.6363216 504.00008483600004 532.5989216\n",
      "resultstothebasemodel. 108.0 533.5453216 209.00083880000003 543.5079216\n",
      "6.3 108.0 560.2565834 120.45325 570.2191834\n",
      "EnglishConstituencyParsing 130.41585 560.2565834 255.97449780000008 570.2191834\n",
      "ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish 107.691 581.2303216 503.99772864479985 591.1929216\n",
      "constituencyparsing. 108.0 592.1393216 191.61265474040002 602.1019216\n",
      "Thistaskpresentsspecificchallenges: 194.70650032 592.1393216 344.76786394200013 602.1019216\n",
      "theoutputissubjecttostrongstructural 347.85185651020015 592.1393216 504.00238117739957 602.1019216\n",
      "constraints 108.0 603.0483216 152.03130471600002 613.0109216000001\n",
      "and 155.45584884000002 603.0483216 170.129563128 613.0109216000001\n",
      "is 173.564269104 603.0483216 180.342224388 613.0109216000001\n",
      "significantly 183.766768512 603.0483216 234.01712665200003 613.0109216000001\n",
      "longer 237.45183262800006 603.0483216 263.415364488 613.0109216000001\n",
      "than 266.8399086120001 603.0483216 284.3386177560001 613.0109216000001\n",
      "the 287.7733237320001 603.0483216 300.1911068760001 613.0109216000001\n",
      "input. 303.6156510000001 603.0483216 327.04888171200014 613.0109216000001\n",
      "Furthermore, 332.95291772400014 603.0483216 386.2924788720002 613.0109216000001\n",
      "RNN 389.9609074440002 603.0483216 411.4125770160002 613.0109216000001\n",
      "sequence-to-sequence 414.83712114000014 603.0483216 503.9972105879999 613.0109216000001\n",
      "modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37]. 108.0 613.9573216 448.1032388 623.9199216\n",
      "Wetraineda4-layertransformerwithd 107.532 630.1271444 262.1605333 640.3089216000001\n",
      "model 262.161 634.0301172 283.60961928 641.0039172\n",
      "=1024ontheWallStreetJournal(WSJ)portionofthe 286.915 630.1271444 504.00032336800007 640.3089216000001\n",
      "PennTreebank[25],about40Ktrainingsentences. 108.0 641.2553216 313.38889614040005 651.2179216\n",
      "Wealsotraineditinasemi-supervisedsetting, 316.48218381440006 641.2553216 505.24372541599996 651.2179216\n",
      "usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences 108.0 652.1643216 503.9980474463998 662.1269216000001\n",
      "[37]. 108.0 663.0733216 127.07938760420001 673.0359215999999\n",
      "Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens 130.1647051982 663.0733216 503.9957185795999 673.0359215999999\n",
      "forthesemi-supervisedsetting. 108.0 673.9823216 233.07048040000004 683.9449216\n",
      "Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual 107.532 690.3713216 503.9962074629997 700.3339215999999\n",
      "(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters 107.671 701.2803216 503.9979726479998 711.2429216\n",
      "remained 108.0 712.1893216 145.812251292 722.1519216\n",
      "unchanged 148.972587264 712.1893216 192.993730128 722.1519216\n",
      "from 196.143904248 712.1893216 215.89854453600003 722.1519216\n",
      "the 219.05888050800004 712.1893216 231.47666365200004 722.1519216\n",
      "English-to-German 234.62683777200004 712.1893216 313.0864970640001 722.1519216\n",
      "base 316.2468330360001 712.1893216 334.30444404000013 722.1519216\n",
      "translation 337.4546181600001 712.1893216 380.35795730400014 722.1519216\n",
      "model. 383.5182932760001 712.1893216 411.46338627600005 722.1519216\n",
      "During 416.544312276 712.1893216 445.33283899199995 722.1519216\n",
      "inference, 448.483013112 712.1893216 488.8255655519999 722.1519216\n",
      "we 492.1484911559999 712.1893216 503.99721058799986 722.1519216\n",
      "9 303.509 742.0773216 308.4903 752.0399216\n",
      "Table4: 107.691 70.94432159999997 139.15827060400002 80.90692160000003\n",
      "TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23 142.223961876 70.94432159999997 503.99505866799973 80.90692160000003\n",
      "ofWSJ) 108.0 81.85332159999996 140.92639300000002 91.81592160000002\n",
      "Parser 206.758 93.90258340000003 234.8724572 103.86518339999998\n",
      "Training 334.084 93.90258340000003 370.995433 103.86518339999998\n",
      "WSJ23F1 414.477 93.90258340000003 460.97245419999996 103.86518339999998\n",
      "Vinyals&Kaiserelal. 151.027 105.2793216 241.8659868 115.24192160000007\n",
      "(2014)[37] 244.95439280000002 105.2793216 290.60302600000006 115.24192160000007\n",
      "WSJonly,discriminative 302.558 105.2793216 402.5227284 115.24192160000007\n",
      "88.3 429.008 105.2793216 446.44255 115.24192160000007\n",
      "Petrovetal. 172.58599999999996 116.1883216 220.30685399999996 126.15092160000006\n",
      "(2006)[29] 223.39525999999995 116.1883216 269.0438932 126.15092160000006\n",
      "WSJonly,discriminative 302.558 116.1883216 402.5227284 126.15092160000006\n",
      "90.4 429.008 116.1883216 446.44255 126.15092160000006\n",
      "Zhuetal. 177.493 127.09732159999999 215.400693 137.05992160000005\n",
      "(2013)[40] 218.489099 127.09732159999999 264.1377322 137.05992160000005\n",
      "WSJonly,discriminative 302.558 127.09732159999999 402.5227284 137.05992160000005\n",
      "90.4 429.008 127.09732159999999 446.44255 137.05992160000005\n",
      "Dyeretal. 178.051 138.00632159999998 219.8241818 147.96892160000004\n",
      "(2016)[8] 222.91258779999998 138.00632159999998 263.579921 147.96892160000004\n",
      "WSJonly,discriminative 302.558 138.00632159999998 402.5227284 147.96892160000004\n",
      "91.7 429.008 138.00632159999998 446.44255 147.96892160000004\n",
      "Transformer(4layers) 175.899 148.91532159999997 265.73176420000004 158.87792160000004\n",
      "WSJonly,discriminative 302.558 148.91532159999997 402.5227284 158.87792160000004\n",
      "91.3 429.008 148.91532159999997 446.44255 158.87792160000004\n",
      "Zhuetal. 177.493 159.82432159999996 215.400693 169.78692160000003\n",
      "(2013)[40] 218.489099 159.82432159999996 264.1377322 169.78692160000003\n",
      "semi-supervised 320.167 159.82432159999996 384.91393739999995 169.78692160000003\n",
      "91.3 429.008 159.82432159999996 446.44255 169.78692160000003\n",
      "Huang&Harper(2009)[14] 163.27099999999996 170.73432159999993 278.3589552 180.6969216\n",
      "semi-supervised 320.167 170.73432159999993 384.91393739999995 180.6969216\n",
      "91.3 429.008 170.73432159999993 446.44255 180.6969216\n",
      "McCloskyetal. 164.83499999999998 181.64332159999992 228.05765959999997 191.6059216\n",
      "(2006)[26] 231.1460656 181.64332159999992 276.79469880000005 191.6059216\n",
      "semi-supervised 320.167 181.64332159999992 384.91393739999995 191.6059216\n",
      "92.1 429.008 181.64332159999992 446.44255 191.6059216\n",
      "Vinyals&Kaiserelal. 151.027 192.5523215999999 241.8659868 202.51492159999998\n",
      "(2014)[37] 244.95439280000002 192.5523215999999 290.60302600000006 202.51492159999998\n",
      "semi-supervised 320.167 192.55232160000003 384.91393739999995 202.5149216000001\n",
      "92.1 429.008 192.55232160000003 446.44255 202.5149216000001\n",
      "Transformer(4layers) 175.899 203.46132160000002 265.73176420000004 213.42392160000009\n",
      "semi-supervised 320.167 203.46132160000002 384.91393739999995 213.42392160000009\n",
      "92.7 429.008 203.46132160000002 446.44255 213.42392160000009\n",
      "Luongetal. 172.511 214.3703216 220.381293 224.33292160000008\n",
      "(2015)[23] 223.469699 214.3703216 269.11833220000005 224.33292160000008\n",
      "multi-task 332.336 214.3703216 372.7443056 224.33292160000008\n",
      "93.0 429.008 214.3703216 446.44255 224.33292160000008\n",
      "Dyeretal. 178.051 225.2793216 219.8241818 235.24192160000007\n",
      "(2016)[8] 222.91258779999998 225.2793216 263.579921 235.24192160000007\n",
      "generative 331.992 225.2793216 373.08772500000003 235.24192160000007\n",
      "93.3 429.008 225.2793216 446.44255 235.24192160000007\n",
      "increasedthemaximumoutputlengthtoinputlength+300. 108.0 261.6081444 344.58427155 271.78992160000007\n",
      "Weusedabeamsizeof21andα=0.3 347.68169370299995 261.6081444 504.24929999999995 271.78992160000007\n",
      "forbothWSJonlyandthesemi-supervisedsetting. 108.0 272.7363216 311.66543180000014 282.6989216\n",
      "Our 108.0 289.1253216 123.80167986 299.08792159999996\n",
      "results 126.860397312 289.1253216 153.39299288400002 299.08792159999996\n",
      "in 156.45171033600002 289.1253216 164.357631192 299.08792159999996\n",
      "Table 167.416348644 289.1253216 189.75209934000003 299.08792159999996\n",
      "4 192.81081679200003 289.1253216 197.89174279200003 299.08792159999996\n",
      "show 200.95046024400006 289.1253216 222.15824536800005 299.08792159999996\n",
      "that 225.21696282000005 289.1253216 240.45974082000006 299.08792159999996\n",
      "despite 243.51845827200006 289.1253216 272.30698498800007 299.08792159999996\n",
      "the 275.36570244000006 289.1253216 287.783485584 299.08792159999996\n",
      "lack 290.8422030360001 289.1253216 307.7718484680001 299.08792159999996\n",
      "of 310.83056592000014 289.1253216 319.2953886360001 299.08792159999996\n",
      "task-specific 322.3541060880001 289.1253216 373.15320423600014 299.08792159999996\n",
      "tuning 376.2119216880002 289.1253216 402.1856154 299.08792159999996\n",
      "our 405.24433285200007 289.1253216 418.790081568 299.08792159999996\n",
      "model 421.84879902 289.1253216 447.25342902 299.08792159999996\n",
      "performs 450.312146472 289.1253216 486.99643219199993 299.08792159999996\n",
      "sur- 490.0551496439999 289.1253216 505.65359246399987 299.08792159999996\n",
      "prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe 108.0 300.0343216 504.0013948799997 309.9969216\n",
      "RecurrentNeuralNetworkGrammar[8]. 108.0 310.9433216 271.39660260000005 320.9059216\n",
      "IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley- 108.0 327.3323216 505.65672295720015 337.29492159999995\n",
      "Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences. 108.0 338.2413216 419.56038979999994 348.2039216\n",
      "7 108.0 364.5404368 113.9776 376.4956368\n",
      "Conclusion 125.9328 364.5404368 183.0667008 376.4956368\n",
      "Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon 108.0 388.8513216 503.9967921587998 398.81392159999996\n",
      "attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith 108.0 399.7603216 503.99553687119993 409.7229216\n",
      "multi-headedself-attention. 108.0 410.6693216 218.12658040000005 420.6319216\n",
      "For 108.0 427.0583216 121.962384648 437.0209216\n",
      "translation 124.97029284 427.0583216 167.873631984 437.0209216\n",
      "tasks, 170.881540176 427.0583216 193.745707176 437.0209216\n",
      "the 196.88571944400002 427.0583216 209.30350258800001 437.0209216\n",
      "Transformer 212.31141078000002 427.0583216 262.74468225600003 437.0209216\n",
      "can 265.7525904480001 427.0583216 279.8572410240001 437.0209216\n",
      "be 282.86514921600013 427.0583216 292.45793750400014 437.0209216\n",
      "trained 295.46584569600014 427.0583216 323.6853087000002 437.0209216\n",
      "significantly 326.69321689200024 427.0583216 376.9435750320002 437.0209216\n",
      "faster 379.9514832240002 427.0583216 402.41933799600014 437.0209216\n",
      "than 405.42724618800014 427.0583216 422.92595533200006 437.0209216\n",
      "architectures 425.93386352400006 427.0583216 477.8507653919999 437.0209216\n",
      "based 480.8586735839999 427.0583216 503.99721058799986 437.0209216\n",
      "on 108.0 437.9673216 118.16185200000001 447.9299216\n",
      "recurrent 121.66769094 437.9673216 158.34181480799998 447.9299216\n",
      "or 161.847653748 437.9673216 170.312476464 447.9299216\n",
      "convolutional 173.808153552 437.9673216 229.09879028400005 447.9299216\n",
      "layers. 232.60462922400006 437.9673216 259.41159480000005 447.9299216\n",
      "On 265.53919155600005 437.9673216 277.95697470000005 447.9299216\n",
      "both 281.46281364000004 437.9673216 299.53058649600007 447.9299216\n",
      "WMT 303.03642543600006 437.9673216 327.8719917240001 447.9299216\n",
      "2014 331.3676688120001 437.9673216 351.69137281200005 447.9299216\n",
      "English-to-German 355.1972117520001 437.9673216 433.6568710439999 447.9299216\n",
      "and 437.1627099839999 437.9673216 451.8364242719999 447.9299216\n",
      "WMT 455.34226321199986 437.9673216 480.17782949999986 447.9299216\n",
      "2014 483.67350658799984 437.9673216 503.99721058799975 447.9299216\n",
      "English-to-Frenchtranslationtasks,weachieveanewstateoftheart. 108.0 448.8763216 391.6376130240001 458.8389216\n",
      "Intheformertaskourbest 395.4787930800001 448.8763216 503.997210588 458.8389216\n",
      "modeloutperformsevenallpreviouslyreportedensembles. 108.0 459.7853216 343.92433060000013 469.7479216\n",
      "Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. 107.532 476.1733216 487.9305286655998 486.13592159999996\n",
      "We 491.02642654079983 476.1733216 503.99546026799976 486.13592159999996\n",
      "plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand 108.0 487.0833216 504.00029899399954 497.0459216\n",
      "toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs 108.0 497.99232159999997 503.99721058799975 507.9549216\n",
      "suchasimages,audioandvideo. 108.0 508.90132159999996 236.98068134880006 518.8639215999999\n",
      "Makinggenerationlesssequentialisanotherresearchgoalsofours. 240.06868884480002 508.90132159999996 505.60792390559965 518.8639215999999\n",
      "The 107.691 525.2893216 123.49267986000001 535.2519216000001\n",
      "code 128.47198734 525.2893216 147.65756391600002 535.2519216000001\n",
      "we 152.636871396 525.2893216 164.48559082800003 535.2519216000001\n",
      "used 169.46489830800002 525.2893216 188.09157302400004 535.2519216000001\n",
      "to 193.07088050400003 525.2893216 200.97680136000005 535.2519216000001\n",
      "train 205.95610884000004 525.2893216 224.57262170400006 535.2519216000001\n",
      "and 229.55192918400007 525.2893216 244.2256434720001 535.2519216000001\n",
      "evaluate 249.2049509520001 525.2893216 282.5663110680001 535.2519216000001\n",
      "our 287.5456185480001 525.2893216 301.09136726400016 535.2519216000001\n",
      "models 306.07067474400014 525.2893216 335.4282651720001 535.2519216000001\n",
      "is 340.40757265200017 525.2893216 347.1855279360002 535.2519216000001\n",
      "available 352.1648354160002 525.2893216 388.4019996480001 535.2519216000001\n",
      "at 393.3813071280001 525.2893216 400.7181642720001 535.2519216000001\n",
      "https://github.com/ 405.693 525.3490972 505.05100605999996 535.3116972\n",
      "tensorflow/tensor2tensor. 108.0 536.1983216 235.98964999999998 546.2206972\n",
      "Acknowledgements 108.0 558.7935834 192.00803048400005 568.7561834\n",
      "WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful 201.972 558.8633216000001 504.0025651440001 568.8259216\n",
      "comments,correctionsandinspiration. 108.0 569.7723215999999 262.6793276000001 579.7349216\n",
      "References 108.0 596.0724368 163.54385919999999 608.0276368\n",
      "[1] 112.981 614.9033216 124.5973916 624.8659216\n",
      "JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. 129.579 614.9033216 358.19255554280005 624.8659216\n",
      "Layernormalization. 361.8074650276001 614.9033216 444.2332951551999 624.8659216\n",
      "arXivpreprint 447.844 614.6741818 504.0027976212 624.6367818\n",
      "arXiv:1607.06450,2016. 129.579 625.5831817999999 229.75314999999998 635.7749216\n",
      "[2] 112.981 643.6963216 124.5973916 653.6589216\n",
      "DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. 129.579 643.6963216 355.6493229400001 653.6589216\n",
      "Neuralmachinetranslationbyjointly 358.8907544760001 643.6963216 504.3451129799997 653.6589216\n",
      "learningtoalignandtranslate. 129.579 654.6053216 250.49507620000006 664.5679216\n",
      "CoRR,abs/1409.0473,2014. 254.08100000000002 654.3761818 368.65024860000005 664.5679216\n",
      "[3] 112.98100000000002 672.4883216 124.59739160000002 682.4509216\n",
      "DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. 129.579 672.4883216000001 383.79258201660014 682.4509216\n",
      "Massiveexplorationofneural 387.3989336280001 672.4883216000001 504.00430239659977 682.4509216\n",
      "machinetranslationarchitectures. 129.579 683.3973215999999 263.7652594000001 693.3599216\n",
      "CoRR,abs/1703.03906,2017. 267.35 683.1681818 386.9015486 693.3599216\n",
      "[4] 112.98100000000002 701.2803216 124.59739160000002 711.2429216\n",
      "JianpengCheng,LiDong,andMirellaLapata. 129.579 701.2803216 312.506682144 711.2429216\n",
      "Longshort-termmemory-networksformachine 316.08484956000007 701.2803216 504.00235968959987 711.2429216\n",
      "reading. 129.579 712.1893216 161.9474874 722.1519216\n",
      "arXivpreprintarXiv:1601.06733,2016. 165.534 711.9601818 325.11415 722.1519216\n",
      "10 301.019 742.0773216 310.98159999999996 752.0399216\n",
      "[5] 112.981 74.15232159999994 124.5973916 84.1149216\n",
      "KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, 129.579 74.15232159999994 505.2423447359999 84.1149216\n",
      "andYoshuaBengio. 129.579 85.06132159999993 209.7286948308 95.0239216\n",
      "Learningphraserepresentationsusingrnnencoder-decoderforstatistical 213.3523514784 85.06132159999993 504.0035552015999 95.0239216\n",
      "machinetranslation. 129.579 95.97032159999992 210.375686 105.93292159999999\n",
      "CoRR,abs/1406.1078,2014. 213.961 95.74118179999994 328.5312486 105.93292159999999\n",
      "[6] 112.981 115.77132159999996 124.5973916 125.73392160000003\n",
      "Francois 129.579 115.77132159999996 164.576418288 125.73392160000003\n",
      "Chollet. 167.919667596 115.77132159999996 200.386784736 125.73392160000003\n",
      "Xception: 206.69729482800003 115.77132159999996 246.77563911600004 125.73392160000003\n",
      "Deep 251.56187140800006 115.77132159999996 273.003379128 125.73392160000003\n",
      "learning 276.34662843600006 115.77132159999996 309.64701744000007 125.73392160000003\n",
      "with 312.99026674800007 115.77132159999996 331.0580396040001 125.73392160000003\n",
      "depthwise 334.40128891200015 115.77132159999996 375.60759877200013 125.73392160000003\n",
      "separable 378.9508480800001 115.77132159999996 417.3220012320001 125.73392160000003\n",
      "convolutions. 420.66525054000016 115.77132159999996 475.1124535559999 125.73392160000003\n",
      "arXiv 481.421 115.54218179999998 504.00063514399994 125.50478180000005\n",
      "preprintarXiv:1610.02357,2016. 129.579 126.45118179999997 264.53215 136.64292160000002\n",
      "[7] 112.981 146.4813216 124.5973916 156.44392160000007\n",
      "JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. 129.579 146.4813216 418.95932465420003 156.44392160000007\n",
      "Empiricalevaluation 422.5730286644 146.4813216 504.00378434139975 156.44392160000007\n",
      "ofgatedrecurrentneuralnetworksonsequencemodeling. 129.579 157.3903216 360.7212826000001 167.35292160000006\n",
      "CoRR,abs/1412.3555,2014. 364.307 157.1611818 478.87624860000005 167.35292160000006\n",
      "[8] 112.98100000000005 177.19132160000004 124.59739160000005 187.1539216000001\n",
      "Chris 129.579 177.19132159999992 151.59973328400002 187.1539216\n",
      "Dyer, 154.62796518000002 177.19132159999992 177.0856581 187.1539216\n",
      "Adhiguna 180.245994072 177.19132159999992 220.32433836000004 187.1539216\n",
      "Kuncoro, 223.35257025600004 177.19132159999992 261.30708747600005 187.1539216\n",
      "Miguel 264.46742344800003 177.19132159999992 293.82501387600007 187.1539216\n",
      "Ballesteros, 296.85324577200004 177.19132159999992 344.5529790600001 187.1539216\n",
      "and 347.7234768840001 177.19132159999992 362.39719117200013 187.1539216\n",
      "Noah 365.4254230680001 177.19132159999992 387.4359945000001 187.1539216\n",
      "A. 390.46422639600013 177.19132159999992 400.34154654000014 187.1539216\n",
      "Smith. 403.3697784360001 177.19132159999992 430.19706771600005 187.1539216\n",
      "Recurrent 435.5117163120001 177.19132159999992 475.57989874800006 187.1539216\n",
      "neural 478.608130644 177.19132159999992 504.00259879199996 187.1539216\n",
      "networkgrammars. 129.579 188.10032159999992 206.9485516 198.06292159999998\n",
      "InProc.ofNAACL,2016. 210.5350876 187.87118179999993 313.29915000000005 198.06292159999998\n",
      "[9] 112.98100000000005 207.90132159999996 124.59739160000005 217.86392160000003\n",
      "JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. 129.579 207.90132159999996 464.5786379223999 217.86392160000003\n",
      "Convolu- 468.1578015983999 207.90132159999996 505.6533219111997 217.86392160000003\n",
      "tionalsequencetosequencelearning. 129.579 218.81032159999995 278.1512538000001 228.77292160000002\n",
      "arXivpreprintarXiv:1705.03122v2,2017. 281.73699999999997 218.58118179999997 450.72214999999994 228.77292160000002\n",
      "[10] 107.99999999999994 238.6113216 124.59769159999995 248.57392160000006\n",
      "Alex 129.579 238.6113216 149.18121250800002 248.57392160000006\n",
      "Graves. 154.4755374 238.6113216 185.44886229600002 248.57392160000006\n",
      "Generating 197.998749516 238.6113216 243.14785795200004 248.57392160000006\n",
      "sequences 248.44218284400006 238.6113216 289.6383308520001 248.57392160000006\n",
      "with 294.93265574400004 238.6113216 312.99026674800007 248.57392160000006\n",
      "recurrent 318.2845916400001 238.6113216 354.9587155080001 248.57392160000006\n",
      "neural 360.25304040000015 238.6113216 385.64750854800013 248.57392160000006\n",
      "networks. 390.94183344000015 238.6113216 430.63402735200003 248.57392160000006\n",
      "arXiv 443.187 238.3821818 465.76663514399996 248.34478180000008\n",
      "preprint 471.060960036 238.3821818 503.995522368 248.34478180000008\n",
      "arXiv:1308.0850,2013. 129.579 249.2911818 224.77115 259.48292160000005\n",
      "[11] 108.00000000000003 269.32132160000003 124.59769160000003 279.2839216000001\n",
      "Kaiming 129.579 269.3213215999999 165.14548200000002 279.2839216\n",
      "He, 168.234685008 269.3213215999999 182.62386744 279.2839216\n",
      "Xiangyu 185.865498228 269.3213215999999 220.862916516 279.2839216\n",
      "Zhang, 223.952119524 269.3213215999999 252.45611438400005 279.2839216\n",
      "Shaoqing 255.69774517200005 269.3213215999999 294.08922202800005 279.2839216\n",
      "Ren, 297.1784250360001 269.3213215999999 316.0896316080001 279.2839216\n",
      "and 319.33126239600006 269.3213215999999 334.0049766840001 279.2839216\n",
      "Jian 337.0941796920001 269.3213215999999 353.46492326400005 279.2839216\n",
      "Sun. 356.5541262720001 269.3213215999999 374.9064309840001 279.2839216\n",
      "Deep 380.4141547680001 269.3213215999999 401.8556624880001 279.2839216\n",
      "residual 404.9448654960001 269.3213215999999 437.117288928 279.2839216\n",
      "learning 440.20649193599996 269.3213215999999 473.50688093999986 279.2839216\n",
      "for 476.5960839479999 269.3213215999999 488.4448033799998 279.2839216\n",
      "im- 491.53400638799985 269.3213215999999 505.64881881599985 279.2839216\n",
      "age 129.579 280.2303216 143.68365057600002 290.1929216\n",
      "recognition. 147.016738032 280.2303216 195.84443689199998 290.1929216\n",
      "In 202.104137724 280.2303216 210.56896044000004 290.1929216\n",
      "Proceedings 213.903 280.0011818 264.25497665999995 289.9637818\n",
      "of 267.577902264 280.0011818 275.48382312 289.9637818\n",
      "the 278.816910576 280.0011818 291.23469372 289.9637818\n",
      "IEEE 294.567781176 280.0011818 316.57835260800005 289.9637818\n",
      "Conference 319.901278212 280.0011818 366.38158926000006 289.9637818\n",
      "on 369.7045148640001 280.0011818 379.86636686400004 289.9637818\n",
      "Computer 383.1994543200001 280.0011818 423.8468623200001 289.9637818\n",
      "Vision 427.16978792400016 280.0011818 452.4118282920001 289.9637818\n",
      "and 455.73475389600014 280.0011818 470.9775318960002 289.9637818\n",
      "Pattern 474.31061935200023 280.0011818 504.0035508960001 289.9637818\n",
      "Recognition,pages770–778,2016. 129.27 290.9111818 269.74304020000005 301.1029216\n",
      "[12] 108.0 310.94132160000004 124.5976916 320.9039216\n",
      "SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. 129.579 310.94132160000004 434.4132001856001 320.9039216\n",
      "Gradientflowin 437.9862667504001 310.94132160000004 504.0019611855999 320.9039216\n",
      "recurrentnets: 129.579 321.85032160000003 186.84402480000003 331.8129216\n",
      "thedifficultyoflearninglong-termdependencies,2001. 189.93243080000002 321.85032160000003 412.1482238000001 331.8129216\n",
      "[13] 108.0 341.65132159999996 124.5976916 351.61392159999997\n",
      "Sepp 129.579 341.65132159999996 149.90270400000003 351.61392159999997\n",
      "Hochreiter 153.032554416 341.65132159999996 196.48463356800002 351.61392159999997\n",
      "and 199.614483984 341.65132159999996 214.27803642000003 351.61392159999997\n",
      "Jürgen 217.40788683600005 341.65132159999996 244.32663278400005 351.61392159999997\n",
      "Schmidhuber. 247.44632134800005 341.65132159999996 303.63120105600007 351.61392159999997\n",
      "Long 309.2608670640001 341.65132159999996 330.7125366360001 351.61392159999997\n",
      "short-term 333.8423870520001 341.65132159999996 376.17666248400013 351.61392159999997\n",
      "memory. 379.29635104800013 341.65132159999996 415.0660700880001 351.61392159999997\n",
      "Neural 420.688 341.4221818 448.765197076 351.3847818\n",
      "computation, 451.895047492 341.4221818 505.245463 351.61392159999997\n",
      "9(8):1735–1780,1997. 129.579 352.56032159999995 221.1751444 362.52292159999996\n",
      "[14] 108.0 372.3613216 124.5976916 382.32392159999995\n",
      "ZhongqiangHuangandMaryHarper. 129.579 372.3613216 282.7689189000001 382.3239216\n",
      "Self-trainingPCFGgrammarswithlatentannotations 286.4576711760001 372.3613216 504.0025987919998 382.3239216\n",
      "acrosslanguages. 129.579 383.2703216 200.905039188 393.2329216\n",
      "InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural 205.33560666 383.0411818 504.0013075240001 393.2329216\n",
      "LanguageProcessing,pages832–841.ACL,August2009. 129.3 393.9501818 363.47063760000003 404.1419216\n",
      "[15] 108.0 413.9803216 124.5976916 423.9429216\n",
      "RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. 129.579 413.9803216 460.12232709199986 423.9429216\n",
      "Exploring 463.67124431439987 413.9803216 503.9998491143997 423.9429216\n",
      "thelimitsoflanguagemodeling. 129.579 424.8893216 258.8138472000001 434.85192159999997\n",
      "arXivpreprintarXiv:1602.02410,2016. 262.399 424.6601818 421.98015 434.85192159999997\n",
      "[16] 107.99999999999994 444.69032159999995 124.59769159999995 454.65292159999996\n",
      "ŁukaszKaiserandSamyBengio. 129.579 444.69032159999995 260.43027905 454.65292159999996\n",
      "Canactivememoryreplaceattention? 264.00117376800006 444.69032159999995 412.0905801964 454.65292159999996\n",
      "InAdvancesinNeural 416.2680378528 444.46118179999996 503.99519281600004 454.65292159999996\n",
      "InformationProcessingSystems,(NIPS),2016. 129.41 455.3701818 317.40315 465.5619216\n",
      "[17] 107.99999999999997 475.4003216 124.59769159999998 485.3629216\n",
      "ŁukaszKaiserandIlyaSutskever. 129.579 475.4003216 263.8777172288001 485.3629216\n",
      "NeuralGPUslearnalgorithms. 267.44544384000005 475.4003216 389.88468202880017 485.3629216\n",
      "InInternationalConference 393.45240864000016 475.1711818 504.00404771840005 485.3629216\n",
      "onLearningRepresentations(ICLR),2016. 129.579 486.0801818 302.99715000000003 496.2719216\n",
      "[18] 108.00000000000003 506.1103216 124.59769160000003 516.0729216\n",
      "NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- 129.579 506.1103216 505.6534016119998 516.0729216\n",
      "rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 129.579 516.7901818 505.24483699999996 526.9819216\n",
      "2017. 129.579 527.9283216 151.99484999999999 537.8909216\n",
      "[19] 108.0 547.7293215999999 124.5976916 557.6919216\n",
      "YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. 129.579 547.7293216 383.6680993296001 557.6919216\n",
      "Structuredattentionnetworks. 387.29528273760013 547.7293216 505.7473290047997 557.6919216\n",
      "InInternationalConferenceonLearningRepresentations,2017. 129.579 558.4091817999999 386.19415000000004 568.6009216\n",
      "[20] 108.00000000000003 578.4393216 124.59769160000003 588.4019216\n",
      "DiederikKingmaandJimmyBa. 129.579 578.4403216000001 260.22993116400005 588.4029216\n",
      "Adam: 263.81642731360006 578.4403216000001 290.63632434440007 588.4029216\n",
      "Amethodforstochasticoptimization. 293.7203169126001 578.4403216000001 442.98358661119994 588.4029216\n",
      "InICLR,2015. 446.57008276079995 578.2111818000001 505.7453403842 588.4029216\n",
      "[21] 108.0 598.2413216 124.5976916 608.2039216000001\n",
      "OleksiiKuchaievandBorisGinsburg. 129.579 598.2413216 280.03268084740006 608.2039216000001\n",
      "FactorizationtricksforLSTMnetworks. 283.646175643 598.2413216 443.9431742805999 608.2039216000001\n",
      "arXivpreprint 447.567 598.0121818 504.0007056056 607.9747818\n",
      "arXiv:1703.10722,2017. 129.579 608.9211818 229.75314999999998 619.1129216\n",
      "[22] 108.0 628.9513216 124.5976916 638.9139216\n",
      "Zhouhan 129.579 628.9513216 165.70438386 638.9139216\n",
      "Lin, 170.08414207200002 628.9513216 186.7394175 638.9139216\n",
      "Minwei 191.57645905200002 628.9513216 223.18998062400001 638.9139216\n",
      "Feng, 227.569738836 628.9513216 250.433905836 638.9139216\n",
      "Cicero 255.28110924000003 628.9513216 282.372606672 638.9139216\n",
      "Nogueira 286.742203032 628.9513216 324.55445432400006 638.9139216\n",
      "dos 328.93421253600013 628.9513216 343.04902496400007 638.9139216\n",
      "Santos, 347.4186213240001 628.9513216 377.0607436080001 638.9139216\n",
      "Mo 381.90794701200014 628.9513216 396.0227594400001 638.9139216\n",
      "Yu, 400.40251765200014 628.9513216 414.25312192800004 638.9139216\n",
      "Bing 419.10032533200007 628.9513216 438.865127472 638.9139216\n",
      "Xiang, 443.234723832 628.9513216 470.61075311999986 638.9139216\n",
      "Bowen 475.4579565239999 628.9513216 504.00259879199984 638.9139216\n",
      "Zhou, 129.579 639.8603216 153.571132572 649.8229216\n",
      "and 156.741630396 639.8603216 171.41534468400002 649.8229216\n",
      "Yoshua 174.453738432 639.8603216 204.400716276 649.8229216\n",
      "Bengio. 207.439110024 639.8603216 239.32700160000002 649.8229216\n",
      "A 244.66197390000005 639.8603216 251.99883104400004 649.8229216\n",
      "structured 255.02706294000004 639.8603216 295.66430908800004 649.8229216\n",
      "self-attentive 298.70270283600007 639.8603216 351.3614199000001 649.8229216\n",
      "sentence 354.3896517960001 639.8603216 389.3769082320001 649.8229216\n",
      "embedding. 392.41530198000015 639.8603216 440.115035268 649.8229216\n",
      "arXiv 445.444 639.6311817999999 468.02363514399997 649.5937818\n",
      "preprint 471.062028892 639.6311817999999 503.996591224 649.5937818\n",
      "arXiv:1703.03130,2017. 129.579 650.5401818 229.75314999999998 660.7319216000001\n",
      "[23] 108.0 670.5703216 124.5976916 680.5329216\n",
      "Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. 129.579 670.5703215999999 459.24571791799997 680.5329216\n",
      "Multi-task 462.8612251588 670.5703215999999 504.2508368961998 680.5329216\n",
      "sequencetosequencelearning. 129.579 681.4793216 252.96580100000006 691.4419216\n",
      "arXivpreprintarXiv:1511.06114,2015. 256.552 681.2501818000001 416.13215 691.4419216\n",
      "[24] 108.0 701.2803216 124.5976916 711.2429216\n",
      "Minh-ThangLuong,HieuPham,andChristopherDManning. 129.579 701.2803216 371.5733435280001 711.2429216\n",
      "Effectiveapproachestoattention- 374.7171415840001 701.2803216 505.6534016119997 711.2429216\n",
      "basedneuralmachinetranslation. 129.579 712.1893216 262.93836360000006 722.1519216\n",
      "arXivpreprintarXiv:1508.04025,2015. 266.524 711.9601818 426.10415 722.1519216\n",
      "11 301.019 742.0773216 310.98159999999996 752.0399216\n",
      "[25] 108.0 74.15232159999994 124.5976916 84.1149216\n",
      "MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated 129.579 74.15232159999994 504.00339579999974 84.1149216\n",
      "corpusofenglish: 129.579 85.06132159999993 200.97099160000002 95.0239216\n",
      "Thepenntreebank. 204.0593976 85.06132159999993 280.6917168000001 95.0239216\n",
      "Computationallinguistics,19(2):313–330,1993. 284.278 84.83218179999994 479.1151444 95.0239216\n",
      "[26] 108.0 108.54432159999988 124.5976916 118.50692159999994\n",
      "DavidMcClosky,EugeneCharniak,andMarkJohnson. 129.579 108.54432159999999 352.5613652570001 118.50692160000006\n",
      "Effectiveself-trainingforparsing. 356.1786458406001 108.54432159999999 492.0564949449998 118.50692160000006\n",
      "In 495.6737755285999 108.54432159999999 503.99751786599995 118.50692160000006\n",
      "ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference, 129.27 119.2241818 505.24521545 129.41592160000005\n",
      "pages152–159.ACL,June2006. 129.579 130.36232159999997 262.2808320000001 140.32492160000004\n",
      "[27] 108.0 153.84532159999992 124.5976916 163.8079216\n",
      "AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. 129.579 153.84532159999992 398.83261114400005 163.8079216\n",
      "Adecomposableattention 402.17167616000006 153.84532159999992 504.00339579999974 163.8079216\n",
      "model. 129.579 164.75432160000003 156.97615 174.7169216000001\n",
      "InEmpiricalMethodsinNaturalLanguageProcessing,2016. 160.562686 164.52518180000004 407.44515 174.7169216000001\n",
      "[28] 108.0 188.23732159999997 124.5976916 198.19992160000004\n",
      "RomainPaulus,CaimingXiong,andRichardSocher. 129.579 188.23732159999997 341.96828211600007 198.19992160000004\n",
      "Adeepreinforcedmodelforabstractive 345.5313263052001 188.23732159999997 503.997218988 198.19992160000004\n",
      "summarization. 129.579 199.14632159999996 191.2873444 209.10892160000003\n",
      "arXivpreprintarXiv:1705.04304,2017. 194.873 198.91718179999998 354.45315 209.10892160000003\n",
      "[29] 107.99999999999997 222.6293215999999 124.59769159999998 232.59192159999998\n",
      "Slav 129.579 222.62932160000003 147.443535816 232.5919216000001\n",
      "Petrov, 150.929051052 222.62932160000003 179.19932331599998 232.5919216000001\n",
      "Leon 182.928723 222.62932160000003 203.81132886 232.5919216000001\n",
      "Barrett, 207.28668224400002 222.62932160000003 238.04660824800004 232.5919216000001\n",
      "Romain 241.77600793200003 222.62932160000003 273.95859321600005 232.5919216000001\n",
      "Thibaux, 277.43394660000007 222.62932160000003 313.84386231600007 232.5919216000001\n",
      "and 317.5732620000001 222.62932160000003 332.24697628800004 232.5919216000001\n",
      "Dan 335.7223296720001 222.62932160000003 352.65197510400014 232.5919216000001\n",
      "Klein. 356.12732848800016 222.62932160000003 381.24742663200016 232.5919216000001\n",
      "Learning 387.99489636000015 222.62932160000003 424.67918208000003 232.5919216000001\n",
      "accurate, 428.16469731600006 222.62932160000003 464.55428932800004 232.5919216000001\n",
      "compact, 468.27352716000007 222.62932160000003 505.242344736 232.5919216000001\n",
      "and 129.579 233.53832160000002 144.25271428800002 243.50092160000008\n",
      "interpretable 147.71790582 233.53832160000002 199.075905828 243.50092160000008\n",
      "tree 202.54109736 233.53832160000002 217.77371350800004 243.50092160000008\n",
      "annotation. 221.23890504000002 233.53832160000002 266.6827071840001 243.50092160000008\n",
      "In 273.38952950400005 233.53832160000002 281.85435222 243.50092160000008\n",
      "Proceedings 285.32 233.30918180000003 335.67197666 243.2717818000001\n",
      "of 339.137168192 233.30918180000003 347.04308904799996 243.2717818000001\n",
      "the 350.50828058 233.30918180000003 362.92606372399996 243.2717818000001\n",
      "21st 366.391255256 233.30918180000003 383.33106254 243.2717818000001\n",
      "International 386.796254072 233.30918180000003 440.43050892800005 243.2717818000001\n",
      "Conference 443.89570046000006 233.30918180000003 490.3760115080001 243.2717818000001\n",
      "on 493.8412030400001 233.30918180000003 504.00305504000005 243.2717818000001\n",
      "ComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July 129.25 244.21818180000002 504.346771288 254.40992160000008\n",
      "2006. 129.579 255.3563216 151.99484999999999 265.31892160000007\n",
      "[30] 108.0 278.83932160000006 124.5976916 288.8019216\n",
      "OfirPressandLiorWolf. 129.579 278.83932159999995 233.71765929600002 288.80192159999996\n",
      "Usingtheoutputembeddingtoimprovelanguagemodels. 238.54453899600003 278.83932159999995 476.585922096 288.80192159999996\n",
      "arXiv 481.421 278.61018179999996 504.00063514399994 288.5727818\n",
      "preprintarXiv:1608.05859,2016. 129.579 289.5191818 264.53215 299.7109216\n",
      "[31] 108.0 313.2313216 124.5976916 323.1939216\n",
      "RicoSennrich,BarryHaddow,andAlexandraBirch. 129.579 313.2313216 338.1378218176001 323.19392159999995\n",
      "Neuralmachinetranslationofrarewords 341.7549629248001 313.2313216 504.0025190911998 323.19392159999995\n",
      "withsubwordunits. 129.22 324.1403216 207.99427819999997 334.1029216\n",
      "arXivpreprintarXiv:1508.07909,2015. 211.58100000000002 323.9111818 371.16115 334.1029216\n",
      "[32] 108.0 347.62332160000005 124.5976916 357.5859216\n",
      "NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, 129.579 347.62332160000005 505.24880050079986 357.5859216\n",
      "andJeffDean. 129.579 358.53232160000005 189.168100128 368.4949216\n",
      "Outrageouslylargeneuralnetworks: 194.30999724000003 358.53232160000005 343.4961464520001 368.4949216\n",
      "Thesparsely-gatedmixture-of-experts 347.56088725200016 358.53232160000005 504.00259879199996 368.4949216\n",
      "layer. 129.579 369.44132160000004 151.4369444 379.4039216\n",
      "arXivpreprintarXiv:1701.06538,2017. 155.023 369.2121818 314.60415 379.4039216\n",
      "[33] 108.0 392.9243216 124.5976916 402.8869216\n",
      "NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- 129.579 392.9243216 505.6512098399998 402.8869216\n",
      "nov. 129.579 403.8333216 146.44292215980002 413.7959216\n",
      "Dropout: 150.0256225588 403.8333216 186.4682454906 413.7959216\n",
      "asimplewaytopreventneuralnetworksfromoverfitting. 189.5463401996 403.8333216 421.29154938900007 413.7959216\n",
      "JournalofMachine 424.865 403.6041818 503.9972643058 413.5667818\n",
      "LearningResearch,15(1):1929–1958,2014. 129.3 414.5131818 306.43474440000006 424.7049216\n",
      "[34] 108.0 438.22532160000003 124.5976916 448.1879216\n",
      "Sainbayar 129.579 438.22532160000003 170.216246148 448.1879216\n",
      "Sukhbaatar, 173.37658212000002 438.22532160000003 221.23890504000002 448.1879216\n",
      "Arthur 224.56183064400003 438.22532160000003 251.65332807600007 448.1879216\n",
      "Szlam, 254.81366404800005 438.22532160000003 282.74859519600005 448.1879216\n",
      "Jason 286.08168265200004 438.22532160000003 308.66131779600005 448.1879216\n",
      "Weston, 311.8114919160001 438.22532160000003 344.6037883200001 448.1879216\n",
      "and 347.92671392400007 438.22532160000003 362.6004282120001 448.1879216\n",
      "Rob 365.7607641840001 438.22532160000003 382.70057146800013 448.1879216\n",
      "Fergus. 385.8609074400001 438.22532160000003 415.879018248 448.1879216\n",
      "End-to-end 421.6001409240001 438.22532160000003 467.3183130719999 448.1879216\n",
      "memory 470.4786490439999 438.22532160000003 504.3481017599999 448.1879216\n",
      "networks. 129.579 449.1353216 169.271193912 459.0979216\n",
      "InC.Cortes, 174.413091024 449.1353216 227.22423586800005 459.0979216\n",
      "N.D.Lawrence, 230.32360072800006 449.1353216 298.4791420920001 459.0979216\n",
      "D.D.Lee, 301.5683451000001 449.1353216 345.0509098080001 459.0979216\n",
      "M.Sugiyama, 348.15027466800007 449.1353216 405.8899177320001 459.0979216\n",
      "andR.Garnett, 408.9892825920001 449.1353216 471.941955732 459.0979216\n",
      "editors, 475.041320592 449.1353216 505.2423447359999 459.0979216\n",
      "AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates, 128.971 459.8151818 505.24585387900004 470.0069216\n",
      "Inc.,2015. 129.579 470.9533216 172.1890402 480.9159216\n",
      "[35] 108.0 494.43632160000004 124.5976916 504.3989216\n",
      "IlyaSutskever,OriolVinyals,andQuocVVLe. 129.579 494.43632160000004 325.48934470800003 504.3989216\n",
      "Sequencetosequencelearningwithneural 329.70651328800005 494.43632160000004 504.0025987919998 504.3989216\n",
      "networks. 129.579 505.34532160000003 168.4929156 515.3079216\n",
      "InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014. 172.07945160000003 505.1161818 494.1286402 515.3079216\n",
      "[36] 108.0 528.8283216 124.5976916 538.7909216\n",
      "ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. 129.579 528.8283216 505.7402754839997 538.7909216\n",
      "Rethinkingtheinceptionarchitectureforcomputervision. 129.579 539.7373216 361.17956220000013 549.6999216\n",
      "CoRR,abs/1512.00567,2015. 364.764 539.5081818 484.3155486 549.6999216\n",
      "[37] 108.0 563.2203216 124.5976916 573.1829216\n",
      "Vinyals&Kaiser, 129.579 563.2203216 203.252427 573.1829216\n",
      "Koo, 206.29082074800002 563.2203216 225.974328072 573.1829216\n",
      "Petrov, 229.002559968 563.2203216 257.28299408400005 573.1829216\n",
      "Sutskever, 260.31122598 563.2203216 302.03579029200006 573.1829216\n",
      "andHinton. 305.0640221880001 563.2203216 353.42427585600007 573.1829216\n",
      "Grammarasaforeignlanguage. 358.3934214840001 563.2203216 490.5686304479998 573.1829216\n",
      "In 495.53777607599983 563.2203216 504.0025987919998 573.1829216\n",
      "AdvancesinNeuralInformationProcessingSystems,2015. 128.971 573.9001817999999 365.24315 584.0919216\n",
      "[38] 108.0 597.6123216 124.5976916 607.5749216\n",
      "Yonghui 129.579 597.6123216 164.048001984 607.5749216\n",
      "Wu, 167.665621296 597.6123216 184.381867836 607.5749216\n",
      "Mike 188.28401900400002 597.6123216 209.64423190800002 607.5749216\n",
      "Schuster, 213.26185122000004 597.6123216 250.39325842800005 607.5749216\n",
      "Zhifeng 254.29540959600007 597.6123216 286.4678330280001 607.5749216\n",
      "Chen, 290.0956141920001 597.6123216 314.0877467640001 607.5749216\n",
      "Quoc 317.98989793200013 597.6123216 340.0004693640001 607.5749216\n",
      "V 343.6180886760001 597.6123216 350.95494582000015 607.5749216\n",
      "Le, 354.57256513200014 597.6123216 367.8337819920001 607.5749216\n",
      "Mohammad 371.7359331600002 597.6123216 420.8481638760001 607.5749216\n",
      "Norouzi, 424.4657831880001 597.6123216 460.306635192 607.5749216\n",
      "Wolfgang 464.20878636000003 597.6123216 504.0025987919999 607.5749216\n",
      "Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. 129.579 608.5213216 403.44091140000006 618.4839216\n",
      "Google’sneuralmachine 406.6530528920001 608.5213216 504.0033957999998 618.4839216\n",
      "translationsystem: 129.579 619.4303216000001 205.30731287600003 629.3929216\n",
      "Bridgingthegapbetweenhumanandmachinetranslation. 208.39641625800002 619.4303216000001 442.93684209199995 629.3929216\n",
      "arXivpreprint 446.534 619.2011818000001 503.999372686 629.1637818\n",
      "arXiv:1609.08144,2016. 129.579 630.1101818 229.75314999999998 640.3019216\n",
      "[39] 108.0 653.8223216 124.5976916 663.7849216\n",
      "Jie 129.579 653.8223216 140.868817572 663.7849216\n",
      "Zhou, 144.405142068 653.8223216 168.39727464 663.7849216\n",
      "Ying 172.197807288 653.8223216 191.97277128000002 663.7849216\n",
      "Cao, 195.509095776 653.8223216 214.420302348 663.7849216\n",
      "Xuguang 218.220834996 653.8223216 255.47418442800006 663.7849216\n",
      "Wang, 259.0206707760001 653.8223216 285.02485004400006 663.7849216\n",
      "Peng 288.82538269200006 653.8223216 309.149086692 663.7849216\n",
      "Li, 312.6854111880001 653.8223216 324.25976061600005 663.7849216\n",
      "and 328.06029326400005 653.8223216 342.7340075520001 663.7849216\n",
      "Wei 346.28049390000007 653.8223216 362.4073530240001 663.7849216\n",
      "Xu. 365.9436775200001 653.8223216 380.9019236640001 663.7849216\n",
      "Deep 387.8424685800001 653.8223216 409.2839763000001 663.7849216\n",
      "recurrent 412.8203007960001 653.8223216 449.49442466400006 663.7849216\n",
      "models 453.040911012 653.8223216 482.3985014399999 663.7849216\n",
      "with 485.93482593599987 653.8223216 504.0025987919998 663.7849216\n",
      "fast-forwardconnectionsforneuralmachinetranslation. 129.579 664.7313216 353.47847240000016 674.6939216\n",
      "CoRR,abs/1606.04199,2016. 357.063 664.5021818 476.6145486 674.6939216\n",
      "[40] 108.0 688.2143216 124.5976916 698.1769216\n",
      "Muhua 129.579 688.2143216 158.36752671600001 698.1769216\n",
      "Zhu, 161.77174713600002 688.2143216 180.682953708 698.1769216\n",
      "Yue 184.310734872 688.2143216 200.13273843600004 698.1769216\n",
      "Zhang, 203.536958856 688.2143216 232.04095371600005 698.1769216\n",
      "Wenliang 235.66873488000004 688.2143216 274.385391 698.1769216\n",
      "Chen, 277.78961142 688.2143216 301.78174399200003 698.1769216\n",
      "Min 305.4095251560001 688.2143216 322.34933244000007 698.1769216\n",
      "Zhang, 325.7535528600001 688.2143216 354.2575477200001 698.1769216\n",
      "and 357.88532888400016 688.2143216 372.5590431720001 698.1769216\n",
      "Jingbo 375.9632635920001 688.2143216 403.0649228760001 698.1769216\n",
      "Zhu. 406.4691432960001 688.2143216 425.380349868 698.1769216\n",
      "Fast 431.883935148 688.2143216 448.671314652 698.1769216\n",
      "and 452.075535072 688.2143216 466.7492493599999 698.1769216\n",
      "accurate 470.1534697799999 688.2143216 504.00259879199984 698.1769216\n",
      "shift-reduceconstituentparsing. 129.579 699.1233216 255.03802180000005 709.0859216\n",
      "InProceedingsofthe51stAnnualMeetingoftheACL(Volume 258.4259035560001 698.8941818 503.9968397520001 709.0859216\n",
      "1: 128.832 709.8031818 137.1308458 719.7657818\n",
      "LongPapers),pages434–443.ACL,August2013. 140.2192518 709.8031818 342.0416376 719.9949216\n",
      "12 301.019 742.0773216 310.98159999999996 752.0399216\n",
      "AttentionVisualizations 108.0 72.50643679999996 230.7679488 84.46163679999995\n",
      "Input-Input Layer5 108.0 67.05727097086412 250.495413625422 83.39286808486406\n",
      "It 122.19648613185 156.160323775832 129.97529750984998 160.48534290199996\n",
      "i 133.86465140984998 158.75792888608396 141.64346278784998 160.484825012\n",
      "s 133.86465140984998 154.86852319708396 141.64346278784998 158.75792888608396\n",
      "i 145.53292026584998 158.757410996084 153.31173164384998 160.48430712200002\n",
      "n 145.53292026584998 154.43239186991605 153.31173164384998 158.757410996084\n",
      "t 157.20108554384998 158.32179755891605 164.97989692184998 160.48430712200002\n",
      "hi 157.20108554384998 152.26988230683196 164.97989692184998 158.32179755891605\n",
      "s 157.20108554384998 148.38047661783207 164.97989692184998 152.26988230683207\n",
      "s 168.86976871184999 156.595419323 176.64858008985 160.484825012\n",
      "pirit 168.86976871184999 144.06375419304197 176.64858008985 156.595419323\n",
      "t 180.53803756784998 158.32179755891605 188.31684894584998 160.48430712200002\n",
      "h 180.53803756784998 153.9967784327481 188.31684894584998 158.32179755891605\n",
      "at 180.53803756784998 147.50924974349607 188.31684894584998 153.996778432748\n",
      "a 192.20620284584996 156.15980588583204 199.98501422384996 160.484825012\n",
      "m 203.87436812384996 154.00455724412598 211.65317950184996 160.48430712200002\n",
      "aj 203.87436812384996 147.952641992042 211.65317950184996 154.00455724412598\n",
      "orit 203.87436812384996 137.14787298800002 211.65317950184996 147.952641992042\n",
      "y 203.87436812384996 133.25846729900002 211.65317950184996 137.14787298800002\n",
      "of 215.54263697985 153.99729632274807 223.32144835785 160.484825012\n",
      "A 227.21132014785 155.29635782287403 234.99013152585 160.484825012\n",
      "m 227.21132014785 148.816607945 234.99013152585 155.29635782287403\n",
      "eri 227.21132014785 140.17434850404197 234.99013152585 148.816607945\n",
      "c 227.21132014785 136.28494281504197 234.99013152585 140.17434850404197\n",
      "a 227.21132014785 131.95992368887403 234.99013152585 136.28494281504197\n",
      "n 227.21132014785 127.63490456270608 234.99013152585 131.95992368887403\n",
      "g 238.87948542584996 156.15928799583207 246.65829680384996 160.48430712200002\n",
      "o 238.87948542584996 151.83426886966413 246.65829680384996 156.15928799583207\n",
      "v 238.87948542584996 147.94486318066402 246.65829680384996 151.83426886966402\n",
      "er 238.87948542584996 141.02949986562203 246.65829680384996 147.94486318066402\n",
      "n 238.87948542584996 136.70448073945408 246.65829680384996 141.02949986562203\n",
      "m 238.87948542584996 130.22473086157993 246.65829680384996 136.70448073945397\n",
      "e 238.87948542584996 125.8997117354121 246.65829680384996 130.22473086158004\n",
      "nt 238.87948542584996 119.41218304616007 246.65829680384996 125.89971173541198\n",
      "s 238.87948542584996 115.52277735715995 246.65829680384996 119.41218304615995\n",
      "h 250.54775428185 156.15980588583204 258.32656565985 160.484825012\n",
      "a 250.54775428185 151.8347867596641 258.32656565985 156.15980588583204\n",
      "v 250.54775428185 147.94538107066398 258.32656565985 151.83478675966398\n",
      "e 250.54775428185 143.62036194449604 258.32656565985 147.94538107066398\n",
      "p 262.21591955985 156.15928799583207 269.99473093785 160.48430712200002\n",
      "a 262.21591955985 151.83426886966413 269.99473093785 156.15928799583207\n",
      "s 262.21591955985 147.94486318066402 269.99473093785 151.83426886966402\n",
      "s 262.21591955985 144.05545749166401 269.99473093785 147.94486318066402\n",
      "e 262.21591955985 139.73043836549607 269.99473093785 144.05545749166401\n",
      "d 262.21591955985 135.40541923932813 269.99473093785 139.73043836549607\n",
      "n 273.88408483784997 156.15980588583204 281.66289621584997 160.484825012\n",
      "e 273.88408483784997 151.8347867596641 281.66289621584997 156.15980588583204\n",
      "w 273.88408483784997 146.21848494474796 281.66289621584997 151.83478675966398\n",
      "l 285.55235369385 158.75792888608396 293.33116507185 160.484825012\n",
      "a 285.55235369385 154.43290975991601 293.33116507185 158.75792888608396\n",
      "w 285.55235369385 148.816607945 293.33116507185 154.43290975991601\n",
      "s 285.55235369385 144.927202256 293.33116507185 148.816607945\n",
      "si 297.22103686185 154.868005307084 304.99984823985 160.48430712200002\n",
      "n 297.22103686185 150.54298618091605 304.99984823985 154.868005307084\n",
      "c 297.22103686185 146.65358049191605 304.99984823985 150.54298618091605\n",
      "e 297.22103686185 142.3285613657481 304.99984823985 146.65358049191605\n",
      "2 308.88920213985 156.15980588583204 316.66801351785 160.484825012\n",
      "0 308.88920213985 151.8347867596641 316.66801351785 156.15980588583204\n",
      "0 308.88920213985 147.50976763349604 316.66801351785 151.83478675966398\n",
      "9 308.88920213985 143.1847485073281 316.66801351785 147.50976763349604\n",
      "m 320.55747099585 154.00455724412598 328.33628237385 160.48430712200002\n",
      "a 320.55747099585 149.67953811795803 328.33628237385 154.00455724412598\n",
      "ki 320.55747099585 144.063236303042 328.33628237385 149.67953811795803\n",
      "n 320.55747099585 139.73821717687406 328.33628237385 144.063236303042\n",
      "g 320.55747099585 135.41319805070611 328.33628237385 139.73821717687406\n",
      "t 332.22563627385 158.32231544891602 340.00444765185 160.484825012\n",
      "h 332.22563627385 153.99729632274807 340.00444765185 158.32231544891602\n",
      "e 332.22563627385 149.67227719658 340.00444765185 153.99729632274796\n",
      "r 343.89390512985 157.89499871312591 351.67271650785 160.48534290199996\n",
      "e 343.89390512985 153.56997958695797 351.67271650785 157.89499871312591\n",
      "gi 343.89390512985 147.51806433487388 351.67271650785 153.56997958695797\n",
      "str 343.89390512985 138.87580489391587 351.67271650785 147.518064334874\n",
      "ati 343.89390512985 130.66138007874792 351.67271650785 138.87580489391598\n",
      "o 343.89390512985 126.33636095257998 351.67271650785 130.66138007874792\n",
      "n 343.89390512985 122.01134182641204 351.67271650785 126.33636095257998\n",
      "or 355.56207040785 153.56894380695803 363.34088178585 160.48430712200002\n",
      "v 367.23075357585003 156.59448712100004 375.00956495385003 160.48389281000004\n",
      "oti 367.23075357585003 148.38006230583198 375.00956495385003 156.59448712100004\n",
      "n 367.23075357585003 144.05504317966415 375.00956495385003 148.3800623058321\n",
      "g 367.23075357585003 139.7300240534961 375.00956495385003 144.05504317966404\n",
      "pr 378.89902243185 153.56894380695803 386.67783380985003 160.48430712200002\n",
      "o 378.89902243185 149.2439246807901 386.67783380985003 153.56894380695803\n",
      "c 378.89902243185 145.35451899178997 386.67783380985003 149.24392468078997\n",
      "e 378.89902243185 141.02949986562203 386.67783380985003 145.35451899178997\n",
      "s 378.89902243185 137.14009417662203 386.67783380985003 141.02949986562203\n",
      "s 378.89902243185 133.25068848762203 386.67783380985003 137.14009417662203\n",
      "m 390.56718770985 154.00507513412595 398.34599908785 160.484825012\n",
      "or 390.56718770985 147.08971181908396 398.34599908785 154.00507513412595\n",
      "e 390.56718770985 142.76469269291601 398.34599908785 147.08971181908396\n",
      "diffi 402.23483509785 148.52153100263604 410.01364647585 160.48534290199996\n",
      "c 402.23483509785 144.63212531363604 410.01364647585 148.52153100263604\n",
      "ult 402.23483509785 136.4177004984681 410.01364647585 144.63212531363604\n",
      ". 413.90403615585 158.32231544891602 421.68284753385 160.484825012\n",
      "< 425.57230501185 155.94106696524807 433.35111638985 160.48389281000004\n",
      "E 425.57230501185 150.7525997761221 433.35111638985 155.94106696524807\n",
      "O 425.57230501185 144.70068452403802 433.35111638985 150.752599776122\n",
      "S 425.57230501185 139.51221733491207 433.35111638985 144.70068452403802\n",
      "> 425.57230501185 134.9693914901601 433.35111638985 139.51221733491207\n",
      "< 437.24098817985 155.94148127724804 445.01979955785 160.48430712200002\n",
      "p 437.24098817985 151.6164621510801 445.01979955785 155.94148127724804\n",
      "a 437.24098817985 147.29144302491204 445.01979955785 151.61646215108\n",
      "d 437.24098817985 142.9664238987441 445.01979955785 147.29144302491204\n",
      "> 437.24098817985 138.423598053992 445.01979955785 142.96642389874398\n",
      "< 448.90873914585 155.941999167248 456.68755052385 160.484825012\n",
      "p 448.90873914585 151.61698004108007 456.68755052385 155.941999167248\n",
      "a 448.90873914585 147.291960914912 456.68755052385 151.61698004107996\n",
      "d 448.90873914585 142.96694178874407 456.68755052385 147.291960914912\n",
      "> 448.90873914585 138.42411594399198 456.68755052385 142.96694178874395\n",
      "< 460.57638653385 155.94251705724798 468.35519791185 160.48534290199996\n",
      "p 460.57638653385 151.61749793108004 468.35519791185 155.94251705724798\n",
      "a 460.57638653385 147.29247880491198 468.35519791185 151.61749793107992\n",
      "d 460.57638653385 142.96745967874404 468.35519791185 147.29247880491198\n",
      "> 460.57638653385 138.42463383399195 468.35519791185 142.96745967874392\n",
      "< 472.24506970184996 155.941999167248 480.02388107984996 160.484825012\n",
      "p 472.24506970184996 151.61698004108007 480.02388107984996 155.941999167248\n",
      "a 472.24506970184996 147.291960914912 480.02388107984996 151.61698004107996\n",
      "d 472.24506970184996 142.96694178874407 480.02388107984996 147.291960914912\n",
      "> 472.24506970184996 138.42411594399198 480.02388107984996 142.96694178874395\n",
      "< 483.91385644785 155.94106696524807 491.69266782585 160.48389281000004\n",
      "p 483.91385644785 151.61604783908012 491.69266782585 155.94106696524807\n",
      "a 483.91385644785 147.29102871291207 491.69266782585 151.61604783908\n",
      "d 483.91385644785 142.96600958674412 491.69266782585 147.29102871291207\n",
      "> 483.91385644785 138.42318374199203 491.69266782585 142.966009586744\n",
      "< 495.58253961584995 155.94148127724804 503.36135099384995 160.48430712200002\n",
      "p 495.58253961584995 151.6164621510801 503.36135099384995 155.94148127724804\n",
      "a 495.58253961584995 147.29144302491204 503.36135099384995 151.61646215108\n",
      "d 495.58253961584995 142.9664238987441 503.36135099384995 147.29144302491204\n",
      "> 495.58253961584995 138.423598053992 503.36135099384995 142.96642389874398\n",
      "It 122.19648613185 241.3820234598321 129.97529750984998 245.70704258600006\n",
      "i 133.86465140984998 245.27538935008397 141.64346278784998 247.002285476\n",
      "s 133.86465140984998 241.38598366108397 141.64346278784998 245.27538935008397\n",
      "i 145.53292026584998 245.71290282208395 153.31173164384998 247.43979894799998\n",
      "n 145.53292026584998 241.387883695916 153.31173164384998 245.71290282208395\n",
      "t 157.20160343384998 251.32707320891598 164.98041481184998 253.48958277199995\n",
      "hi 157.20160343384998 245.2751579568319 164.98041481184998 251.32707320891598\n",
      "s 157.20160343384998 241.385752267832 164.98041481184998 245.275157956832\n",
      "s 168.86976871184999 253.919276105 176.64858008985 257.808681794\n",
      "pirit 168.86976871184999 241.38761097504198 176.64858008985 253.919276105\n",
      "t 180.53803756784998 252.197024830916 188.31684894584998 254.35953439399998\n",
      "h 180.53803756784998 247.87200570474806 188.31684894584998 252.197024830916\n",
      "at 180.53803756784998 241.38447701549603 188.31684894584998 247.87200570474795\n",
      "a 192.20620284584996 241.3861665798321 199.98501422384996 245.71118570600004\n",
      "m 203.87436812384996 262.13532823412595 211.65317950184996 268.615078112\n",
      "aj 203.87436812384996 256.083412982042 211.65317950184996 262.13532823412595\n",
      "orit 203.87436812384996 245.278643978 211.65317950184996 256.083412982042\n",
      "y 203.87436812384996 241.389238289 211.65317950184996 245.278643978\n",
      "of 215.54263697985 241.3847084087481 223.32144835785 247.87223709800003\n",
      "A 227.21132014785 269.050585588874 234.99013152585 274.239052778\n",
      "m 227.21132014785 262.570835711 234.99013152585 269.050585588874\n",
      "eri 227.21132014785 253.92857627004196 234.99013152585 262.570835711\n",
      "c 227.21132014785 250.03917058104196 234.99013152585 253.92857627004196\n",
      "a 227.21132014785 245.714151454874 234.99013152585 250.03917058104196\n",
      "n 227.21132014785 241.38913232870607 234.99013152585 245.714151454874\n",
      "g 238.87948542584996 282.027377173832 246.65829680384996 286.3523963\n",
      "o 238.87948542584996 277.702358047664 246.65829680384996 282.027377173832\n",
      "v 238.87948542584996 273.812952358664 246.65829680384996 277.702358047664\n",
      "er 238.87948542584996 266.897589043622 246.65829680384996 273.812952358664\n",
      "n 238.87948542584996 262.5725699174541 246.65829680384996 266.897589043622\n",
      "m 238.87948542584996 256.0928200395799 246.65829680384996 262.57256991745396\n",
      "e 238.87948542584996 251.7678009134121 246.65829680384996 256.09282003958003\n",
      "nt 238.87948542584996 245.28027222416006 246.65829680384996 251.76780091341197\n",
      "s 238.87948542584996 241.39086653515994 246.65829680384996 245.28027222415994\n",
      "h 250.54775428185 253.92821944383206 258.32656565985 258.25323857\n",
      "a 250.54775428185 249.60320031766412 258.32656565985 253.92821944383206\n",
      "v 250.54775428185 245.713794628664 258.32656565985 249.603200317664\n",
      "e 250.54775428185 241.38877550249606 258.32656565985 245.713794628664\n",
      "p 262.21591955985 262.14423355983206 269.99473093785 266.469252686\n",
      "a 262.21591955985 257.8192144336641 269.99473093785 262.14423355983206\n",
      "s 262.21591955985 253.929808744664 269.99473093785 257.819214433664\n",
      "s 262.21591955985 250.040403055664 269.99473093785 253.929808744664\n",
      "e 262.21591955985 245.71538392949606 269.99473093785 250.040403055664\n",
      "d 262.21591955985 241.3903648033281 269.99473093785 245.71538392949606\n",
      "n 273.88460272785005 251.3297581578321 281.66341410585005 255.65477728400003\n",
      "e 273.88460272785005 247.00473903166414 281.66341410585005 251.3297581578321\n",
      "w 273.88460272785005 241.388437216748 281.66341410585005 247.00473903166403\n",
      "l 285.55287158385 255.21898092808397 293.33168296185 256.945877054\n",
      "a 285.55287158385 250.89396180191602 293.33168296185 255.21898092808397\n",
      "w 285.55287158385 245.277659987 293.33168296185 250.89396180191602\n",
      "s 285.55287158385 241.388254298 293.33168296185 245.277659987\n",
      "si 297.22103686185 253.92762221308396 304.99984823985 259.543924028\n",
      "n 297.22103686185 249.60260308691602 304.99984823985 253.92762221308396\n",
      "c 297.22103686185 245.71319739791602 304.99984823985 249.60260308691602\n",
      "e 297.22103686185 241.38817827174807 304.99984823985 245.71319739791602\n",
      "2 308.88920213985 254.3646971358321 316.66801351785 258.68971626200005\n",
      "0 308.88920213985 250.03967800966416 316.66801351785 254.3646971358321\n",
      "0 308.88920213985 245.7146588834961 316.66801351785 250.03967800966404\n",
      "9 308.88920213985 241.38963975732815 316.66801351785 245.7146588834961\n",
      "m 320.55747099585 259.98183803612596 328.33628237385 266.461587914\n",
      "a 320.55747099585 255.65681890995802 328.33628237385 259.98183803612596\n",
      "ki 320.55747099585 250.040517095042 328.33628237385 255.65681890995802\n",
      "n 320.55747099585 245.71549796887405 328.33628237385 250.040517095042\n",
      "g 320.55747099585 241.3904788427061 328.33628237385 245.71549796887405\n",
      "t 332.22563627385 250.03586986091602 340.00444765185 252.198379424\n",
      "h 332.22563627385 245.71085073474808 340.00444765185 250.03586986091602\n",
      "e 332.22563627385 241.38583160858002 340.00444765185 245.71085073474796\n",
      "r 343.89390512985 277.27423692512593 351.67271650785 279.864581114\n",
      "e 343.89390512985 272.949217798958 351.67271650785 277.27423692512593\n",
      "gi 343.89390512985 266.8973025468739 351.67271650785 272.949217798958\n",
      "str 343.89390512985 258.2550431059159 351.67271650785 266.897302546874\n",
      "ati 343.89390512985 250.04061829074794 351.67271650785 258.255043105916\n",
      "o 343.89390512985 245.71559916458 351.67271650785 250.04061829074794\n",
      "n 343.89390512985 241.39058003841205 351.67271650785 245.71559916458\n",
      "or 355.56258829784997 241.38568670295808 363.34139967584997 248.30105001800007\n",
      "v 367.23075357585003 258.25308320299996 375.00956495385003 262.14248889199996\n",
      "oti 367.23075357585003 250.0386583878319 375.00956495385003 258.25308320299996\n",
      "n 367.23075357585003 245.71363926166407 375.00956495385003 250.03865838783202\n",
      "g 367.23075357585003 241.388620135496 375.00956495385003 245.71363926166396\n",
      "pr 378.89902243185 261.70634378895807 386.67783380985003 268.62170710400005\n",
      "o 378.89902243185 257.3813246627901 386.67783380985003 261.70634378895807\n",
      "c 378.89902243185 253.49191897379 386.67783380985003 257.38132466279\n",
      "e 378.89902243185 249.16689984762206 386.67783380985003 253.49191897379\n",
      "s 378.89902243185 245.27749415862206 386.67783380985003 249.16689984762206\n",
      "s 378.89902243185 241.38808846962206 386.67783380985003 245.27749415862206\n",
      "m 390.56718770985 252.6276964581259 398.34599908785 259.10744633599995\n",
      "or 390.56718770985 245.71233314308392 398.34599908785 252.6276964581259\n",
      "e 390.56718770985 241.38731401691598 398.34599908785 245.71233314308392\n",
      "diffi 402.23483509785 253.49037638663606 410.01364647585 265.454188286\n",
      "c 402.23483509785 249.60097069763606 410.01364647585 253.49037638663606\n",
      "ult 402.23483509785 241.3865458824681 410.01364647585 249.60097069763606\n",
      ". 413.90362184385 241.38348163091598 421.68243322185 243.54599119399995\n",
      "< 425.57230501185 262.3552397612481 433.35111638985 266.89806560600005\n",
      "E 425.57230501185 257.1667725721221 433.35111638985 262.3552397612481\n",
      "O 425.57230501185 251.11485732003803 433.35111638985 257.166772572122\n",
      "S 425.57230501185 245.92639013091207 433.35111638985 251.11485732003803\n",
      "> 425.57230501185 241.3835642861601 433.35111638985 245.92639013091207\n",
      "< 437.23995239985 258.90609236124806 445.01876377785 263.44891820600003\n",
      "p 437.23995239985 254.5810732350801 445.01876377785 258.90609236124806\n",
      "a 437.23995239985 250.25605410891205 445.01876377785 254.58107323508\n",
      "d 437.23995239985 245.9310349827441 445.01876377785 250.25605410891205\n",
      "> 437.23995239985 241.38820913799202 445.01876377785 245.931034982744\n",
      "< 448.90873914585 258.90609236124806 456.68755052385 263.44891820600003\n",
      "p 448.90873914585 254.5810732350801 456.68755052385 258.90609236124806\n",
      "a 448.90873914585 250.25605410891205 456.68755052385 254.58107323508\n",
      "d 448.90873914585 245.9310349827441 456.68755052385 250.25605410891205\n",
      "> 448.90873914585 241.38820913799202 456.68755052385 245.931034982744\n",
      "< 460.57638653385 258.9055744712481 468.35519791185 263.44840031600006\n",
      "p 460.57638653385 254.58055534508014 468.35519791185 258.9055744712481\n",
      "a 460.57638653385 250.25553621891208 468.35519791185 254.58055534508003\n",
      "d 460.57638653385 245.93051709274414 468.35519791185 250.25553621891208\n",
      "> 460.57638653385 241.38769124799205 468.35519791185 245.93051709274403\n",
      "< 472.24506970184996 258.90609236124806 480.02388107984996 263.44891820600003\n",
      "p 472.24506970184996 254.5810732350801 480.02388107984996 258.90609236124806\n",
      "a 472.24506970184996 250.25605410891205 480.02388107984996 254.58107323508\n",
      "d 472.24506970184996 245.9310349827441 480.02388107984996 250.25605410891205\n",
      "> 472.24506970184996 241.38820913799202 480.02388107984996 245.931034982744\n",
      "< 483.91385644785 258.90650667324803 491.69266782585 263.449332518\n",
      "p 483.91385644785 254.5814875470801 491.69266782585 258.90650667324803\n",
      "a 483.91385644785 250.25646842091203 491.69266782585 254.58148754707997\n",
      "d 483.91385644785 245.93144929474408 491.69266782585 250.25646842091203\n",
      "> 483.91385644785 241.388623449992 491.69266782585 245.93144929474397\n",
      "< 495.58253961584995 258.90609236124806 503.36135099384995 263.44891820600003\n",
      "p 495.58253961584995 254.5810732350801 503.36135099384995 258.90609236124806\n",
      "a 495.58253961584995 250.25605410891205 503.36135099384995 254.58107323508\n",
      "d 495.58253961584995 245.9310349827441 503.36135099384995 250.25605410891205\n",
      "> 495.58253961584995 241.38820913799202 503.36135099384995 245.931034982744\n",
      "Figure 108.0 312.1313216 134.53259557200002 322.0939216\n",
      "3: 137.98762525200002 312.1313216 145.89354610799998 322.0939216\n",
      "An 150.913500996 312.1313216 163.33128414 322.0939216\n",
      "example 166.78631382 312.1313216 201.07240246800004 322.0939216\n",
      "of 204.527432148 312.1313216 212.99225486400005 322.0939216\n",
      "the 216.447284544 312.1313216 228.86506768800004 322.0939216\n",
      "attention 232.32009736800003 312.1313216 267.886579368 322.0939216\n",
      "mechanism 271.341609048 312.1313216 317.6288449080001 322.0939216\n",
      "following 321.08387458800007 312.1313216 360.3594325680001 322.0939216\n",
      "long-distance 363.8144622480001 312.1313216 418.56652082399995 322.0939216\n",
      "dependencies 422.021550504 312.1313216 476.76344722799985 322.0939216\n",
      "in 480.21847690799984 312.1313216 488.1243977639998 322.0939216\n",
      "the 491.57942744399986 312.1313216 503.99721058799975 322.0939216\n",
      "encoderself-attentioninlayer5of6. 108.0 323.04132159999995 255.69146033400006 333.00392159999996\n",
      "Manyoftheattentionheadsattendtoadistantdependencyof 258.7760606208 323.04132159999995 503.99679215879985 333.00392159999996\n",
      "theverb‘making’,completingthephrase‘making...moredifficult’. 108.0 333.95032160000005 376.743177333 343.9129216\n",
      "Attentionshereshownonlyfor 379.857037776 333.95032160000005 504.1711575839997 343.9129216\n",
      "theword‘making’. 108.0 344.85932160000004 184.5426558 354.8219216\n",
      "Differentcolorsrepresentdifferentheads. 187.6310618 344.85932160000004 353.94670620000016 354.8219216\n",
      "Bestviewedincolor. 357.03511220000007 344.85932160000004 440.91024159999995 354.8219216\n",
      "13 301.019 742.0773216 310.98159999999996 752.0399216000001\n",
      "Input-Input Layer5 108.0 103.30519485798402 278.684131549932 122.87233134198391\n",
      "T 125.0048624361 209.52231595565195 134.3224933041 215.215388416\n",
      "h 125.0048624361 204.34171319304392 134.3224933041 209.52231595565195\n",
      "e 125.0048624361 199.161110430436 134.3224933041 204.34171319304403\n",
      "L 138.9812467041 210.034165313392 148.2988775721 215.21476807600004\n",
      "a 138.9812467041 204.85356255078398 148.2988775721 210.034165313392\n",
      "w 138.9812467041 198.12623306408807 148.2988775721 204.8535625507841\n",
      "will 152.95775504009998 202.2812760912161 162.2753859081 215.2141477360001\n",
      "n 166.9341393081 210.03354497339205 176.2517701761 215.2141477360001\n",
      "e 166.9341393081 204.85294221078402 176.2517701761 210.03354497339205\n",
      "v 166.9341393081 200.19412677678417 176.2517701761 204.85294221078414\n",
      "e 166.9341393081 195.01352401417603 176.2517701761 200.19412677678406\n",
      "r 166.9341393081 191.910752935132 176.2517701761 195.01352401417603\n",
      "b 180.91114391609997 210.034165313392 190.22877478409998 215.21476807600004\n",
      "e 180.91114391609997 204.85356255078398 190.22877478409998 210.034165313392\n",
      "p 194.8876522521 210.03354497339205 204.2052831201 215.2141477360001\n",
      "e 194.8876522521 204.85294221078402 204.2052831201 210.03354497339205\n",
      "rf 194.8876522521 199.1598697504361 204.2052831201 204.85294221078414\n",
      "e 194.8876522521 193.97926698782805 204.2052831201 199.1598697504361\n",
      "ct 194.8876522521 186.73015017252408 204.2052831201 193.97926698782805\n",
      ", 208.86403652009997 212.62446669469603 218.18166738809998 215.21476807600004\n",
      "b 222.84042078809998 210.03354497339205 232.15805165609999 215.2141477360001\n",
      "ut 222.84042078809998 202.26264082948012 232.15805165609999 210.03354497339205\n",
      "it 236.8169291241 210.55595264200008 246.1345599921 215.21476807600004\n",
      "s 236.8169291241 205.89713720800012 246.1345599921 210.55595264200008\n",
      "a 250.7939337321 210.034165313392 260.1115646001 215.21476807600004\n",
      "p 250.7939337321 204.85356255078398 260.1115646001 210.034165313392\n",
      "pli 250.7939337321 195.53593168278405 260.1115646001 204.8535625507841\n",
      "c 250.7939337321 190.8771162487841 260.1115646001 195.53593168278405\n",
      "ati 250.7939337321 181.0376980521761 260.1115646001 190.8771162487841\n",
      "o 250.7939337321 175.85709528956795 260.1115646001 181.03769805217598\n",
      "n 250.7939337321 170.67649252696003 260.1115646001 175.85709528956806\n",
      "s 264.7703180001 210.55533230200012 274.0879488681 215.2141477360001\n",
      "h 264.7703180001 205.3747295393921 274.0879488681 210.55533230200012\n",
      "o 264.7703180001 200.19412677678406 274.0879488681 205.3747295393921\n",
      "ul 264.7703180001 192.94500996148008 274.0879488681 200.19412677678406\n",
      "d 264.7703180001 187.76440719887205 274.0879488681 192.94500996148008\n",
      "b 278.74682633610007 210.034165313392 288.06445720410005 215.21476807600004\n",
      "e 278.74682633610007 204.85356255078398 288.06445720410005 210.034165313392\n",
      "j 292.7232106041 213.14563368330414 302.0408414721 215.2141477360001\n",
      "u 292.7232106041 207.9650309206961 302.0408414721 213.14563368330414\n",
      "st 292.7232106041 200.71591410539202 302.0408414721 207.9650309206961\n",
      "- 306.69959487209996 212.111996996956 316.01722574009995 215.21476807600004\n",
      "t 320.6761032081 212.62446669469603 329.9937340761 215.21476807600004\n",
      "hi 320.6761032081 205.37534987939205 329.9937340761 212.62446669469603\n",
      "s 320.6761032081 200.71653444539209 329.9937340761 205.37534987939205\n",
      "i 334.65310781610003 213.14563368330414 343.9707386841 215.2141477360001\n",
      "s 334.65310781610003 208.48681824930418 343.9707386841 213.14563368330414\n",
      "w 348.62949208410004 208.48743858930402 357.9471229521 215.21476807600004\n",
      "h 348.62949208410004 203.306835826696 357.9471229521 208.48743858930402\n",
      "at 348.62949208410004 195.53593168278405 357.9471229521 203.306835826696\n",
      "w 362.60600042010003 208.48681824930406 371.9236312881 215.2141477360001\n",
      "e 362.60600042010003 203.30621548669603 371.9236312881 208.48681824930406\n",
      "a 376.5823846881 210.034165313392 385.90001555609996 215.21476807600004\n",
      "r 376.5823846881 206.93139423434798 385.90001555609996 210.034165313392\n",
      "e 376.5823846881 201.75079147174006 385.90001555609996 206.9313942343481\n",
      "mi 390.55889302410003 205.38528785026006 399.8765238921 215.215388416\n",
      "s 390.55889302410003 200.72647241625998 399.8765238921 205.38528785025994\n",
      "si 390.55889302410003 193.99914292956407 399.8765238921 200.72647241625998\n",
      "n 390.55889302410003 188.81854016695593 399.8765238921 193.99914292956396\n",
      "g 390.55889302410003 183.637937404348 399.8765238921 188.81854016695604\n",
      ", 404.53527729210003 212.62384635469607 413.8529081601 215.2141477360001\n",
      "i 418.51228190010005 213.14501334330407 427.82991276810003 215.21352739600002\n",
      "n 418.51228190010005 207.96441058069604 427.82991276810003 213.14501334330407\n",
      "m 432.48879023610004 207.4525612229561 441.8064211041 215.2141477360001\n",
      "y 432.48879023610004 202.79374578895613 441.8064211041 207.4525612229561\n",
      "o 446.4651745041 210.034165313392 455.7828053721 215.21476807600004\n",
      "pi 446.4651745041 202.78504849808814 455.7828053721 210.034165313392\n",
      "ni 446.4651745041 195.53593168278405 455.7828053721 202.78504849808803\n",
      "o 446.4651745041 190.35532892017602 455.7828053721 195.53593168278405\n",
      "n 446.4651745041 185.174726157568 455.7828053721 190.35532892017602\n",
      ". 460.44093843210004 212.62508703469598 469.7585693001 215.215388416\n",
      "< 474.4185633801 209.7732716490881 483.7361942481 215.21476807600004\n",
      "E 474.4185633801 203.55841186013208 483.7361942481 209.7732716490881\n",
      "O 474.4185633801 196.3092950448281 483.7361942481 203.55841186013208\n",
      "S 474.4185633801 190.09443525587199 483.7361942481 196.309295044828\n",
      "> 474.4185633801 184.65293882896003 483.7361942481 190.09443525587199\n",
      "< 488.3950717161 209.77203096908806 497.7127025841 215.21352739600002\n",
      "p 488.3950717161 204.59142820648003 497.7127025841 209.77203096908806\n",
      "a 488.3950717161 199.410825443872 497.7127025841 204.59142820648003\n",
      "d 488.3950717161 194.23022268126397 497.7127025841 199.410825443872\n",
      "> 488.3950717161 188.788726254352 497.7127025841 194.23022268126397\n",
      "T 125.0048624361 322.480522827652 134.3224933041 328.173595288\n",
      "h 125.0048624361 317.29992006504403 134.3224933041 322.480522827652\n",
      "e 125.0048624361 312.119317302436 134.3224933041 317.299920065044\n",
      "L 138.9812467041 324.03069727739205 148.2988775721 329.21130004\n",
      "a 138.9812467041 318.8500945147841 148.2988775721 324.03069727739205\n",
      "w 138.9812467041 312.122765028088 148.2988775721 318.850094514784\n",
      "will 152.95775504009998 312.125748367216 162.2753859081 325.058620012\n",
      "n 166.93475964809997 330.24563560139205 176.25239051609998 335.426238364\n",
      "e 166.93475964809997 325.0650328387841 176.25239051609998 330.24563560139205\n",
      "v 166.93475964809997 320.406217404784 176.25239051609998 325.065032838784\n",
      "e 166.93475964809997 315.225614642176 176.25239051609998 320.406217404784\n",
      "r 166.93475964809997 312.12284356313205 176.25239051609998 315.225614642176\n",
      "b 180.91114391609997 317.302117433392 190.22877478409998 322.482720196\n",
      "e 180.91114391609997 312.12151467078405 190.22877478409998 317.302117433392\n",
      "p 194.8876522521 335.42286917339203 204.2052831201 340.603471936\n",
      "e 194.8876522521 330.24226641078405 204.2052831201 335.42286917339203\n",
      "rf 194.8876522521 324.549193950436 204.2052831201 330.242266410784\n",
      "e 194.8876522521 319.36859118782803 204.2052831201 324.549193950436\n",
      "ct 194.8876522521 312.119474372524 204.2052831201 319.36859118782803\n",
      ", 208.86403652009997 312.116953186696 218.18166738809998 314.707254568\n",
      "b 222.84042078809998 319.89129252539203 232.15805165609999 325.071895288\n",
      "ut 222.84042078809998 312.12038838148 232.15805165609999 319.89129252539203\n",
      "it 236.8169291241 316.777018978 246.1345599921 321.435834412\n",
      "s 236.8169291241 312.118203544 246.1345599921 316.777018978\n",
      "a 250.7939337321 351.48744194939206 260.1115646001 356.66804471200004\n",
      "p 250.7939337321 346.3068391867841 260.1115646001 351.48744194939206\n",
      "pli 250.7939337321 336.98920831878405 260.1115646001 346.30683918678403\n",
      "c 250.7939337321 332.33039288478403 260.1115646001 336.98920831878405\n",
      "ati 250.7939337321 322.49097468817604 260.1115646001 332.33039288478403\n",
      "o 250.7939337321 317.31037192556806 260.1115646001 322.49097468817604\n",
      "n 250.7939337321 312.12976916296003 260.1115646001 317.310371925568\n",
      "s 264.7703180001 334.916504986 274.0879488681 339.57532042\n",
      "h 264.7703180001 329.73590222339203 274.0879488681 334.916504986\n",
      "o 264.7703180001 324.55529946078406 274.0879488681 329.73590222339203\n",
      "ul 264.7703180001 317.30618264548 274.0879488681 324.555299460784\n",
      "d 264.7703180001 312.12557988287205 274.0879488681 317.30618264548\n",
      "b 278.74682633610007 317.302117433392 288.06445720410005 322.482720196\n",
      "e 278.74682633610007 312.12151467078405 288.06445720410005 317.302117433392\n",
      "j 292.7232106041 324.550633263304 302.0408414721 326.619147316\n",
      "u 292.7232106041 319.37003050069603 302.0408414721 324.550633263304\n",
      "st 292.7232106041 312.120913685392 302.0408414721 319.37003050069603\n",
      "- 306.70021521210003 312.11874534895605 316.0178460801 315.22151642800003\n",
      "t 320.67672354810003 324.026984914696 329.9943544161 326.61728629600003\n",
      "hi 320.67672354810003 316.77786809939204 329.9943544161 324.026984914696\n",
      "s 320.67672354810003 312.119052665392 329.9943544161 316.77786809939204\n",
      "i 334.65310781610003 316.778765607304 343.9707386841 318.84727966\n",
      "s 334.65310781610003 312.119950173304 343.9707386841 316.778765607304\n",
      "w 348.62949208410004 325.073145645304 357.9471229521 331.80047513200003\n",
      "h 348.62949208410004 319.89254288269603 357.9471229521 325.073145645304\n",
      "at 348.62949208410004 312.121638738784 357.9471229521 319.89254288269603\n",
      "w 362.60600042010003 317.303139009304 371.9236312881 324.030468496\n",
      "e 362.60600042010003 312.12253624669603 371.9236312881 317.303139009304\n",
      "a 376.5823846881 320.40505811339204 385.90001555609996 325.585660876\n",
      "r 376.5823846881 317.30228703434807 385.90001555609996 320.40505811339204\n",
      "e 376.5823846881 312.12168427174004 385.90001555609996 317.302287034348\n",
      "mi 390.55889302410003 333.87134933026005 399.8765238921 343.70144989600004\n",
      "s 390.55889302410003 329.21253389626 399.8765238921 333.87134933026005\n",
      "si 390.55889302410003 322.48520440956406 399.8765238921 329.21253389626\n",
      "n 390.55889302410003 317.3046016469561 399.8765238921 322.48520440956406\n",
      "g 390.55889302410003 312.12399888434805 399.8765238921 317.30460164695603\n",
      ", 404.5358976321 312.116953186696 413.85352850009997 314.707254568\n",
      "i 418.51228190010005 317.302208499304 427.82991276810003 319.370722552\n",
      "n 418.51228190010005 312.12160573669604 427.82991276810003 317.302208499304\n",
      "m 432.48879023610004 316.778429258956 441.8064211041 324.540015772\n",
      "y 432.48879023610004 312.119613824956 441.8064211041 316.778429258956\n",
      "o 446.4651745041 336.988235129392 455.7828053721 342.168837892\n",
      "pi 446.4651745041 329.739118314088 455.7828053721 336.988235129392\n",
      "ni 446.4651745041 322.490001498784 455.7828053721 329.739118314088\n",
      "o 446.4651745041 317.30939873617604 455.7828053721 322.490001498784\n",
      "n 446.4651745041 312.128795973568 455.7828053721 317.309398736176\n",
      ". 460.44093843210004 312.116332846696 469.7585693001 314.70663422800004\n",
      "< 474.4180671081 337.236764673088 483.73569797609997 342.6782611\n",
      "E 474.4180671081 331.02190488413197 483.73569797609997 337.236764673088\n",
      "O 474.4180671081 323.772788068828 483.73569797609997 331.02190488413197\n",
      "S 474.4180671081 317.557928279872 483.73569797609997 323.772788068828\n",
      "> 474.4180671081 312.11643185296 483.73569797609997 317.557928279872\n",
      "< 488.3950717161 333.106416885088 497.7127025841 338.547913312\n",
      "p 488.3950717161 327.92581412248 497.7127025841 333.106416885088\n",
      "a 488.3950717161 322.74521135987203 497.7127025841 327.92581412248\n",
      "d 488.3950717161 317.564608597264 497.7127025841 322.745211359872\n",
      "> 488.3950717161 312.123112170352 497.7127025841 317.564608597264\n",
      "Input-Input Layer5 108.0 319.299803264656 280.60189986553803 339.086791670656\n",
      "T 125.19592522615002 426.710355922518 134.61824688815003 432.467394458\n",
      "h 125.19592522615002 421.47154507844596 134.61824688815003 426.710355922518\n",
      "e 125.19592522615002 416.232734234374 134.61824688815003 421.471545078446\n",
      "L 139.32934498815 427.227956303928 148.75166665015 432.46676714800003\n",
      "a 139.32934498815 421.989145459856 148.75166665015 427.227956303928\n",
      "w 139.32934498815 415.186229219892 148.75166665015 421.98914545985605\n",
      "will 153.46289021215 419.387957371144 162.88521187415 432.466139838\n",
      "n 167.59630997415 427.227328993928 177.01863163615 432.466139838\n",
      "e 167.59630997415 421.98851814985596 177.01863163615 427.227328993928\n",
      "v 167.59630997415 417.27735731885605 177.01863163615 421.988518149856\n",
      "e 167.59630997415 412.03854647478397 177.01863163615 417.277357318856\n",
      "r 167.59630997415 408.900913361338 177.01863163615 412.03854647478397\n",
      "b 181.73035704615 427.227956303928 191.15267870815 432.46676714800003\n",
      "e 181.73035704615 421.989145459856 191.15267870815 427.227956303928\n",
      "p 195.86390227015002 427.227328993928 205.28622393215002 432.466139838\n",
      "e 195.86390227015002 421.98851814985596 205.28622393215002 427.227328993928\n",
      "rf 195.86390227015002 416.231479614374 205.28622393215002 421.988518149856\n",
      "e 195.86390227015002 410.992668770302 205.28622393215002 416.231479614374\n",
      "ct 195.86390227015002 403.662102517266 205.28622393215002 410.992668770302\n",
      ", 209.99732203215 429.84736172596405 219.41964369415 432.46676714800003\n",
      "b 224.13074179415 427.227328993928 233.55306345615 432.466139838\n",
      "ut 224.13074179415 419.36911272782004 233.55306345615 427.227328993928\n",
      "it 238.26428701815001 427.75560631700006 247.68660868015002 432.46676714800003\n",
      "s 238.26428701815001 423.044445486 247.68660868015002 427.755606317\n",
      "a 252.39833409014997 427.227956303928 261.82065575214995 432.46676714800003\n",
      "p 252.39833409014997 421.989145459856 261.82065575214995 427.227956303928\n",
      "pli 252.39833409014997 412.56682379785605 261.82065575214995 421.98914545985605\n",
      "c 252.39833409014997 407.855662966856 261.82065575214995 412.56682379785605\n",
      "ati 252.39833409014997 397.905691291784 261.82065575214995 407.855662966856\n",
      "o 252.39833409014997 392.666880447712 261.82065575214995 397.905691291784\n",
      "n 252.39833409014997 387.42806960364 261.82065575214995 392.666880447712\n",
      "s 266.53175385214996 427.754979007 275.95407551414996 432.466139838\n",
      "h 266.53175385214996 422.51616816292795 275.95407551414996 427.754979007\n",
      "o 266.53175385214996 417.277357318856 275.95407551414996 422.516168162928\n",
      "ul 266.53175385214996 409.94679106582 275.95407551414996 417.277357318856\n",
      "d 266.53175385214996 404.70798022174796 275.95407551414996 409.94679106582\n",
      "b 280.66529907615 427.227956303928 290.08762073815 432.46676714800003\n",
      "e 280.66529907615 421.989145459856 290.08762073815 427.227956303928\n",
      "j 294.79871883815 430.374384429036 304.22104050015 432.466139838\n",
      "u 294.79871883815 425.135573584964 304.22104050015 430.374384429036\n",
      "st 294.79871883815 417.80500733192804 304.22104050015 425.135573584964\n",
      "- 308.93213860015 429.32913403455404 318.35446026215 432.46676714800003\n",
      "t 323.06568382414997 429.84736172596405 332.48800548615 432.46676714800003\n",
      "hi 323.06568382414997 422.51679547292804 332.48800548615 429.84736172596405\n",
      "s 323.06568382414997 417.805634641928 332.48800548615 422.51679547292804\n",
      "i 337.19973089615 430.374384429036 346.62205255815 432.466139838\n",
      "s 337.19973089615 425.663223598036 346.62205255815 430.374384429036\n",
      "w 351.33315065815003 425.663850908036 360.75547232015003 432.46676714800003\n",
      "h 351.33315065815003 420.425040063964 360.75547232015003 425.663850908036\n",
      "at 351.33315065815003 412.56682379785605 360.75547232015003 420.42504006396405\n",
      "w 365.46669588214996 425.663223598036 374.88901754414997 432.466139838\n",
      "e 365.46669588214996 420.42441275396396 374.88901754414997 425.663223598036\n",
      "a 379.60011564415 427.227956303928 389.02243730615 432.46676714800003\n",
      "r 379.60011564415 424.090323190482 389.02243730615 427.227956303928\n",
      "e 379.60011564415 418.85151234641 389.02243730615 424.090323190482\n",
      "mi 393.73366086815 422.52684510459 403.15598253015 432.467394458\n",
      "s 393.73366086815 417.81568427359 403.15598253015 422.52684510459\n",
      "si 393.73366086815 411.012768033626 403.15598253015 417.81568427359\n",
      "n 393.73366086815 405.773957189554 403.15598253015 411.012768033626\n",
      "g 393.73366086815 400.53514634548196 403.15598253015 405.773957189554\n",
      ", 407.86708063015 429.846734415964 417.28940229215 432.466139838\n",
      "i 422.00112770215003 430.373882581036 431.42344936415003 432.46563799\n",
      "n 422.00112770215003 425.135071736964 431.42344936415003 430.373882581036\n",
      "m 436.13467292615 424.617345893554 445.55699458815 432.466139838\n",
      "y 436.13467292615 419.90618506255396 445.55699458815 424.617345893554\n",
      "o 450.26809268815 427.227956303928 459.69041435015 432.46676714800003\n",
      "pi 450.26809268815 419.89739005089206 459.69041435015 427.227956303928\n",
      "ni 450.26809268815 412.56682379785605 459.69041435015 419.897390050892\n",
      "o 450.26809268815 407.328012953784 459.69041435015 412.56682379785605\n",
      "n 450.26809268815 402.089202109712 459.69041435015 407.328012953784\n",
      ". 464.40088514015 429.847989035964 473.82320680215 432.467394458\n",
      "< 478.53555952215004 426.96413129739204 487.95788118415004 432.46676714800003\n",
      "E 478.53555952215004 420.679442748838 487.95788118415004 426.96413129739204\n",
      "O 478.53555952215004 413.348876495802 487.95788118415004 420.679442748838\n",
      "S 478.53555952215004 407.064187947248 487.95788118415004 413.348876495802\n",
      "> 478.53555952215004 401.56155209664 487.95788118415004 407.064187947248\n",
      "< 492.66910474615 426.963002139392 502.09142640815 432.46563799\n",
      "p 492.66910474615 421.72419129532 502.09142640815 426.963002139392\n",
      "a 492.66910474615 416.485380451248 502.09142640815 421.72419129532\n",
      "d 492.66910474615 411.24656960717596 502.09142640815 416.485380451248\n",
      "> 492.66910474615 405.743933756568 502.09142640815 411.246569607176\n",
      "T 125.19592522615002 540.937735670518 134.61824688815003 546.694774206\n",
      "h 125.19592522615002 535.698924826446 134.61824688815003 540.937735670518\n",
      "e 125.19592522615002 530.460113982374 134.61824688815003 535.698924826446\n",
      "L 139.32934498815 542.505452991928 148.75166665015 547.7442638360001\n",
      "a 139.32934498815 537.266642147856 148.75166665015 542.505452991928\n",
      "w 139.32934498815 530.463725907892 148.75166665015 537.266642147856\n",
      "will 153.46289021215 530.466617305144 162.88521187415 543.544799772\n",
      "n 167.59693728415002 548.790095495928 177.01925894615002 554.02890634\n",
      "e 167.59693728415002 543.551284651856 177.01925894615002 548.790095495928\n",
      "v 167.59693728415002 538.840123820856 177.01925894615002 543.551284651856\n",
      "e 167.59693728415002 533.6013129767839 177.01925894615002 538.840123820856\n",
      "r 167.59693728415002 530.463679863338 177.01925894615002 533.601312976784\n",
      "b 181.73035704615 535.701146883928 191.15267870815 540.939957728\n",
      "e 181.73035704615 530.4623360398559 191.15267870815 535.701146883928\n",
      "p 195.86390227015002 554.0254992939281 205.28622393215002 559.264310138\n",
      "e 195.86390227015002 548.786688449856 205.28622393215002 554.0254992939281\n",
      "rf 195.86390227015002 543.029649914374 205.28622393215002 548.786688449856\n",
      "e 195.86390227015002 537.790839070302 205.28622393215002 543.029649914374\n",
      "ct 195.86390227015002 530.4602728172661 205.28622393215002 537.790839070302\n",
      ", 209.99732203215 530.457723303964 219.41964369415 533.077128726\n",
      "b 224.13074179415 538.319413361928 233.55306345615 543.558224206\n",
      "ut 224.13074179415 530.46119709582 233.55306345615 538.319413361928\n",
      "it 238.26428701815001 535.170274003 247.68660868015002 539.881434834\n",
      "s 238.26428701815001 530.459113172 247.68660868015002 535.170274003\n",
      "a 252.39833409014997 570.2705699779281 261.82065575214995 575.509380822\n",
      "p 252.39833409014997 565.031759133856 261.82065575214995 570.2705699779281\n",
      "pli 252.39833409014997 555.609437471856 261.82065575214995 565.031759133856\n",
      "c 252.39833409014997 550.898276640856 261.82065575214995 555.6094374718559\n",
      "ati 252.39833409014997 540.948304965784 261.82065575214995 550.898276640856\n",
      "o 252.39833409014997 535.7094941217119 261.82065575214995 540.948304965784\n",
      "n 252.39833409014997 530.47068327764 261.82065575214995 535.7094941217119\n",
      "s 266.53175385214996 553.513571175 275.95407551414996 558.2247320060001\n",
      "h 266.53175385214996 548.2747603309281 275.95407551414996 553.513571175\n",
      "o 266.53175385214996 543.035949486856 275.95407551414996 548.2747603309281\n",
      "ul 266.53175385214996 535.7053832338199 275.95407551414996 543.035949486856\n",
      "d 266.53175385214996 530.466572389748 275.95407551414996 535.7053832338199\n",
      "b 280.66529907615 535.701146883928 290.08762073815 540.939957728\n",
      "e 280.66529907615 530.4623360398559 290.08762073815 535.701146883928\n",
      "j 294.79871883815 543.031105399036 304.22104050015 545.122860808\n",
      "u 294.79871883815 537.792294554964 304.22104050015 543.031105399036\n",
      "st 294.79871883815 530.461728301928 304.22104050015 537.792294554964\n",
      "- 308.93276591014995 530.459661064554 318.35508757214996 533.597294178\n",
      "t 323.06631113415 542.501573455964 332.48863279615 545.120978878\n",
      "hi 323.06631113415 535.171007202928 332.48863279615 542.501573455964\n",
      "s 323.06631113415 530.459846371928 332.48863279615 535.171007202928\n",
      "i 337.19973089615 535.171914795036 346.62205255815 537.2636702039999\n",
      "s 337.19973089615 530.460753964036 346.62205255815 535.171914795036\n",
      "w 351.33315065815003 543.5596140740361 360.75547232015003 550.362530314\n",
      "h 351.33315065815003 538.320803229964 360.75547232015003 543.5596140740361\n",
      "at 351.33315065815003 530.462586963856 360.75547232015003 538.320803229964\n",
      "w 365.46669588214996 535.702305400036 374.88901754414997 542.50522164\n",
      "e 365.46669588214996 530.463494555964 374.88901754414997 535.702305400036\n",
      "a 379.60011564415 538.838951503928 389.02243730615 544.077762348\n",
      "r 379.60011564415 535.701318390482 389.02243730615 538.838951503928\n",
      "e 379.60011564415 530.46250754641 389.02243730615 535.701318390482\n",
      "mi 393.73366086815 552.4566723865901 403.15598253015 562.3972217400001\n",
      "s 393.73366086815 547.74551155559 403.15598253015 552.4566723865901\n",
      "si 393.73366086815 540.942595315626 403.15598253015 547.74551155559\n",
      "n 393.73366086815 535.703784471554 403.15598253015 540.942595315626\n",
      "g 393.73366086815 530.464973627482 403.15598253015 535.703784471554\n",
      ", 407.86770794014996 530.457723303964 417.29002960214996 533.077128726\n",
      "i 422.00112770215003 535.701238973036 431.42344936415003 537.792994382\n",
      "n 422.00112770215003 530.462428128964 431.42344936415003 535.701238973036\n",
      "m 436.13467292615 535.1715746675541 445.55699458815 543.020368612\n",
      "y 436.13467292615 530.460413836554 445.55699458815 535.1715746675541\n",
      "o 450.26809268815 555.608453347928 459.69041435015 560.847264192\n",
      "pi 450.26809268815 548.277887094892 459.69041435015 555.608453347928\n",
      "ni 450.26809268815 540.947320841856 459.69041435015 548.277887094892\n",
      "o 450.26809268815 535.708509997784 459.69041435015 540.947320841856\n",
      "n 450.26809268815 530.469699153712 459.69041435015 535.708509997784\n",
      ". 464.40088514015 530.457095993964 473.82320680215 533.0765014159999\n",
      "< 478.53505767415 555.859775313392 487.95737933615 561.362411164\n",
      "E 478.53505767415 549.575086764838 487.95737933615 555.859775313392\n",
      "O 478.53505767415 542.244520511802 487.95737933615 549.575086764838\n",
      "S 478.53505767415 535.959831963248 487.95737933615 542.244520511802\n",
      "> 478.53505767415 530.4571961126401 487.95737933615 535.959831963248\n",
      "< 492.66910474615 551.683019871392 502.09142640815 557.185655722\n",
      "p 492.66910474615 546.4442090273201 502.09142640815 551.683019871392\n",
      "a 492.66910474615 541.205398183248 502.09142640815 546.44420902732\n",
      "d 492.66910474615 535.9665873391759 502.09142640815 541.205398183248\n",
      "> 492.66910474615 530.463951488568 502.09142640815 535.9665873391759\n",
      "Figure4: 108.0 614.0263216 144.2439388 623.9889216\n",
      "Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. 147.3323448 614.0263216 484.27743939999976 623.9889216\n",
      "Top: 487.3658453999998 614.0263216 505.3881887999997 623.9889216\n",
      "Fullattentionsforhead5. 108.0 624.9353216 211.39552903680004 634.8979216\n",
      "Bottom: 214.48855768320004 624.9353216 247.41726200640005 634.8979216\n",
      "Isolatedattentionsfromjusttheword‘its’forattentionheads5 250.50024835200006 624.9353216 503.99804744639994 634.8979216\n",
      "and6. 108.0 635.8443216000001 132.3485944 645.8069216\n",
      "Notethattheattentionsareverysharpforthisword. 135.4370004 635.8443216000001 343.2568364000001 645.8069216\n",
      "14 301.019 742.0773216 310.98159999999996 752.0399216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Input Layer5 106.973484208 117.560737506848 278.108855361604 137.17960385484798\n",
      "T 124.02330259470001 224.05866603044387 133.3655665907 229.76678933199992\n",
      "h 124.02330259470001 218.86436724866792 133.3655665907 224.05866603044387\n",
      "e 124.02330259470001 213.67006846689196 133.3655665907 218.86436724866792\n",
      "L 138.03663639069998 224.57186857022396 147.3789003867 229.7661673519999\n",
      "a 138.03663639069998 219.377569788448 147.3789003867 224.57186857022396\n",
      "w 138.03663639069998 212.63245518333588 147.3789003867 219.3775697884479\n",
      "will 152.05009458269998 216.798482945552 161.3923585787 229.76554537200002\n",
      "n 166.0634283787 224.57124659022406 175.4056923747 229.76554537200002\n",
      "e 166.0634283787 219.3769478084481 175.4056923747 224.57124659022406\n",
      "v 166.0634283787 214.70581581044803 175.4056923747 219.376947808448\n",
      "e 166.0634283787 209.51151702867207 175.4056923747 214.70581581044803\n",
      "r 166.0634283787 206.40054311800395 175.4056923747 209.51151702867196\n",
      "b 180.07738415469998 224.57186857022396 189.4196481507 229.7661673519999\n",
      "e 180.07738415469998 219.377569788448 189.4196481507 224.57186857022396\n",
      "p 194.0908423467 224.57124659022406 203.4331063427 229.76554537200002\n",
      "e 194.0908423467 219.3769478084481 203.4331063427 224.57124659022406\n",
      "rf 194.0908423467 213.66882450689195 203.4331063427 219.376947808448\n",
      "e 194.0908423467 208.4745257251161 203.4331063427 213.66882450689207\n",
      "ct 194.0908423467 201.206244336228 203.4331063427 208.474525725116\n",
      ", 208.1041761427 227.16901796111188 217.4464401387 229.7661673519999\n",
      "b 222.11750993869998 224.57124659022406 231.4597739347 229.76554537200002\n",
      "ut 222.11750993869998 216.77979841755996 231.4597739347 224.57124659022406\n",
      "it 236.13096813069998 225.09503535399983 245.4732321267 229.7661673519999\n",
      "s 236.13096813069998 220.42390335599998 245.4732321267 225.09503535399995\n",
      "a 250.1449239067 224.57186857022396 259.4871879027 229.7661673519999\n",
      "p 250.1449239067 219.377569788448 259.4871879027 224.57186857022396\n",
      "pli 250.1449239067 210.03530579244784 259.4871879027 219.3775697884479\n",
      "c 250.1449239067 205.364173794448 259.4871879027 210.03530579244796\n",
      "ati 250.1449239067 195.49874301467185 259.4871879027 205.36417379444788\n",
      "o 250.1449239067 190.304444232896 259.4871879027 195.49874301467196\n",
      "n 250.1449239067 185.11014545111993 259.4871879027 190.3044442328959\n",
      "s 264.1582577027 225.09441337400006 273.50052169869997 229.76554537200002\n",
      "h 264.1582577027 219.9001145922241 273.50052169869997 225.09441337400006\n",
      "o 264.1582577027 214.70581581044803 273.50052169869997 219.90011459222399\n",
      "ul 264.1582577027 207.43753442155992 273.50052169869997 214.70581581044803\n",
      "d 264.1582577027 202.24323563978407 273.50052169869997 207.43753442156003\n",
      "b 278.1717158947 224.57186857022396 287.5139798907 229.7661673519999\n",
      "e 278.1717158947 219.377569788448 287.5139798907 224.57186857022396\n",
      "j 292.1850496907 227.69156276488798 301.5273136867 229.76554537200002\n",
      "u 292.1850496907 222.49726398311202 301.5273136867 227.69156276488798\n",
      "st 292.1850496907 215.22898259422402 301.5273136867 222.49726398311202\n",
      "- 306.1983834867 226.6551934413319 315.54064748269997 229.7661673519999\n",
      "t 320.21184167869995 227.16901796111188 329.55410567469994 229.7661673519999\n",
      "hi 320.21184167869995 219.90073657222388 329.55410567469994 227.16901796111188\n",
      "s 320.21184167869995 215.2296045742239 329.55410567469994 219.90073657222388\n",
      "i 334.22579745469994 227.69156276488798 343.56806145069993 229.76554537200002\n",
      "s 334.22579745469994 223.020430766888 343.56806145069993 227.69156276488798\n",
      "w 348.2391312507 223.0210527468879 357.58139524669997 229.7661673519999\n",
      "h 348.2391312507 217.82675396511195 357.58139524669997 223.0210527468879\n",
      "at 348.2391312507 210.03530579244784 357.58139524669997 217.82675396511195\n",
      "w 362.2525894427 223.020430766888 371.5948534387 229.76554537200002\n",
      "e 362.2525894427 217.82613198511206 371.5948534387 223.020430766888\n",
      "a 376.2659232387 224.57186857022396 385.6081872347 229.7661673519999\n",
      "r 376.2659232387 221.46089465955595 385.6081872347 224.57186857022396\n",
      "e 376.2659232387 216.26659587778 385.6081872347 221.46089465955595\n",
      "mi 390.27938143069997 219.9107008162199 399.62164542669996 229.76678933199992\n",
      "s 390.27938143069997 215.23956881821994 399.62164542669996 219.9107008162199\n",
      "si 390.27938143069997 208.49445421310793 399.62164542669996 215.23956881821994\n",
      "n 390.27938143069997 203.30015543133197 399.62164542669996 208.49445421310793\n",
      "g 390.27938143069997 198.10585664955602 399.62164542669996 203.30015543133197\n",
      ", 404.2927152267 227.16839598111198 413.6349792227 229.76554537200002\n",
      "i 418.3066710027 227.6910651808879 427.6489349987 229.76504778799995\n",
      "n 418.3066710027 222.49676639911195 427.6489349987 227.6910651808879\n",
      "m 432.3201291947 221.98343946333205 441.66239319069996 229.76554537200002\n",
      "y 432.3201291947 217.31230746533208 441.66239319069996 221.98343946333205\n",
      "o 446.33346299069996 224.57186857022396 455.67572698669994 229.7661673519999\n",
      "pi 446.33346299069996 217.30358718133584 455.67572698669994 224.57186857022396\n",
      "ni 446.33346299069996 210.03530579244784 455.67572698669994 217.30358718133596\n",
      "o 446.33346299069996 204.841007010672 455.67572698669994 210.03530579244796\n",
      "n 446.33346299069996 199.64670822889593 455.67572698669994 204.8410070106719\n",
      ". 460.3461748067 227.16963994111188 469.6884388027 229.76678933199992\n",
      "< 474.3608769587 224.3102851783359 483.70314095469996 229.7661673519999\n",
      "E 474.3608769587 218.07899509300387 483.70314095469996 224.3102851783359\n",
      "O 474.3608769587 210.81071370411587 483.70314095469996 218.07899509300387\n",
      "S 474.3608769587 204.57942361878384 483.70314095469996 210.81071370411587\n",
      "> 474.3608769587 199.12354144511994 483.70314095469996 204.57942361878395\n",
      "< 488.3742107547 224.30916561433594 497.7164747507 229.76504778799995\n",
      "p 488.3742107547 219.11486683255998 497.7164747507 224.30916561433594\n",
      "a 488.3742107547 213.92056805078403 497.7164747507 219.11486683255998\n",
      "d 488.3742107547 208.72626926900796 497.7164747507 213.9205680507839\n",
      "> 488.3742107547 203.27038709534395 497.7164747507 208.72626926900796\n",
      "T 124.02330259470001 337.31550181444396 133.3655665907 343.02362511599995\n",
      "h 124.02330259470001 332.12120303266795 133.3655665907 337.31550181444396\n",
      "e 124.02330259470001 326.92690425089194 133.3655665907 332.12120303266795\n",
      "L 138.03663639069998 338.86989887422396 147.3789003867 344.064197656\n",
      "a 138.03663639069998 333.67560009244795 147.3789003867 338.86989887422396\n",
      "w 138.03663639069998 326.93048548733594 147.3789003867 333.67560009244795\n",
      "will 152.05009458269998 326.933352317552 161.3923585787 339.900414744\n",
      "n 166.0640503587 345.10114330622395 175.40631435470002 350.29544208799996\n",
      "e 166.0640503587 339.90684452444793 175.40631435470002 345.10114330622395\n",
      "v 166.0640503587 335.2357125264479 175.40631435470002 339.90684452444793\n",
      "e 166.0640503587 330.04141374467196 175.40631435470002 335.23571252644797\n",
      "r 166.0640503587 326.93043983400395 175.40631435470002 330.04141374467196\n",
      "b 180.07738415469998 332.12340621022395 189.4196481507 337.31770499199996\n",
      "e 180.07738415469998 326.92910742844794 189.4196481507 332.12340621022395\n",
      "p 194.0908423467 350.29206399022394 203.4331063427 355.48636277199995\n",
      "e 194.0908423467 345.0977652084479 203.4331063427 350.29206399022394\n",
      "rf 194.0908423467 339.389641906892 203.4331063427 345.0977652084479\n",
      "e 194.0908423467 334.1953431251159 203.4331063427 339.38964190689194\n",
      "ct 194.0908423467 326.927061736228 203.4331063427 334.1953431251159\n",
      ", 208.1041761427 326.924533885112 217.4464401387 329.521683276\n",
      "b 222.11750993869998 334.71942633422395 231.4597739347 339.91372511599997\n",
      "ut 222.11750993869998 326.92797816155996 231.4597739347 334.71942633422395\n",
      "it 236.13096813069998 331.597043942 245.4732321267 336.26817594\n",
      "s 236.13096813069998 326.92591194399995 245.4732321267 331.59704394199997\n",
      "a 250.1449239067 366.39910686222396 259.4871879027 371.593405644\n",
      "p 250.1449239067 361.20480808044795 259.4871879027 366.39910686222396\n",
      "pli 250.1449239067 351.86254408444796 259.4871879027 361.20480808044795\n",
      "c 250.1449239067 347.19141208644794 259.4871879027 351.86254408444796\n",
      "ati 250.1449239067 337.32598130667196 259.4871879027 347.191412086448\n",
      "o 250.1449239067 332.13168252489595 259.4871879027 337.32598130667196\n",
      "n 250.1449239067 326.93738374311994 259.4871879027 332.13168252489595\n",
      "s 264.1582577027 349.78448551799994 273.50052169869997 354.45561751599996\n",
      "h 264.1582577027 344.5901867362239 273.50052169869997 349.78448551799994\n",
      "o 264.1582577027 339.39588795444797 273.50052169869997 344.590186736224\n",
      "ul 264.1582577027 332.12760656556 273.50052169869997 339.39588795444797\n",
      "d 264.1582577027 326.93330778378396 273.50052169869997 332.12760656556\n",
      "b 278.1717158947 332.12340621022395 287.5139798907 337.31770499199996\n",
      "e 278.1717158947 326.92910742844794 287.5139798907 332.12340621022395\n",
      "j 292.1850496907 339.391085024888 301.5273136867 341.465067632\n",
      "u 292.1850496907 334.196786243112 301.5273136867 339.391085024888\n",
      "st 292.1850496907 326.928504854224 301.5273136867 334.196786243112\n",
      "- 306.1990054667 326.92645518133196 315.5412694627 330.03742909199997\n",
      "t 320.21246365869996 338.866052301112 329.55472765469995 341.463201692\n",
      "hi 320.21246365869996 331.597770912224 329.55472765469995 338.866052301112\n",
      "s 320.21246365869996 326.926638914224 329.55472765469995 331.597770912224\n",
      "i 334.22579745469994 331.59867079288796 343.56806145069993 333.67265339999994\n",
      "s 334.22579745469994 326.92753879488794 343.56806145069993 331.59867079288796\n",
      "w 348.2391312507 339.915103174888 357.58139524669997 346.66021778\n",
      "h 348.2391312507 334.72080439311196 357.58139524669997 339.915103174888\n",
      "at 348.2391312507 326.929356220448 357.58139524669997 334.72080439311196\n",
      "w 362.2525894427 332.124554882888 371.5948534387 338.869669488\n",
      "e 362.2525894427 326.930256101112 371.5948534387 332.124554882888\n",
      "a 376.2659232387 335.23455017022394 385.6081872347 340.42884895199995\n",
      "r 376.2659232387 332.12357625955593 385.6081872347 335.23455017022394\n",
      "e 376.2659232387 326.9292774777799 385.6081872347 332.12357625955593\n",
      "mi 390.27938143069997 348.73656677222 399.62164542669996 358.59265528799995\n",
      "s 390.27938143069997 344.0654347742199 399.62164542669996 348.73656677221993\n",
      "si 390.27938143069997 337.32032016910796 399.62164542669996 344.06543477421997\n",
      "n 390.27938143069997 332.12602138733195 399.62164542669996 337.32032016910796\n",
      "g 390.27938143069997 326.93172260555593 399.62164542669996 332.12602138733195\n",
      ", 404.29333720669996 326.924533885112 413.63560120269995 329.521683276\n",
      "i 418.3066710027 332.123497516888 427.6489349987 334.197480124\n",
      "n 418.3066710027 326.929198735112 427.6489349987 332.123497516888\n",
      "m 432.3201291947 331.598333555332 441.66239319069996 339.38043946399995\n",
      "y 432.3201291947 326.92720155733195 441.66239319069996 331.598333555332\n",
      "o 446.33346299069996 351.86156832222395 455.67572698669994 357.05586710399996\n",
      "pi 446.33346299069996 344.59328693333595 455.67572698669994 351.86156832222395\n",
      "ni 446.33346299069996 337.32500554444795 455.67572698669994 344.59328693333595\n",
      "o 446.33346299069996 332.13070676267193 455.67572698669994 337.32500554444795\n",
      "n 446.33346299069996 326.9364079808959 455.67572698669994 332.13070676267193\n",
      ". 460.3461748067 326.923911905112 469.6884388027 329.52106129599997\n",
      "< 474.36025497869997 352.11075490633596 483.70251897469996 357.56663707999996\n",
      "E 474.36025497869997 345.879464821004 483.70251897469996 352.11075490633596\n",
      "O 474.36025497869997 338.611183432116 483.70251897469996 345.879464821004\n",
      "S 474.36025497869997 332.379893346784 483.70251897469996 338.611183432116\n",
      "> 474.36025497869997 326.92401117311994 483.70251897469996 332.37989334678394\n",
      "< 488.3742107547 347.969487670336 497.7164747507 353.425369844\n",
      "p 488.3742107547 342.77518888855997 497.7164747507 347.969487670336\n",
      "a 488.3742107547 337.58089010678395 497.7164747507 342.77518888855997\n",
      "d 488.3742107547 332.386591325008 497.7164747507 337.580890106784\n",
      "> 488.3742107547 326.930709151344 497.7164747507 332.386591325008\n",
      "Input-Input Layer5 106.765759931 335.81292783921594 277.985050645043 355.4414146802159\n",
      "T 123.823939028025 442.36307969507294 133.17078418502498 448.07400208599995\n",
      "h 123.823939028025 437.1662337877809 133.17078418502498 442.36307969507294\n",
      "e 123.823939028025 431.9693878804889 133.17078418502498 437.1662337877809\n",
      "L 137.844144535025 442.876533893708 147.190989692025 448.073379801\n",
      "a 137.844144535025 437.67968798641596 147.190989692025 442.876533893708\n",
      "w 137.844144535025 430.931265783062 147.190989692025 437.67968798641596\n",
      "will 151.864474499025 435.099336438084 161.21131965602498 448.07275751599997\n",
      "n 165.88468000602498 442.87591160870795 175.23152516302497 448.07275751599997\n",
      "e 165.88468000602498 437.67906570141594 175.23152516302497 442.87591160870795\n",
      "v 165.88468000602498 433.005643122916 175.23152516302497 437.679065701416\n",
      "e 165.88468000602498 427.80879721562394 175.23152516302497 433.00564312291596\n",
      "r 165.88468000602498 424.69629777834297 175.23152516302497 427.80879721562394\n",
      "b 179.905507798025 442.876533893708 189.252352955025 448.073379801\n",
      "e 179.905507798025 437.67968798641596 189.252352955025 442.876533893708\n",
      "p 193.925837762025 442.87591160870795 203.272682919025 448.07275751599997\n",
      "e 193.925837762025 437.67906570141594 203.272682919025 442.87591160870795\n",
      "rf 193.925837762025 431.968143310489 203.272682919025 437.679065701416\n",
      "e 193.925837762025 426.77129740319697 203.272682919025 431.968143310489\n",
      "ct 193.925837762025 419.499451871051 203.272682919025 426.77129740319697\n",
      ", 207.946043269025 445.474956847354 217.292888426025 448.073379801\n",
      "b 221.966248776025 442.87591160870795 231.313093933025 448.07275751599997\n",
      "ut 221.966248776025 435.08064274777 231.313093933025 442.87591160870795\n",
      "it 235.98657874002498 443.3999572225 245.33342389702497 448.073379801\n",
      "s 235.98657874002498 438.726534644 245.33342389702497 443.3999572225\n",
      "a 250.00740653202504 442.876533893708 259.35425168902503 448.073379801\n",
      "p 250.00740653202504 437.67968798641596 259.35425168902503 442.876533893708\n",
      "pli 250.00740653202504 428.332842829416 259.35425168902503 437.67968798641596\n",
      "c 250.00740653202504 423.659420250916 259.35425168902503 428.332842829416\n",
      "ati 250.00740653202504 413.78915176512396 259.35425168902503 423.659420250916\n",
      "o 250.00740653202504 408.59230585783195 259.35425168902503 413.78915176512396\n",
      "n 250.00740653202504 403.39545995054 259.35425168902503 408.592305857832\n",
      "s 264.027612039025 443.3993349375 273.374457196025 448.07275751599997\n",
      "h 264.027612039025 438.20248903020797 273.374457196025 443.3993349375\n",
      "o 264.027612039025 433.00564312291596 273.374457196025 438.20248903020797\n",
      "ul 264.027612039025 425.73379759076994 273.374457196025 433.00564312291596\n",
      "d 264.027612039025 420.5369516834779 273.374457196025 425.73379759076994\n",
      "b 278.04794200302496 442.876533893708 287.394787160025 448.073379801\n",
      "e 278.04794200302496 437.67968798641596 287.394787160025 442.876533893708\n",
      "j 292.0681475100249 445.99775789114597 301.41499266702493 448.07275751599997\n",
      "u 292.0681475100249 440.80091198385395 301.41499266702493 445.99775789114597\n",
      "st 292.0681475100249 433.529066451708 301.41499266702493 440.80091198385395\n",
      "- 306.088353017025 444.960880363719 315.435198174025 448.073379801\n",
      "t 320.10868298102497 445.474956847354 329.455528138025 448.073379801\n",
      "hi 320.10868298102497 438.203111315208 329.455528138025 445.474956847354\n",
      "s 320.10868298102497 433.529688736708 329.455528138025 438.203111315208\n",
      "i 334.129510773025 445.99775789114597 343.476355930025 448.07275751599997\n",
      "s 334.129510773025 441.324335312646 343.476355930025 445.99775789114597\n",
      "w 348.14971628002496 441.324957597646 357.496561437025 448.073379801\n",
      "h 348.14971628002496 436.128111690354 357.496561437025 441.324957597646\n",
      "at 348.14971628002496 428.332842829416 357.496561437025 436.128111690354\n",
      "w 362.17004624402495 441.324335312646 371.51689140102496 448.07275751599997\n",
      "e 362.17004624402495 436.12748940535397 371.51689140102496 441.324335312646\n",
      "a 376.19025175102496 442.876533893708 385.537096908025 448.073379801\n",
      "r 376.19025175102496 439.764034456427 385.537096908025 442.876533893708\n",
      "e 376.19025175102496 434.567188549135 385.537096908025 439.764034456427\n",
      "mi 390.21058171502494 438.21308044536494 399.55742687202496 448.07400208599995\n",
      "s 390.21058171502494 433.53965786686496 399.55742687202496 438.21308044536494\n",
      "si 390.21058171502494 426.791235663511 399.55742687202496 433.53965786686496\n",
      "n 390.21058171502494 421.59438975621896 399.55742687202496 426.791235663511\n",
      "g 390.21058171502494 416.39754384892694 399.55742687202496 421.59438975621896\n",
      ", 404.23078722202496 445.474334562354 413.577632379025 448.07275751599997\n",
      "i 418.25161501402494 445.99713560614595 427.59846017102495 448.07213523099995\n",
      "n 418.25161501402494 440.80028969885393 427.59846017102495 445.99713560614595\n",
      "m 432.271944978025 440.28683550021896 441.618790135025 448.07275751599997\n",
      "y 432.271944978025 435.613412921719 441.618790135025 440.28683550021896\n",
      "o 446.292150485025 442.876533893708 455.638995642025 448.073379801\n",
      "pi 446.292150485025 435.60468836156195 455.638995642025 442.876533893708\n",
      "ni 446.292150485025 428.332842829416 455.638995642025 435.604688361562\n",
      "o 446.292150485025 423.135996922124 455.638995642025 428.332842829416\n",
      "n 446.292150485025 417.93915101483196 455.638995642025 423.135996922124\n",
      ". 460.3117337070249 445.47557913235397 469.65857886402495 448.07400208599995\n",
      "< 474.332685956025 442.614822229312 483.679531113025 448.073379801\n",
      "E 474.332685956025 436.380476509593 483.679531113025 442.614822229312\n",
      "O 474.332685956025 429.108630977447 483.679531113025 436.380476509593\n",
      "S 474.332685956025 422.874285257728 483.679531113025 429.108630977447\n",
      "> 474.332685956025 417.41572768604 483.679531113025 422.874285257728\n",
      "< 488.35351374802497 442.61357765931194 497.700358905025 448.07213523099995\n",
      "p 488.35351374802497 437.4167317520199 497.700358905025 442.61357765931194\n",
      "a 488.35351374802497 432.2198858447279 497.700358905025 437.4167317520199\n",
      "d 488.35351374802497 427.02303993743595 497.700358905025 432.21988584472797\n",
      "> 488.35351374802497 421.56448236574795 497.700358905025 427.02303993743595\n",
      "T 123.823939028025 555.6754531730729 133.17078418502498 561.386375564\n",
      "h 123.823939028025 550.478607265781 133.17078418502498 555.6754531730729\n",
      "e 123.823939028025 545.2817613584889 133.17078418502498 550.478607265781\n",
      "L 137.844144535025 557.2304880047079 147.190989692025 562.427333912\n",
      "a 137.844144535025 552.033642097416 147.190989692025 557.2304880047079\n",
      "w 137.844144535025 545.2852198940619 147.190989692025 552.033642097416\n",
      "will 151.864474499025 545.288212587084 161.21131965602498 558.261633665\n",
      "n 165.885302291025 563.464912505708 175.232147448025 568.6617584129999\n",
      "e 165.885302291025 558.268066598416 175.232147448025 563.464912505708\n",
      "v 165.885302291025 553.5946440199159 175.232147448025 558.268066598416\n",
      "e 165.885302291025 548.397798112624 175.232147448025 553.5946440199159\n",
      "r 165.885302291025 545.285298675343 175.232147448025 548.397798112624\n",
      "b 179.905507798025 550.480811523708 189.252352955025 555.677657431\n",
      "e 179.905507798025 545.283965616416 189.252352955025 550.480811523708\n",
      "p 193.925837762025 568.658378658708 203.272682919025 573.855224566\n",
      "e 193.925837762025 563.4615327514159 203.272682919025 568.658378658708\n",
      "rf 193.925837762025 557.750610360489 203.272682919025 563.4615327514159\n",
      "e 193.925837762025 552.5537644531969 203.272682919025 557.750610360489\n",
      "ct 193.925837762025 545.281918921051 203.272682919025 552.553764453197\n",
      ", 207.946043269025 545.2793898303539 217.292888426025 547.877812784\n",
      "b 221.966248776025 553.078104656708 231.313093933025 558.2749505639999\n",
      "ut 221.966248776025 545.28283579577 231.313093933025 553.078104656708\n",
      "it 235.98657874002498 549.9541911435 245.33342389702497 554.627613722\n",
      "s 235.98657874002498 545.280768565 245.33342389702497 549.9541911435\n",
      "a 250.00740653202504 584.773319932708 259.35425168902503 589.9701658399999\n",
      "p 250.00740653202504 579.576474025416 259.35425168902503 584.773319932708\n",
      "pli 250.00740653202504 570.229628868416 259.35425168902503 579.576474025416\n",
      "c 250.00740653202504 565.556206289916 259.35425168902503 570.229628868416\n",
      "ati 250.00740653202504 555.685937804124 259.35425168902503 565.556206289916\n",
      "o 250.00740653202504 550.489091896832 259.35425168902503 555.685937804124\n",
      "n 250.00740653202504 545.2922459895399 259.35425168902503 550.489091896832\n",
      "s 264.027612039025 568.1505512855 273.374457196025 572.823973864\n",
      "h 264.027612039025 562.953705378208 273.374457196025 568.1505512855\n",
      "o 264.027612039025 557.756859470916 273.374457196025 562.953705378208\n",
      "ul 264.027612039025 550.48501393877 273.374457196025 557.756859470916\n",
      "d 264.027612039025 545.288168031478 273.374457196025 550.48501393877\n",
      "b 278.04794200302496 550.480811523708 287.394787160025 555.677657431\n",
      "e 278.04794200302496 545.283965616416 287.394787160025 550.480811523708\n",
      "j 292.0681475100249 557.752054186146 301.41499266702493 559.827053811\n",
      "u 292.0681475100249 552.555208278854 301.41499266702493 557.752054186146\n",
      "st 292.0681475100249 545.283362746708 301.41499266702493 552.555208278854\n",
      "- 306.088353017025 545.281312068719 315.435198174025 548.393811506\n",
      "t 320.109305266025 557.226764002354 329.456150423025 559.8251869559999\n",
      "hi 320.109305266025 549.954918470208 329.456150423025 557.226764002354\n",
      "s 320.109305266025 545.281495891708 329.456150423025 549.954918470208\n",
      "i 334.129510773025 549.9558187921459 343.476355930025 552.030818417\n",
      "s 334.129510773025 545.2823962136459 343.476355930025 549.9558187921459\n",
      "w 348.14971628002496 558.2763292986459 357.496561437025 565.024751502\n",
      "h 348.14971628002496 553.079483391354 357.496561437025 558.2763292986459\n",
      "at 348.14971628002496 545.284214530416 357.496561437025 553.079483391354\n",
      "w 362.17004624402495 550.481960759646 371.51689140102496 557.230382963\n",
      "e 362.17004624402495 545.2851148523539 371.51689140102496 550.481960759646\n",
      "a 376.19025175102496 553.593481093708 385.537096908025 558.7903270009999\n",
      "r 376.19025175102496 550.4809816564269 385.537096908025 553.593481093708\n",
      "e 376.19025175102496 545.284135749135 385.537096908025 550.480981656427\n",
      "mi 390.21058171502494 567.1019942153649 399.55742687202496 576.962915856\n",
      "s 390.21058171502494 562.428571636865 399.55742687202496 567.1019942153649\n",
      "si 390.21058171502494 555.6801494335109 399.55742687202496 562.428571636865\n",
      "n 390.21058171502494 550.483303526219 399.55742687202496 555.6801494335109\n",
      "g 390.21058171502494 545.286457618927 399.55742687202496 550.483303526219\n",
      ", 404.231409507025 545.2793898303539 413.578254664025 547.877812784\n",
      "i 418.25161501402494 550.480902875146 427.59846017102495 552.5559025\n",
      "n 418.25161501402494 545.284056967854 427.59846017102495 550.480902875146\n",
      "m 432.271944978025 549.9554813892189 441.618790135025 557.741403405\n",
      "y 432.271944978025 545.2820588107189 441.618790135025 549.9554813892189\n",
      "o 446.292150485025 570.228652627708 455.638995642025 575.425498535\n",
      "pi 446.292150485025 562.956807095562 455.638995642025 570.228652627708\n",
      "ni 446.292150485025 555.684961563416 455.638995642025 562.956807095562\n",
      "o 446.292150485025 550.4881156561239 455.638995642025 555.684961563416\n",
      "n 446.292150485025 545.291269748832 455.638995642025 550.4881156561239\n",
      ". 460.3117337070249 545.278767545354 469.65857886402495 547.877190499\n",
      "< 474.332685956025 570.4779614053119 483.679531113025 575.936518977\n",
      "E 474.332685956025 564.243615685593 483.679531113025 570.4779614053119\n",
      "O 474.332685956025 556.971770153447 483.679531113025 564.243615685593\n",
      "S 474.332685956025 550.7374244337279 483.679531113025 556.971770153447\n",
      "> 474.332685956025 545.27886686204 483.679531113025 550.7374244337279\n",
      "< 488.35351374802497 566.334663418312 497.700358905025 571.79322099\n",
      "p 488.35351374802497 561.13781751102 497.700358905025 566.334663418312\n",
      "a 488.35351374802497 555.940971603728 497.700358905025 561.13781751102\n",
      "d 488.35351374802497 550.744125696436 497.700358905025 555.940971603728\n",
      "> 488.35351374802497 545.285568124748 497.700358905025 550.744125696436\n",
      "Figure5: 108.0 602.1433216 145.222863876 612.1059216\n",
      "Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe 148.891292448 602.1433216 503.99721058799975 612.1059216\n",
      "sentence. 108.0 613.0523215999999 144.05604416399999 623.0149216\n",
      "Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention 147.10220874 613.0523215999999 504.00139487999985 623.0149216\n",
      "atlayer5of6. 108.0 623.9623216 165.8229304 633.9249216\n",
      "Theheadsclearlylearnedtoperformdifferenttasks. 168.91133639999998 623.9623216 377.2591902000001 633.9249216\n",
      "15 301.019 742.0773216 310.98159999999996 752.0399216\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        for block in page.extract_words(x_tolerance=3, y_tolerance=3, keep_blank_chars=True, use_text_flow=True):\n",
    "            print(block[\"text\"], block[\"x0\"], block[\"top\"], block[\"x1\"], block[\"bottom\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d61406",
   "metadata": {},
   "source": [
    "# PyMuPDF / fitz – Line + Block Level Extraction\n",
    "PyMuPDF (fitz) can extract structured text using:\n",
    "\n",
    "page.get_text(\"blocks\") – returns grouped blocks\n",
    "\n",
    "page.get_text(\"dict\") – full structure with lines, spans, fonts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6713bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: 1\n",
      "Introduction at [108.0, 72.78716278076172, 190.8136749267578, 84.74236297607422]\n",
      "Block 1: Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15]. at [108.0, 99.01528930664062, 504.1676330566406, 152.68905639648438]\n",
      "Block 2: Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains. at [108.0, 159.04031372070312, 504.35089111328125, 245.44107055664062]\n",
      "Block 3: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network. at [107.64099884033203, 251.89015197753906, 505.65545654296875, 294.55706787109375]\n",
      "Block 4: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs. at [107.69100189208984, 300.9082946777344, 505.73870849609375, 343.6730651855469]\n",
      "Block 5: 2\n",
      "Background at [108.0, 363.10015869140625, 188.82911682128906, 375.05535888671875]\n",
      "Block 6: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2. at [107.69100189208984, 389.4489440917969, 505.2414245605469, 486.6390686035156]\n",
      "Block 7: Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. at [108.0, 493.1185607910156, 505.2498779296875, 535.7550659179688]\n",
      "Block 8: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34]. at [108.0, 542.105224609375, 505.6537780761719, 573.9620361328125]\n",
      "Block 9: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9]. at [107.69100189208984, 580.312255859375, 505.656982421875, 623.0770263671875]\n",
      "Block 10: 3\n",
      "Model Architecture at [108.0, 642.504150390625, 226.09344482421875, 654.4593505859375]\n",
      "Block 11: Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next. at [108.0, 668.8084716796875, 505.7431640625, 722.4070434570312]\n",
      "Block 12: 2 at [303.5090026855469, 742.3324584960938, 308.49029541015625, 752.2950439453125]\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc[1]\n",
    "blocks = page.get_text(\"blocks\")\n",
    "\n",
    "for block in blocks:\n",
    "    x0, y0, x1, y1, text, block_no = block[:6]\n",
    "    print(f\"Block {block_no}: {text.strip()} at [{x0}, {y0}, {x1}, {y1}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b6062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
