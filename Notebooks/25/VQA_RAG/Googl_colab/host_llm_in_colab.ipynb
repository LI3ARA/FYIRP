{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LI3ARA/FYIRP/blob/main/Notebooks/25/VQA_RAG/Googl_colab/host_llm_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Mt_D5cFL6S"
      },
      "source": [
        "# Working code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmuWlMKs-CtQ"
      },
      "source": [
        "Colab:\n",
        "\n",
        "- Run llava-cli in a Flask or FastAPI app\n",
        "\n",
        "- Expose it using ngrok so VS Code can reach it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFYJfd5-CtR"
      },
      "source": [
        "Steps:\n",
        "- Load the LLaVA model using llava.cpp\n",
        "\n",
        "- Start a Flask server in Colab\n",
        "\n",
        "- Expose the server via ngrok\n",
        "\n",
        "- Accept an image + prompt, run llava-cli, and return the output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install necessary files and libraries"
      ],
      "metadata": {
        "id": "NSVc5gbFF-5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jngk4eW0FOrL"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install flask llama-cpp-python --quiet\n",
        "\n",
        "# Install and set up ngrok\n",
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU1U-B1kNIEF",
        "outputId": "708aa135-b720-4da7-e17c-71f69ca73610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Add your ngrok auth token (looks like 2N...)\n",
        "!./ngrok config add-authtoken 2vLvftpzesWBGHU32SAnWOWmryA_CvhQGZ7cyfpk6TEkPG1b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf llava.cpp\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y build-essential cmake git"
      ],
      "metadata": {
        "id": "Sx8M-JIoHo_M",
        "outputId": "9b93c035-224f-442a-b739-80d07aad934e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone llama.cpp from ggml-org"
      ],
      "metadata": {
        "id": "PwgctYJwKWEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone & build LLaVA\n",
        "!rm -rf llava.cpp\n",
        "!git clone --recursive https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "\n",
        "# CMake build with Vision Support\n",
        "!cmake -S . -B build -DGGML_CUDA=on -DLLAMA_LLAVA_VISION=on\n",
        "!cmake --build build -j"
      ],
      "metadata": {
        "id": "oEzmyk95R5I3",
        "outputId": "1a111c51-f72a-4ce7-c818-1dd4f09afb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mCMake Warning at CMakeLists.txt:107 (message):\n",
            "  LLAMA_CUDA is deprecated and will be removed in the future.\n",
            "\n",
            "  Use GGML_CUDA instead\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:113 (llama_option_depr)\n",
            "\n",
            "\u001b[0m\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- CUDA Toolkit found\n",
            "-- Using CUDA architectures: native\n",
            "\u001b[0m-- CUDA host compiler is GNU 11.4.0\n",
            "\u001b[0m\n",
            "-- Including CUDA backend\n",
            "-- Configuring done (0.5s)\n",
            "-- Generating done (0.3s)\n",
            "\u001b[33mCMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    LLAMA_LLAVA_VISION\n",
            "\n",
            "\u001b[0m\n",
            "-- Build files have been written to: /content/llama.cpp/llama.cpp/llama.cpp/build\n",
            "[  0%] Built target build_info\n",
            "[  0%] Built target sha256\n",
            "[  1%] Built target sha1\n",
            "[  2%] Built target xxhash\n",
            "[  5%] Built target ggml-base\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n",
            "[ 38%] Built target ggml-cuda\n",
            "[ 38%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 38%] Built target ggml\n",
            "[ 39%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 46%] Built target llama-gguf\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 46%] Built target llama-gguf-hash\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 46%] Built target llama\n",
            "[ 46%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 50%] Built target test-c\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 51%] Built target llama-simple\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 51%] Built target llama-simple-chat\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 52%] Built target llama-quantize-stats\n",
            "[ 52%] Built target llava\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
            "[ 53%] Built target llava_static\n",
            "[ 53%] Built target llava_shared\n",
            "[ 53%] Built target mtmd\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd_shared.so\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX static library libmtmd_static.a\u001b[0m\n",
            "[ 53%] Built target mtmd_static\n",
            "[ 53%] Built target mtmd_shared\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 54%] Built target common\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 82%] Built target test-model-load-cancel\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 85%] Built target test-log\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 85%] Built target test-rope\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 86%] Built target test-barrier\n",
            "[ 86%] Built target test-autorelease\n",
            "[ 86%] Built target test-quantize-fns\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 88%] Built target llama-gbnf-validator\n",
            "[ 88%] Built target llama-llava-clip-quantize-cli\n",
            "[ 88%] Built target llama-lookup-create\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 88%] Built target llama-lookup-merge\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 89%] Built target test-tokenizer-1-spm\n",
            "[ 89%] Built target llama-q8dot\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 89%] Built target test-tokenizer-1-bpe\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 91%] Built target test-grammar-parser\n",
            "[ 91%] Built target llama-vdot\n",
            "[ 91%] Built target llama-gguf-split\n",
            "[ 91%] Built target llama-tokenize\n",
            "[ 91%] Built target llama-batched-bench\n",
            "[ 91%] Built target test-sampling\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 92%] Built target llama-save-load-state\n",
            "[ 92%] Built target test-arg-parser\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 94%] Built target llama-infill\n",
            "[ 94%] Built target llama-eval-callback\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 95%] Built target test-tokenizer-0\n",
            "[ 95%] Built target llama-speculative-simple\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 95%] Built target llama-lookup\n",
            "[ 95%] Built target llama-batched\n",
            "[ 95%] Built target test-llama-grammar\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 97%] Built target llama-gen-docs\n",
            "[ 97%] Built target llama-gritlm\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 97%] Built target llama-lookup-stats\n",
            "[ 97%] Built target llama-lookahead\n",
            "[ 97%] Built target llama-parallel\n",
            "[ 97%] Built target llama-passkey\n",
            "[ 97%] Built target test-quantize-perf\n",
            "[ 97%] Built target llama-quantize\n",
            "[ 97%] Built target llama-embedding\n",
            "[ 97%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 98%] Built target test-gguf\n",
            "[ 98%] Built target llama-qwen2vl-cli\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 98%] Built target llama-llava-cli\n",
            "[ 98%] Built target llama-minicpmv-cli\n",
            "[ 98%] Built target llama-export-lora\n",
            "[ 98%] Built target llama-retrieval\n",
            "[ 98%] Built target test-chat-template\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-speculative\n",
            "[ 99%] Built target llama-gemma3-cli\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[100%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[100%] Built target llama-cli\n",
            "[100%] Built target llama-imatrix\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[100%] Built target llama-perplexity\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[100%] Built target test-grammar-integration\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[100%] Built target llama-run\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[100%] Built target test-json-schema-to-grammar\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[100%] Built target llama-bench\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[100%] Built target test-chat\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[100%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[100%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Verify Build Output\n",
        "!ls build/bin"
      ],
      "metadata": {
        "id": "AtJ0sfl8Genx",
        "outputId": "c90944c5-64d7-40de-fa7d-4970693ea572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libggml-base.so\t\t       llama-q8dot\n",
            "libggml-cpu.so\t\t       llama-quantize\n",
            "libggml-cuda.so\t\t       llama-quantize-stats\n",
            "libggml.so\t\t       llama-qwen2vl-cli\n",
            "libllama.so\t\t       llama-retrieval\n",
            "libllava_shared.so\t       llama-run\n",
            "libmtmd_shared.so\t       llama-save-load-state\n",
            "llama-batched\t\t       llama-server\n",
            "llama-batched-bench\t       llama-simple\n",
            "llama-bench\t\t       llama-simple-chat\n",
            "llama-cli\t\t       llama-speculative\n",
            "llama-convert-llama2c-to-ggml  llama-speculative-simple\n",
            "llama-cvector-generator        llama-tokenize\n",
            "llama-embedding\t\t       llama-tts\n",
            "llama-eval-callback\t       llama-vdot\n",
            "llama-export-lora\t       test-arg-parser\n",
            "llama-gbnf-validator\t       test-autorelease\n",
            "llama-gemma3-cli\t       test-backend-ops\n",
            "llama-gen-docs\t\t       test-barrier\n",
            "llama-gguf\t\t       test-c\n",
            "llama-gguf-hash\t\t       test-chat\n",
            "llama-gguf-split\t       test-chat-template\n",
            "llama-gritlm\t\t       test-gguf\n",
            "llama-imatrix\t\t       test-grammar-integration\n",
            "llama-infill\t\t       test-grammar-parser\n",
            "llama-llava-cli\t\t       test-json-schema-to-grammar\n",
            "llama-llava-clip-quantize-cli  test-llama-grammar\n",
            "llama-lookahead\t\t       test-log\n",
            "llama-lookup\t\t       test-model-load-cancel\n",
            "llama-lookup-create\t       test-quantize-fns\n",
            "llama-lookup-merge\t       test-quantize-perf\n",
            "llama-lookup-stats\t       test-rope\n",
            "llama-minicpmv-cli\t       test-sampling\n",
            "llama-parallel\t\t       test-tokenizer-0\n",
            "llama-passkey\t\t       test-tokenizer-1-bpe\n",
            "llama-perplexity\t       test-tokenizer-1-spm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-llava-cli"
      ],
      "metadata": {
        "id": "vrij82l9Gsy0",
        "outputId": "5866705e-cf31-4efe-9be2-d1b6596fd048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "\n",
            " example usage:\n",
            "\n",
            "     ./build/bin/llama-llava-cli -m <llava-v1.5-7b/ggml-model-q5_k.gguf> --mmproj <llava-v1.5-7b/mmproj-model-f16.gguf> --image <path/to/an/image.jpg> --image <path/to/another/image.jpg> [--temp 0.1] [-p \"describe the image in detail.\"]\n",
            "\n",
            " note: a lower temperature value like 0.1 is recommended for better quality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"llama-llava-cli\""
      ],
      "metadata": {
        "id": "PjNeP-TQHVPK",
        "outputId": "8e9b0070-1ef5-4e12-c83a-b836873d6b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./build/bin/llama-llava-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download LLaVA GGUF Model and Image"
      ],
      "metadata": {
        "id": "aTo0iXAZKRds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GGUF Model from Second State (7B, Q4_K_M)\n",
        "# !mkdir -p models\n",
        "# !wget -O models/llava-v1.5-7b-Q4_K_M.gguf https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "mKcl4x_aEdlh",
        "outputId": "662fb755-34a8-4c9c-8652-50e61c5a8b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 05:30:07--  https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.121, 13.35.202.40, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/2687b20ac8b7a23f6c70296d5b1e7f908fef2ce4769ecdebd1bb9503528a75bf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-Q4_K_M.gguf%3B+filename%3D%22llava-v1.5-7b-Q4_K_M.gguf%22%3B&Expires=1744785007&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4NTAwN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvMjY4N2IyMGFjOGI3YTIzZjZjNzAyOTZkNWIxZTdmOTA4ZmVmMmNlNDc2OWVjZGViZDFiYjk1MDM1MjhhNzViZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VIq469Az8738vtlzRmH5N70N5JRE7OZmN8fwpLwtcYG1e0ORwADmtkYoG86i0AEz3NOmpSCZYig6zVgcJV5r95beqfcZNGuFOfOZhJFxZm5g9362hW4wMzuAc3P0QmAeuota625rQHsPQQ3wjVSYiEMUtCvh6EO0iYK0Kb9t88OaJDB96R88Mm3Wz00wIDDkQ6VNaDHQ4SyB7PhAHLDyftKTPAczv41eKD8yzVTZnsNAfHZY0ByXp2M-FOCZDYULpPhzBQmhKG09WZZoaR7s5bTOO5X9-WiELj9sbLlY5tSyZqDg4MIJDWlpa-ii4pXNRS9%7EJwRqryd%7EPw8KoSauMg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-16 05:30:07--  https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/2687b20ac8b7a23f6c70296d5b1e7f908fef2ce4769ecdebd1bb9503528a75bf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-Q4_K_M.gguf%3B+filename%3D%22llava-v1.5-7b-Q4_K_M.gguf%22%3B&Expires=1744785007&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4NTAwN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvMjY4N2IyMGFjOGI3YTIzZjZjNzAyOTZkNWIxZTdmOTA4ZmVmMmNlNDc2OWVjZGViZDFiYjk1MDM1MjhhNzViZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VIq469Az8738vtlzRmH5N70N5JRE7OZmN8fwpLwtcYG1e0ORwADmtkYoG86i0AEz3NOmpSCZYig6zVgcJV5r95beqfcZNGuFOfOZhJFxZm5g9362hW4wMzuAc3P0QmAeuota625rQHsPQQ3wjVSYiEMUtCvh6EO0iYK0Kb9t88OaJDB96R88Mm3Wz00wIDDkQ6VNaDHQ4SyB7PhAHLDyftKTPAczv41eKD8yzVTZnsNAfHZY0ByXp2M-FOCZDYULpPhzBQmhKG09WZZoaR7s5bTOO5X9-WiELj9sbLlY5tSyZqDg4MIJDWlpa-ii4pXNRS9%7EJwRqryd%7EPw8KoSauMg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.35.202.117, 13.35.202.18, 13.35.202.120, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.35.202.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4081004320 (3.8G) [binary/octet-stream]\n",
            "Saving to: ‘models/llava-v1.5-7b-Q4_K_M.gguf’\n",
            "\n",
            "models/llava-v1.5-7 100%[===================>]   3.80G   302MB/s    in 14s     \n",
            "\n",
            "2025-04-16 05:30:21 (283 MB/s) - ‘models/llava-v1.5-7b-Q4_K_M.gguf’ saved [4081004320/4081004320]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p models\n",
        "!mkdir -p models\n",
        "!wget -O models/llava.gguf https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q5_K_M.gguf\n"
      ],
      "metadata": {
        "id": "W10AWxpqFAiT",
        "outputId": "6c88244d-7efc-4cc2-c2bf-3003915135a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 06:35:02--  https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q5_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/217d0d6a0069af51040ea4bbd1761d038e378e9dfd1851cb5be57f405d01920d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-Q5_K_M.gguf%3B+filename%3D%22llava-v1.5-7b-Q5_K_M.gguf%22%3B&Expires=1744788902&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4ODkwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvMjE3ZDBkNmEwMDY5YWY1MTA0MGVhNGJiZDE3NjFkMDM4ZTM3OGU5ZGZkMTg1MWNiNWJlNTdmNDA1ZDAxOTIwZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZxupVwyMmeH4iJ2BgdUtLBptxTdADlUXMe%7EBJqApeho2FBH21rCZRzbmO7u9HicAeuRGWchcfInhoc3Qy2cJMGjQDZ%7EJtrY0PmE5afBDXWPTAKGSSLMrMa%7EXsUNwPHW5vPryvcJQI6g5c2IQPkhBciE00x67yFtsFBKKYreIRJqWPR3uCTxItZ0wK1IfcvH8jKTz3m8l4Yz%7Ea%7EjjH%7EWNKvtyEfW6QPpckOf14zqJqmsgNYR54dbpIUn3sIL4LFDBJC43g9N2GUOZl7O-wI9LKHFFhN3jto-UO-Bp-U3YN47BXdgeYc234cu9IuhI-cBq7Si7U-fbZrRzTHwUI0psuQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-16 06:35:02--  https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/217d0d6a0069af51040ea4bbd1761d038e378e9dfd1851cb5be57f405d01920d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-Q5_K_M.gguf%3B+filename%3D%22llava-v1.5-7b-Q5_K_M.gguf%22%3B&Expires=1744788902&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4ODkwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvMjE3ZDBkNmEwMDY5YWY1MTA0MGVhNGJiZDE3NjFkMDM4ZTM3OGU5ZGZkMTg1MWNiNWJlNTdmNDA1ZDAxOTIwZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZxupVwyMmeH4iJ2BgdUtLBptxTdADlUXMe%7EBJqApeho2FBH21rCZRzbmO7u9HicAeuRGWchcfInhoc3Qy2cJMGjQDZ%7EJtrY0PmE5afBDXWPTAKGSSLMrMa%7EXsUNwPHW5vPryvcJQI6g5c2IQPkhBciE00x67yFtsFBKKYreIRJqWPR3uCTxItZ0wK1IfcvH8jKTz3m8l4Yz%7Ea%7EjjH%7EWNKvtyEfW6QPpckOf14zqJqmsgNYR54dbpIUn3sIL4LFDBJC43g9N2GUOZl7O-wI9LKHFFhN3jto-UO-Bp-U3YN47BXdgeYc234cu9IuhI-cBq7Si7U-fbZrRzTHwUI0psuQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.75.101, 3.165.75.33, 3.165.75.112, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.75.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4783157024 (4.5G) [binary/octet-stream]\n",
            "Saving to: ‘models/llava.gguf’\n",
            "\n",
            "models/llava.gguf   100%[===================>]   4.45G   266MB/s    in 21s     \n",
            "\n",
            "2025-04-16 06:35:23 (218 MB/s) - ‘models/llava.gguf’ saved [4783157024/4783157024]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm the path of the model\n",
        "!ls -lh models/llava.gguf"
      ],
      "metadata": {
        "id": "6bzgvaI-HGQ6",
        "outputId": "63464290-056b-42d3-9c6a-1a0de3897558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 4.5G Feb 28  2024 models/llava.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the projection Layer"
      ],
      "metadata": {
        "id": "7HpnGG42O0MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision projection layer\n",
        "!wget -O models/mmproj.gguf https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-mmproj-model-f16.gguf"
      ],
      "metadata": {
        "id": "AD5LSo2bO3af",
        "outputId": "cba27c55-46d0-4d59-affb-4d9412a9467f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 06:20:55--  https://huggingface.co/second-state/Llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-mmproj-model-f16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/50da4e5b0a011615f77686f9b02613571e65d23083c225e107c08c3b1775d9b1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-mmproj-model-f16.gguf%3B+filename%3D%22llava-v1.5-7b-mmproj-model-f16.gguf%22%3B&Expires=1744788055&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4ODA1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvNTBkYTRlNWIwYTAxMTYxNWY3NzY4NmY5YjAyNjEzNTcxZTY1ZDIzMDgzYzIyNWUxMDdjMDhjM2IxNzc1ZDliMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Q%7E27OPd3mGRjJklv4S-Yi3KgmZnGTv13FyLQwoaL4BZDARgMR3qhADYDboFEJrtrNyLSC4xTHXl9HRUlMYHRXHwTJBIV2TM5Ao5F0o0tmPRYQW8i0-pLR5HZSRHsNzpag5axDN2M1-vz96kGoTVvIiD9eZt%7EtJ-UlqDxO5gWOxGhesvXAPjcMIrkBM2HW9vzoLH1GwQdCYwiPp40GKS4clkySTEdu1a80ZC7krArLV89K7LqduuJvFqFXGg%7EOJKXAIOfitwFPckkflL%7EaeEW1aESy7KU3DlwERReacDshhUXwSWuvJ7WNAzVJd6ZQuZjy2mjJ1FZ1QopGHlDMJvrKg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-16 06:20:55--  https://cdn-lfs-us-1.hf.co/repos/43/d0/43d0efd222e373f385d959b6e01e13661ead062fc9fff5a8adb0f53e8dba9f6c/50da4e5b0a011615f77686f9b02613571e65d23083c225e107c08c3b1775d9b1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava-v1.5-7b-mmproj-model-f16.gguf%3B+filename%3D%22llava-v1.5-7b-mmproj-model-f16.gguf%22%3B&Expires=1744788055&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc4ODA1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQzL2QwLzQzZDBlZmQyMjJlMzczZjM4NWQ5NTliNmUwMWUxMzY2MWVhZDA2MmZjOWZmZjVhOGFkYjBmNTNlOGRiYTlmNmMvNTBkYTRlNWIwYTAxMTYxNWY3NzY4NmY5YjAyNjEzNTcxZTY1ZDIzMDgzYzIyNWUxMDdjMDhjM2IxNzc1ZDliMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Q%7E27OPd3mGRjJklv4S-Yi3KgmZnGTv13FyLQwoaL4BZDARgMR3qhADYDboFEJrtrNyLSC4xTHXl9HRUlMYHRXHwTJBIV2TM5Ao5F0o0tmPRYQW8i0-pLR5HZSRHsNzpag5axDN2M1-vz96kGoTVvIiD9eZt%7EtJ-UlqDxO5gWOxGhesvXAPjcMIrkBM2HW9vzoLH1GwQdCYwiPp40GKS4clkySTEdu1a80ZC7krArLV89K7LqduuJvFqFXGg%7EOJKXAIOfitwFPckkflL%7EaeEW1aESy7KU3DlwERReacDshhUXwSWuvJ7WNAzVJd6ZQuZjy2mjJ1FZ1QopGHlDMJvrKg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.45.103, 13.33.45.57, 13.33.45.80, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.45.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 624434368 (596M) [binary/octet-stream]\n",
            "Saving to: ‘models/mmproj.gguf’\n",
            "\n",
            "models/mmproj.gguf  100%[===================>] 595.51M  8.62MB/s    in 41s     \n",
            "\n",
            "2025-04-16 06:21:37 (14.4 MB/s) - ‘models/mmproj.gguf’ saved [624434368/624434368]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test The Model - Vision"
      ],
      "metadata": {
        "id": "7On3ABKIEwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O cat.jpg https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg"
      ],
      "metadata": {
        "id": "eDME7s05Lx1C",
        "outputId": "ca8bd18e-96e1-477b-9f60-5a9103bf3d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 06:24:47--  https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2001:df2:e500:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279603 (273K) [image/jpeg]\n",
            "Saving to: ‘cat.jpg’\n",
            "\n",
            "\rcat.jpg               0%[                    ]       0  --.-KB/s               \rcat.jpg             100%[===================>] 273.05K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-04-16 06:24:47 (27.6 MB/s) - ‘cat.jpg’ saved [279603/279603]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-llava-cli \\\n",
        "  -m models/llava.gguf \\\n",
        "  --mmproj models/mmproj.gguf \\\n",
        "  --image cat.jpg \\\n",
        "  -p \"Describe this image in detail.\" \\\n",
        "  --temp 0.1"
      ],
      "metadata": {
        "id": "Nz61K_jtE1oc",
        "outputId": "a909eaae-7adf-4a66-eaa7-ba227d849352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40082 MiB free\n",
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from models/llava.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_K - Medium\n",
            "print_info: file size   = 4.45 GiB (5.68 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  4560.87 MiB\n",
            "..................................................................................................\n",
            "clip_ctx: CLIP using CUDA0 backend\n",
            "clip_model_loader: model name:   openai/clip-vit-large-patch14-336\n",
            "clip_model_loader: description:  image encoder for LLaVA\n",
            "clip_model_loader: GGUF version: 3\n",
            "clip_model_loader: alignment:    32\n",
            "clip_model_loader: n_tensors:    377\n",
            "clip_model_loader: n_kv:         19\n",
            "\n",
            "load_hparams: text_encoder:       0\n",
            "load_hparams: vision_encoder:     1\n",
            "load_hparams: llava_projector:    1\n",
            "load_hparams: minicpmv_projector: 0\n",
            "load_hparams: minicpmv_version:   2\n",
            "load_hparams: glm_projector:      0\n",
            "load_hparams: model size:         13889957695810.24 MiB\n",
            "load_hparams: metadata size:      0.13 MiB\n",
            "alloc_compute_meta:      CUDA0 compute buffer size =    32.89 MiB\n",
            "alloc_compute_meta:        CPU compute buffer size =     3.55 MiB\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_context: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   355.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_context: graph nodes  = 1094\n",
            "llama_context: graph splits = 356 (with bs=512), 1 (with bs=1)\n",
            "encode_image_with_clip: image embedding created: 576 tokens\n",
            "\n",
            "encode_image_with_clip: image encoded in   198.08 ms by CLIP (    0.34 ms per image patch)\n",
            "\n",
            " The image features a close-up of a brown and white cat sitting on a sidewalk. The cat is looking directly at the camera, capturing the viewer's attention. The cat appears to be a tabby, with its fur and markings visible. The scene is set outdoors, with a red hose in the background, possibly indicating that the cat is near a water source.\n",
            "llama_perf_context_print:        load time =    7519.59 ms\n",
            "llama_perf_context_print: prompt eval time =    5479.21 ms /   622 tokens (    8.81 ms per token,   113.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12498.62 ms /    82 runs   (  152.42 ms per token,     6.56 tokens per second)\n",
            "llama_perf_context_print:       total time =   20185.93 ms /   704 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the LLM - Mistral"
      ],
      "metadata": {
        "id": "1B61WtaQVNhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -hf lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF -p \"What's the capital of France?\""
      ],
      "metadata": {
        "id": "PABxhptqZYsu",
        "outputId": "1da35766-757f-41d2-d4e0-b8c3d43bb260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./build/bin/llama-cli: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "# !./build/bin/llama-cli -hf lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF\n"
      ],
      "metadata": {
        "id": "vBybjyA2VQ9l",
        "outputId": "9e98915e-bc8c-47ba-ce5b-6408f0cb69e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/lmstudio-community_Mistral-7B-Instruct-v0.3-GGUF_Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\n",
            "curl_perform_with_retry: Trying to download from https://huggingface.co/lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf to /root/.cache/llama.cpp/lmstudio-community_Mistral-7B-Instruct-v0.3-GGUF_Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (server_etag:\"f13896f73ac927396749ee484a4a1691-1\", server_last_modified:Wed, 22 May 2024 19:28:46 GMT)...\n",
            "curl_perform_with_retry: Trying to download from https://huggingface.co/lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (attempt 1 of 3)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1148  100  1148    0     0   4900      0 --:--:-- --:--:-- --:--:--  4900\n",
            "100 4170M  100 4170M    0     0  20.7M      0  0:03:21  0:03:21 --:--:-- 22.1M\n",
            "common_download_file_single: file metadata saved: /root/.cache/llama.cpp/lmstudio-community_Mistral-7B-Instruct-v0.3-GGUF_Mistral-7B-Instruct-v0.3-Q4_K_M.gguf.json\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40082 MiB free\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /root/.cache/llama.cpp/lmstudio-community_Mistral-7B-Instruct-v0.3-GGUF_Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = /models/Mistral-7B-Instruct-v0.3-GGUF...\n",
            "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
            "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 228\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  4169.52 MiB\n",
            "................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_context: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   297.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_context: graph nodes  = 1094\n",
            "llama_context: graph splits = 356 (with bs=512), 1 (with bs=1)\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 6\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "main: chat template example:\n",
            "[INST] You are a helpful assistant\n",
            "Hello [/INST]Hi there</s>[INST] How are you? [/INST]\n",
            "\n",
            "system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CUDA : ARCHS = 800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 3058717021\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            "\n",
            "> \n",
            "\n",
            "llama_perf_sampler_print:    sampling time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)\n",
            "llama_perf_sampler_print:    sampling time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)\n",
            "llama_perf_context_print:        load time =     858.50 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  720725.38 ms /     2 tokens\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Yp3vdavDideX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask API with Dual Endpoints"
      ],
      "metadata": {
        "id": "Yd3b4MYtifMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import subprocess\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"🧠 API is running! Use /chat (Mistral) or /vision (LLaVA)\"\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    prompt = request.json.get(\"prompt\", \"\")\n",
        "    if not prompt:\n",
        "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
        "\n",
        "    result = subprocess.run([\n",
        "        \"./build/bin/llama-cli\",\n",
        "        \"-m\", \"models/mistral.gguf\",\n",
        "        \"-p\", prompt,\n",
        "        \"--temp\", \"0.7\"\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    output = result.stdout.strip().split(\"\\n\")[-1]\n",
        "    return jsonify({\"response\": output})\n",
        "\n",
        "@app.route('/vision', methods=['POST'])\n",
        "def vision():\n",
        "    img = request.files[\"image\"]\n",
        "    prompt = request.form.get(\"prompt\", \"Describe the image\")\n",
        "    img_path = f\"/tmp/{uuid.uuid4().hex}.jpg\"\n",
        "    img.save(img_path)\n",
        "\n",
        "    result = subprocess.run([\n",
        "        \"./build/bin/llama-llava-cli\",\n",
        "        \"-m\", \"models/llava.gguf\",\n",
        "        \"--mmproj\", \"models/mmproj.gguf\",\n",
        "        \"--image\", img_path,\n",
        "        \"-p\", prompt,\n",
        "        \"--temp\", \"0.1\"\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    os.remove(img_path)\n",
        "    output = result.stdout.strip().split(\"\\n\")[-1]\n",
        "    return jsonify({\"response\": output})\n"
      ],
      "metadata": {
        "id": "4ftEf0YSiedg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Run the Flask App in Colab\n",
        "import threading # to in the background and use other cells\n",
        "\n",
        "def run_app():\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "RkEOtYaTionx",
        "outputId": "6a280d04-5bff-4768-d1a8-03f1e7dd8ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start ngrok in background\n",
        "ngrok = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Fetch the public URL\n",
        "try:\n",
        "    r = requests.get(\"http://localhost:4040/api/tunnels\")\n",
        "    public_url = r.json()[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\" Public API:\", public_url)\n",
        "except Exception as e:\n",
        "    print(\"Failed to get ngrok URL:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "FNwiH1DejCS6",
        "outputId": "252a6e87-8ae0-41de-97f3-be020d926c61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Public API: https://7b9e-34-142-149-123.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cBTml3RZicWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From VS Code"
      ],
      "metadata": {
        "id": "MdNk1DJTjksJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use /chat endpoint"
      ],
      "metadata": {
        "id": "Itg8_ZNdlnGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(\n",
        "    \"https://your-ngrok-url.ngrok.io/chat\",\n",
        "    json={\"prompt\": \"What is retrieval-augmented generation?\"}\n",
        ")\n",
        "print(response.json()[\"response\"])\n"
      ],
      "metadata": {
        "id": "mR3K25g8lpcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use /vision endpoint"
      ],
      "metadata": {
        "id": "IOGs3OOnlmOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\"image\": open(\"cat.jpg\", \"rb\")}\n",
        "data = {\"prompt\": \"What is this image about?\"}\n",
        "\n",
        "response = requests.post(\"https://your-ngrok-url.ngrok.io/vision\", files=files, data=data)\n",
        "print(response.json()[\"response\"])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vrpDcXTTl6Sz",
        "outputId": "9a629414-41b0-496f-9b51-f2b95553f282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cat.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-821f16cd8f1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cat.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is this image about?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://your-ngrok-url.ngrok.io/vision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cat.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4-Hv03lKjiyS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NBETG2Pll0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Flask API"
      ],
      "metadata": {
        "id": "e9MmZsm8GFgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Flask API\n",
        "import os\n",
        "import subprocess\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '👋 LLaVA API is running! Use POST /generate'\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    img_file = request.files['image']\n",
        "    prompt = request.form.get('prompt', '')\n",
        "\n",
        "    # Save the uploaded image\n",
        "    img_path = \"input.jpg\"\n",
        "    img_file.save(img_path)\n",
        "\n",
        "    # Run LLaVA CLI on the image and prompt\n",
        "    result = subprocess.run([\n",
        "        \"./llava-cli\",\n",
        "        \"-m\", \"models/llava.gguf\",\n",
        "        \"--image\", img_path,\n",
        "        \"-p\", prompt\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    output = result.stdout.strip().split(\"\\n\")[-1]\n",
        "    return jsonify({\"response\": output})\n",
        "\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "CoaqtaEgGLel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-J7KHGGNRB6"
      },
      "outputs": [],
      "source": [
        "import subprocess, time, requests\n",
        "from llama_cpp import Llama\n",
        "from flask import Flask, request, jsonify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Q1cYiuMW84",
        "outputId": "f3a51257-fe64-4175-99ad-7acf14392814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Public URL: https://0232-34-16-171-130.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Start ngrok on port 5000 and fetch public URL\n",
        "\n",
        "ngrok_proc = subprocess.Popen(['./ngrok', 'http', '5000'])\n",
        "time.sleep(5)\n",
        "try:\n",
        "    res = requests.get('http://localhost:4040/api/tunnels')\n",
        "    url = res.json()['tunnels'][0]['public_url']\n",
        "    print('✅ Public URL:', url)\n",
        "except Exception as e:\n",
        "    print('❌ Could not get ngrok URL:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8c072a489f2347c69b10cf2952de5a6e",
            "1810a08aadb540aabba47a1f92acd2fc",
            "be82ef8f8b0c41118f79bc00c858598e",
            "200e9bee33de444a922ee749c9a6c5db",
            "2d4b76d82be54f7e980ba04dabcc9a3c",
            "a2a85f91c7c249edb4f64fab09272f49",
            "31c88a5585ad49778453703229c72eee",
            "82fc804174714681a6ef470015cabc7d",
            "be67c35135e341b8a46fef422e270530",
            "dbea87103d1b4171a522257bef479930",
            "2110b2f6a80b47838f8ff201987bd59f"
          ]
        },
        "id": "8adlw1oINOkW",
        "outputId": "9d0940fa-c357-419d-c32e-e19caa7680aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c072a489f2347c69b10cf2952de5a6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-2-13b-chat.Q3_K_S.gguf:   0%|          | 0.00/5.66G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/./llama-2-13b-chat.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q3_K:  281 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q3_K - Small\n",
            "print_info: file size   = 5.27 GiB (3.48 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 40\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 5120\n",
            "print_info: n_embd_v_gqa     = 5120\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 13B\n",
            "print_info: model params     = 13.02 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: layer  37 assigned to device CPU\n",
            "load_tensors: layer  38 assigned to device CPU\n",
            "load_tensors: layer  39 assigned to device CPU\n",
            "load_tensors: layer  40 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q3_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  5396.11 MiB\n",
            "...................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
            "llama_init_from_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   204.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id='TheBloke/Llama-2-13B-chat-GGUF',\n",
        "    filename='llama-2-13b-chat.Q3_K_S.gguf',\n",
        "    n_gpu_layers=40,\n",
        "    n_ctx=2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tcj_IcOMken",
        "outputId": "209c65ac-0a46-4bc8-9e9a-e3360b07580e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     602.16 ms /     4 tokens (  150.54 ms per token,     6.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     603.06 ms /     5 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:11:07] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 3 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:12] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:19] \"\u001b[31m\u001b[1mGET /generate HTTP/1.1\u001b[0m\" 405 -\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   44834.83 ms /   200 runs   (  224.17 ms per token,     4.46 tokens per second)\n",
            "llama_perf_context_print:       total time =   44957.55 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:31] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 2 prefix-match hit, remaining 2 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     317.11 ms /     2 tokens (  158.55 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   44713.26 ms /   199 runs   (  224.69 ms per token,     4.45 tokens per second)\n",
            "llama_perf_context_print:       total time =   45153.57 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:14:16] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Start Flask app with LLaMA endpoint\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '✅ LLaMA Flask API is running! Use /generate'\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    prompt = request.json.get('prompt', '')\n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'Prompt is required'}), 400\n",
        "    output = llm(prompt, max_tokens=200)\n",
        "    return jsonify({'response': output['choices'][0]['text'].strip()})\n",
        "\n",
        "app.run(port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp0AL-wF8Svw"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DvZGmwsFPUF"
      },
      "source": [
        "# Step by step working- test code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-nmRAx4FUxf"
      },
      "source": [
        "Without llm , just using a test text to pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BhImDgFaW3"
      },
      "source": [
        "## In colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "EbhDaJW5FwA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqAiqrFm_gn4",
        "outputId": "0e72bfe3-b52a-4f10-e8d0-f894b888088a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.11/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmr8FWlpAlHR",
        "outputId": "9acb59c2-f4d4-4292-a66f-37ee2eaa149c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n",
        "\n",
        "# 🧠 Your ngrok auth token goes here (not your API key)\n",
        "!./ngrok config add-authtoken <here>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RMtckAoTEZSu",
        "outputId": "2d5b1431-4ff4-42f9-cb01-f7c014d95824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement llama_cpp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llama_cpp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install llama_cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxBHSwfaEYWH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "UsI9kLv0FzhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f6Mpll0_frA",
        "outputId": "98b5c725-ac53-4025-c213-4dc70f24395e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ngrok public URL: https://d67b-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import subprocess, time, requests\n",
        "from flask import Flask\n",
        "\n",
        "# Start ngrok on port 5000\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Try to get the public tunnel URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\"✅ ngrok public URL:\", public_url)\n",
        "except Exception as e:\n",
        "    print(\"❌ ngrok failed:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmwkYtOnA5Ke",
        "outputId": "ecc0e471-f557-4293-9394-dc66afd53381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"✅ LLaMA Flask API is running!\"\n",
        "\n",
        "app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsySGo50CHKs",
        "outputId": "cbca4b9e-7d17-4e60-f3ea-51da30d3c975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Raw response: {\"tunnels\":[{\"name\":\"command_line\",\"ID\":\"e2a1c3a7137a897877e63bb2f6a64125\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://f4cf-104-199-189-124.ngrok-free.app\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:5000\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n",
            "\n",
            "✅ Public URL: https://f4cf-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Check if ngrok is alive\n",
        "import subprocess, time, requests\n",
        "\n",
        "# Start ngrok (it runs in the background)\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(4)\n",
        "\n",
        "# Try to read the tunnel info\n",
        "try:\n",
        "    res = requests.get(\"http://localhost:4040/api/tunnels\")\n",
        "    print(\"🔍 Raw response:\", res.text)\n",
        "    data = res.json()\n",
        "    print(\"✅ Public URL:\", data[\"tunnels\"][0][\"public_url\"])\n",
        "except Exception as e:\n",
        "    print(\"❌ Could not fetch ngrok tunnel:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2daf9o11Cimp",
        "outputId": "ceff1232-2a55-4bdc-8d00-bd47ff15709e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tunnels': [{'name': 'command_line',\n",
              "   'ID': 'e2a1c3a7137a897877e63bb2f6a64125',\n",
              "   'uri': '/api/tunnels/command_line',\n",
              "   'public_url': 'https://f4cf-104-199-189-124.ngrok-free.app',\n",
              "   'proto': 'https',\n",
              "   'config': {'addr': 'http://localhost:5000', 'inspect': True},\n",
              "   'metrics': {'conns': {'count': 0,\n",
              "     'gauge': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0},\n",
              "    'http': {'count': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0}}}],\n",
              " 'uri': '/api/tunnels'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "requests.get(\"http://localhost:4040/api/tunnels\").json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUCNhATA518",
        "outputId": "987e1af2-34f7-4829-9405-c04c02726cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:43] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:24:24] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:29:44] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"✅ Hello from Flask with ngrok!\"\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    data = request.get_json()\n",
        "    prompt = data.get(\"prompt\", \"No prompt provided.\")\n",
        "    return jsonify({\n",
        "        \"response\": f\"🧪 Test response to: '{prompt}'\"\n",
        "    })\n",
        "\n",
        "app.run(port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USnGfbJ_Fdxn"
      },
      "source": [
        "## In VS code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-MlibVFFjFu",
        "outputId": "69a1e3e9-208c-4ceb-eb91-b213c0d28405"
      },
      "outputs": [
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\requests\\models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://f4cf-104-199-189-124.ngrok-free.app/generate\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your real one\u001b[39;00m\n\u001b[32m      5\u001b[39m response = requests.post(url, json={\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHello world!\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🧠 Response:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\requests\\models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "# In vs code\n",
        "import requests\n",
        "\n",
        "url = \"https://f4cf-104-199-189-124.ngrok-free.app/generate\"  # Replace with your real one\n",
        "response = requests.post(url, json={\"prompt\": \"Hello world!\"})\n",
        "print(\"🧠 Response:\", response.json()[\"response\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8cvx9qlBQM2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QrZyIVH-Ctc",
        "outputId": "92464e8c-60ae-46d4-d1d8-150511a0757f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ],
      "source": [
        "! pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM0XBaZN-Ctc",
        "outputId": "c400d25c-ffe5-4ac4-a5e8-9b43ab6c7dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                    \r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPyngrokNgrokError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use ngrok to expose your Flask server\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyngrok\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ngrok\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m public_url = \u001b[43mngrok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPublic URL:\u001b[39m\u001b[33m\"\u001b[39m, public_url)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\ngrok.py:346\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(addr, proto, name, pyngrok_config, **options)\u001b[39m\n\u001b[32m    342\u001b[39m _upgrade_legacy_params(pyngrok_config, options)\n\u001b[32m    344\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpening tunnel named: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m api_url = \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m.api_url\n\u001b[32m    348\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    350\u001b[39m tunnel = NgrokTunnel(api_request(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/tunnels\u001b[39m\u001b[33m\"\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, data=options,\n\u001b[32m    351\u001b[39m                                  timeout=pyngrok_config.request_timeout),\n\u001b[32m    352\u001b[39m                      pyngrok_config, api_url)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\ngrok.py:171\u001b[39m, in \u001b[36mget_ngrok_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    167\u001b[39m     pyngrok_config = conf.get_default()\n\u001b[32m    169\u001b[39m install_ngrok(pyngrok_config)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\process.py:265\u001b[39m, in \u001b[36mget_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config.ngrok_path):\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config.ngrok_path]\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\process.py:428\u001b[39m, in \u001b[36m_start_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    425\u001b[39m kill_process(pyngrok_config.ngrok_path)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process.startup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process.startup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    429\u001b[39m                             ngrok_process.logs,\n\u001b[32m    430\u001b[39m                             ngrok_process.startup_error)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process was unable to start.\u001b[39m\u001b[33m\"\u001b[39m, ngrok_process.logs)\n",
            "\u001b[31mPyngrokNgrokError\u001b[39m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ],
      "source": [
        "# Use ngrok to expose your Flask server\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqyR7Odh-Ctc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "IRPenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1810a08aadb540aabba47a1f92acd2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a85f91c7c249edb4f64fab09272f49",
            "placeholder": "​",
            "style": "IPY_MODEL_31c88a5585ad49778453703229c72eee",
            "value": "llama-2-13b-chat.Q3_K_S.gguf: 100%"
          }
        },
        "200e9bee33de444a922ee749c9a6c5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbea87103d1b4171a522257bef479930",
            "placeholder": "​",
            "style": "IPY_MODEL_2110b2f6a80b47838f8ff201987bd59f",
            "value": " 5.66G/5.66G [00:32&lt;00:00, 191MB/s]"
          }
        },
        "2110b2f6a80b47838f8ff201987bd59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d4b76d82be54f7e980ba04dabcc9a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c88a5585ad49778453703229c72eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82fc804174714681a6ef470015cabc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c072a489f2347c69b10cf2952de5a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1810a08aadb540aabba47a1f92acd2fc",
              "IPY_MODEL_be82ef8f8b0c41118f79bc00c858598e",
              "IPY_MODEL_200e9bee33de444a922ee749c9a6c5db"
            ],
            "layout": "IPY_MODEL_2d4b76d82be54f7e980ba04dabcc9a3c"
          }
        },
        "a2a85f91c7c249edb4f64fab09272f49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be67c35135e341b8a46fef422e270530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be82ef8f8b0c41118f79bc00c858598e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fc804174714681a6ef470015cabc7d",
            "max": 5658980224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be67c35135e341b8a46fef422e270530",
            "value": 5658980224
          }
        },
        "dbea87103d1b4171a522257bef479930": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}