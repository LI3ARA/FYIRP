{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb51b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483f2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generator import MistralGenerator\n",
    "from image_generator import LlavaGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ef0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generator import MultimodalAnswerPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c420c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# from tqdm import tqdm\n",
    "# from image_generator import LlavaGenerator\n",
    "from utils import load_data, save_batch_output, log_time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120f75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_gen = MistralGenerator(model=\"mistral-instruct-7b@q4_k_m\",base_url=os.getenv('REMOTE_URL'))\n",
    "llava_gen = LlavaGenerator(model=\"llava-v1.5-7b@q5_k_m\",base_url=os.getenv('LOCAL_URL'))\n",
    "\n",
    "pipeline = MultimodalAnswerPipeline(mistral=mistral_gen, llava=llava_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42032c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Load dataset\n",
    "DATASET_PATH = \"../Data/VisDoM-main/spiqa/spiqa.csv\"\n",
    "IMAGE_FOLDER = \"../Data/spiqa/test-A/SPIQA_testA_Images/SPIQA_testA_Images\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "id_list = df[\"q_id\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b71cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../Eval_outputs/SPIQA/ReAlignQA\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31b6023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_batches = {\n",
    "    int(f.split(\"_\")[1])\n",
    "    for f in os.listdir(OUTPUT_DIR)\n",
    "    if f.startswith(\"batch_\") and f.endswith(\".csv\")\n",
    "}\n",
    "existing_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ecd68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b8e0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fca4af61ef147a9884318baa2f76f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819d7486433942c6a6d6e9895814855c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Figure 9 in the paper \"Disentangling Language and Knowledge in Task-Oriented '\n",
      "                                                     'Dialogs\" depicts the transformation process between the original and '\n",
      "                                                     'pre-processed SMD Navigate datasets. The visualization is presented as a '\n",
      "                                                     'table with two columns - one for the \"Original SMD Navigate Dataset\" and '\n",
      "                                                     'another for the \"Preprocessed SMD Navigate Dataset.\" Each column contains '\n",
      "                                                     'several rows of words or phrases, with an arrow pointing from the left to '\n",
      "                                                     'the right. This visual representation illustrates how pre-processing '\n",
      "                                                     'simplifies and focuses on essential information for improved navigation by '\n",
      "                                                     'eliminating unnecessary details from the original dataset.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Increasing the value of β2 in the salient object detection model optimized '\n",
      "                                                     'by the FLoss function leads to a trade-off between precision and recall. '\n",
      "                                                     'Specifically, when β2 is low, both precision and recall are high, indicating '\n",
      "                                                     'good performance in correctly identifying relevant objects while minimizing '\n",
      "                                                     'incorrect identifications. However, as β2 increases, precision decreases '\n",
      "                                                     'while recall remains relatively stable or even increases at some values. '\n",
      "                                                     'This means that the model becomes more selective and discriminative with '\n",
      "                                                     'higher β2 values, but it may also result in a loss of detection for certain '\n",
      "                                                     'instances that are not well-suited for this trade-off. Therefore, by '\n",
      "                                                     'adjusting the value of β2, one can find an optimal balance between precision '\n",
      "                                                     'and recall to suit specific application requirements, such as prioritizing '\n",
      "                                                     'recall in applications where comprehensive detection is more important than '\n",
      "                                                     'reducing false positives.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Figure 1 in the paper demonstrates the difference between 1-D and 2-D cake '\n",
      "                                                     'division by illustrating a square-shaped cake, which is a two-dimensional '\n",
      "                                                     'object. In 2-D division, due to geometric constraints like rectangularity, '\n",
      "                                                     'it is possible for an efficient allocation to leave some unallocated '\n",
      "                                                     'portions of the cake, unlike in 1-D division where the cake is divided into '\n",
      "                                                     'equal parts without any such issues arising from shape or geometry. This is '\n",
      "                                                     'supported by the caption, which mentions that under certain geometric '\n",
      "                                                     'constraints, some cake might be left unallocated, and the visual model being '\n",
      "                                                     'a square-shaped cake suggests this concept is being applied in a '\n",
      "                                                     'two-dimensional context.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('According to Table 2, ChoiceNet performs better under symmetric noise (20% '\n",
      "                                                     'and 50%) settings compared to the Pair-45% asymmetric noise setting in terms '\n",
      "                                                     'of test accuracies on the CIFAR-10 dataset. This indicates that ChoiceNet '\n",
      "                                                     'demonstrates a stronger ability to handle symmetrically distributed noises, '\n",
      "                                                     'as it achieves higher accuracy in these conditions. However, its performance '\n",
      "                                                     'is lower when confronted with asymmetrically distributed noises like the one '\n",
      "                                                     'presented in the Pair-45% setting. This suggests that ChoiceNet might have '\n",
      "                                                     'specific strengths and limitations depending on the type of noise '\n",
      "                                                     'encountered. Therefore, when applying this model to real-world scenarios '\n",
      "                                                     \"where different noise patterns can arise, it's crucial to consider these \"\n",
      "                                                     'factors to ensure optimal performance.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('There were 3482 negative samples allocated to the training set of the CNSE '\n",
      "                                                     'dataset, as per the 60% split used in the experiments, according to the data '\n",
      "                                                     'breakdown provided in Table 1 of the paper \"Matching Article Pairs with '\n",
      "                                                     'Graphical Decomposition and Convolutions.\"')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('In the context of the provided figure illustrating the Conditional VSR '\n",
      "                                                     '(D_s,t), the warped triplets play a crucial role in improving frame '\n",
      "                                                     'alignment and achieving more accurate video super-resolution results. The '\n",
      "                                                     'method involves using motion information to analyze and interpret data '\n",
      "                                                     'through a mathematical process. Specifically, the warped triplets are used '\n",
      "                                                     'within a conditional VSR equation (D_s,t) as an efficient technique for '\n",
      "                                                     'super-resolution in this research paper. This approach helps enhance the '\n",
      "                                                     'alignment of frames, thereby leading to better video super-resolution '\n",
      "                                                     'outcomes.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('In the figure from the Pascal VOC 2012 validation set provided in the paper, '\n",
      "                                                     'the denoiser that produces the most accurate segmentation of the sheep can '\n",
      "                                                     'be seen in image (c), where the denoiser was trained with the reconstruction '\n",
      "                                                     'and segmentation joint loss. This conclusion is supported by the associated '\n",
      "                                                     'segmentation label map where the boundaries of the sheep appear to be more '\n",
      "                                                     'precise compared to the other methods shown. Specifically, the segmentation '\n",
      "                                                     'of the sheep in image (c) shows no major discontinuities or inaccuracies, '\n",
      "                                                     'unlike the separate denoiser in image (b), which has a noticeable inaccuracy '\n",
      "                                                     'in the red-boxed region. The segmentation in images (a) and (d) are not '\n",
      "                                                     'directly compared to the sheep in this figure, so it is not possible to '\n",
      "                                                     'definitively state whether they produce more accurate or less accurate '\n",
      "                                                     'segmentations of the sheep compared to image (c).')\n",
      "ic| self.mistral.generate_answer(detection_prompt): ('The information needed to answer the question is not provided in the figure '\n",
      "                                                     'caption alone. To determine if human evaluators demonstrated higher accuracy '\n",
      "                                                     'in identifying human-written reviews compared to machine-generated reviews, '\n",
      "                                                     'we would need data comparing the accuracy of H1 (individual votes) and H2 '\n",
      "                                                     '(majority votes) for both human-written and machine-generated reviews. So, '\n",
      "                                                     'YES, analyzing the visual (Figure 2) would be necessary to answer this '\n",
      "                                                     'question accurately.')\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('According to the data presented in Figure 2 from the paper \"Judge the '\n",
      "                                                     'Judges: A Large-Scale Evaluation Study of Neural Language Models for Online '\n",
      "                                                     'Review Generation,\" human evaluators did not demonstrate higher accuracy in '\n",
      "                                                     'identifying human-written reviews compared to machine-generated reviews when '\n",
      "                                                     'considering both individual (H1) and majority (H2) voting criteria.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the provided information from Table 2, there is no explicit '\n",
      "                                                     'comparison of exact match (EM), F1 scores, or speed-up in training between '\n",
      "                                                     'the SRU model and the cuDNN-optimized LSTM on the SQuAD dataset within the '\n",
      "                                                     'table itself. However, it does state that the SRU model outperforms other '\n",
      "                                                     'models and is more than five times faster than cuDNN LSTM in terms of '\n",
      "                                                     'training speed. But for EM and F1 scores, the data from Table 2 does not '\n",
      "                                                     'provide a direct comparison between these two specific models.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The ablation experiment that resulted in the most significant drop in '\n",
      "                                                     'accuracy (from 80.1% to 65.6%) was the removal of Glove initialization. This '\n",
      "                                                     \"indicates that Glove initialization plays a critical role in the model's \"\n",
      "                                                     'performance when working on the Penn WSJ dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to:  ../Eval_outputs/SPIQA/ReAlignQA/batch_0_20250424_233542.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1749cb4f7454ec0abe99ed088eb68bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): 'NO'\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the data presented in the \"BDD100K: A Diverse Driving Dataset for '\n",
      "                                                     'Heterogeneous Multitask Learning\" paper and Table 11 of that same paper, the '\n",
      "                                                     'object category with the highest combined total of bounding box and instance '\n",
      "                                                     \"track annotations in the BDD100K MOT dataset is 'Vehicle'.\")\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The provided information indicates that the visual model has identified '\n",
      "                                                     \"'Table 1' as 'Dataset Statistics.' However, no specific data or numbers have \"\n",
      "                                                     'been provided in the question. Therefore, to give a comprehensive answer, I '\n",
      "                                                     'would need additional information about the contents of Table 1, such as the '\n",
      "                                                     'number of samples, mean and standard deviation values, or any other relevant '\n",
      "                                                     'statistics. The table caption suggests that it contains some sort of '\n",
      "                                                     'statistical details about a dataset.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'NO'\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the provided information from Figure (b) in the comprehensive study '\n",
      "                                                     'of the ACNN model, it appears that the model achieves its highest accuracy '\n",
      "                                                     'compared to all others within the category of question types in the WikiQA '\n",
      "                                                     'dataset. However, for a more accurate interpretation, it would be ideal to '\n",
      "                                                     'have access to the specific accuracy values for each question type.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"The SRU model's translation training configuration addresses the challenges \"\n",
      "                                                     'associated with large vocabulary sizes by employing various techniques such '\n",
      "                                                     'as token-based batching, shared embedding, and positional encoding. '\n",
      "                                                     'Token-based batching reduces memory requirements by processing tokens in '\n",
      "                                                     'batches rather than individual words. Shared embedding helps manage large '\n",
      "                                                     'vocabulary sizes efficiently by reusing embeddings across multiple '\n",
      "                                                     'sentences. Positional encoding handles the additional dimensions needed due '\n",
      "                                                     'to high order dependencies within sentence structures, thereby contributing '\n",
      "                                                     'to effective translation during training.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The multitask learning approach that achieved the highest mean Intersection '\n",
      "                                                     'over Union (IoU) for semantic segmentation, as per Table 8 in the BDD100K '\n",
      "                                                     'paper, is the one trained on both lane detection and semantic segmentation '\n",
      "                                                     'tasks. This method outperformed the baseline Sem-Seg model significantly, '\n",
      "                                                     'improving the mean IoU from 35.78% to a remarkable 82.46%.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on Table 5, the fastest attack method for crafting adversarial '\n",
      "                                                     'examples using the Cross-Entropy (CE) objective function and ResNet-32 model '\n",
      "                                                     'across both MNIST and CIFAR-10 datasets is not the C&W-wb attack as '\n",
      "                                                     'previously mentioned. Instead, the fastest attack is the Fast Gradient Sign '\n",
      "                                                     'Method (FGSM), which has an average time cost that is 9.8 seconds faster '\n",
      "                                                     'than the C&W-wb attack. This information was derived from the table provided '\n",
      "                                                     'where the values are averages between MNIST and CIFAR-10 datasets using '\n",
      "                                                     'Resnet-32 as the model.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The method that achieves the lowest end-point error on the Sintel \"Final\" '\n",
      "                                                     \"test set is Sintel (ft), with an error of 1.0. Compared to Devon (ft)'s \"\n",
      "                                                     'error of 6.35, Sintel (ft) performs significantly better in terms of '\n",
      "                                                     'end-point error.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The model that achieved the best F1-all score in the KITTI 2015 test set '\n",
      "                                                     'among the fine-tuned models is Kitti-All2012. It scored 9.36, which is '\n",
      "                                                     \"higher than Devon's score of 8.78 on the same test set. Therefore, \"\n",
      "                                                     'Kitti-All2012 outperforms Devon in terms of F1-all score on this specific '\n",
      "                                                     'test set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to:  ../Eval_outputs/SPIQA/ReAlignQA/batch_10_20250424_234733.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2794b13051b49ccb32b221da3aef927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 20:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): ('YES (Assuming that the figure provides sufficient data to compare the '\n",
      "                                                     'runtime performance of One Step ALOQ and WSN for F-SRE1 and F-SRE2)')\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the visual comparison provided in the paper \"Alternating '\n",
      "                                                     'Optimisation and Quadrature for Robust Control,\" One Step ALOQ does not '\n",
      "                                                     'appear to show superior runtime efficiency over WSN for both F-SRE1 and '\n",
      "                                                     'F-SRE2. The graph indicates that, in some instances, WSN has a faster '\n",
      "                                                     \"runtime compared to One Step ALOQ. However, it's important to note that this \"\n",
      "                                                     'analysis is based solely on the provided visual model and does not consider '\n",
      "                                                     'other factors such as algorithm complexity or problem size, which could '\n",
      "                                                     'potentially impact runtime performance. For a definitive conclusion, further '\n",
      "                                                     'investigation or more detailed data would be necessary.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the data provided in the Table 1 of the paper \"Global Relation '\n",
      "                                                     'Embedding for Relation Extraction,\" approximately 67,304 out of the total '\n",
      "                                                     '67,304 entity pairs in the NYT training set are associated with a '\n",
      "                                                     'corresponding relational fact in the knowledge base. This implies that 100% '\n",
      "                                                     'of the entity pairs in the NYT training set have a match in the knowledge '\n",
      "                                                     'base.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes, the question requires analyzing the visual content (Table 14).'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"The BoSsNet's multi-hop encoder, as demonstrated in Table 14, significantly \"\n",
      "                                                     'enhances performance on bAbI tasks such as restaurant sorting by rating '\n",
      "                                                     '(task 3) and preference-based restaurant recommendation (task 5). This '\n",
      "                                                     \"improvement is due to the encoder's ability to access multiple knowledge \"\n",
      "                                                     'bases (KB entries) during reasoning and inferencing.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'This multi-step reasoning and inference process enables the model to '\n",
      "                                                     'overcome the limitations of single-step reasoning, thereby expanding its '\n",
      "                                                     'capabilities to handle complex tasks that require a deeper understanding of '\n",
      "                                                     'the data. This is particularly beneficial for tasks like restaurant sorting '\n",
      "                                                     'by rating, where the model needs to compare multiple restaurants based on '\n",
      "                                                     'their ratings, and preference-based restaurant recommendation, where the '\n",
      "                                                     'model must make decisions based on user preferences, which often involves '\n",
      "                                                     'multi-step reasoning and inference over various KB entries.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     \"In summary, BoSsNet's multi-hop encoder improves performance on these tasks \"\n",
      "                                                     'by allowing for a more comprehensive analysis of the data, enabling it to '\n",
      "                                                     'provide accurate predictions or answers that would be difficult with a '\n",
      "                                                     'single-step approach.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The authors show that their S-ACNN model with a single filter outperforms '\n",
      "                                                     'the S-CNN model on the Yelp P. and DBpedia datasets as indicated by lower F1 '\n",
      "                                                     'scores in Table 2. This superior performance is attributed to the adaptive '\n",
      "                                                     'filter mechanism of S-ACNN, which enables it to capture sentence-specific '\n",
      "                                                     'features more effectively compared to S-CNN. Although S-ACNN does not always '\n",
      "                                                     'achieve the lowest overall test error rates, the authors justify this by '\n",
      "                                                     'arguing that a single adaptive filter provides sufficient flexibility and '\n",
      "                                                     'selectivity in extracting relevant information from the input text. This '\n",
      "                                                     'results in better performance while still being expressive in terms of '\n",
      "                                                     'capturing sentence-specific features.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The underperformance of the transfer learning-based VAGER model in '\n",
      "                                                     'comparison to LR for the \"Bubble\" class in Table 2 from the paper \"Learning '\n",
      "                                                     'to Learn Image Classifiers with Visual Analogy\" can be attributed to the '\n",
      "                                                     'fact that the VAGER model relies on a pre-trained feature extractor. This '\n",
      "                                                     'pre-trained feature extractor has not been exposed to the specific features '\n",
      "                                                     'of the target novel classes, such as the \"Bubble\" class. As a result, when '\n",
      "                                                     'it comes to recognizing subtle distinctions between categories with limited '\n",
      "                                                     'training data in a 1-shot binary classification setting, the VAGER model may '\n",
      "                                                     'struggle to learn and effectively generalize compared to a more specialized '\n",
      "                                                     'LR model that has been specifically trained for this task. However, the '\n",
      "                                                     'VAGER model demonstrates superior accuracy across other novel classes '\n",
      "                                                     'because it benefits from its ability to leverage knowledge learned from a '\n",
      "                                                     'wide range of pre-training data, which allows it to perform well in '\n",
      "                                                     'recognizing common visual patterns and features.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The error rate of ITN (B-CNN) on the MNIST dataset decreases as the update '\n",
      "                                                     'threshold (Tu) increases, specifically from 1e-3 to 1e-1, according to Table '\n",
      "                                                     '7 in the \"Resisting Large Data Variations via Introspective Transformation '\n",
      "                                                     'Network\" paper.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the data presented in Table 6, it appears that the adversarial '\n",
      "                                                     'examples generated with the 2-keyword constraint demonstrate a significant '\n",
      "                                                     'deviation from the original syntactic structure as predicted by the language '\n",
      "                                                     'model, since they have higher perplexity scores compared to the original '\n",
      "                                                     'input. This suggests that the generated examples are less predictable and '\n",
      "                                                     \"structured according to the language model's expectations.\")\n",
      "ic| self.mistral.generate_answer(detection_prompt): ('Yes (The answer can be inferred from the text and visual representation '\n",
      "                                                     'provided.)')\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"Based on the visual model's response and the provided caption, the figure in \"\n",
      "                                                     'the paper suggests that participants with graduate degrees who have formal '\n",
      "                                                     'training in reasoning, logic, or argumentation show statistically '\n",
      "                                                     'significant improvements in accuracy for the argument reasoning '\n",
      "                                                     'comprehension task. This conclusion is drawn from the mean values displayed '\n",
      "                                                     'in the figure which indicate higher levels of accuracy for those with such '\n",
      "                                                     'training compared to others.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): ('The information needed to answer this question is not presented visually but '\n",
      "                                                     'given in text, so it can be answered from text alone. Therefore, I would '\n",
      "                                                     'reply with \"NO\" as there\\'s no figure or graph provided that shows the '\n",
      "                                                     'accuracy comparison for CIFAR-10 based on the sharing of layers. However, '\n",
      "                                                     'the caption does indicate a potential advantage of sharing layers, but '\n",
      "                                                     'without concrete numerical data or comparative results, it is not possible '\n",
      "                                                     'to definitively state whether this leads to better accuracy for the given '\n",
      "                                                     'network.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The empirical data from the \"Delayed Impact of Fair Machine Learning\" paper '\n",
      "                                                     'indicates that for both black and white groups in the TransUnion TransRisk '\n",
      "                                                     'dataset, the probability of debt repayment varies with credit scores. For '\n",
      "                                                     'the black group, the payback rate increases with higher credit scores. At a '\n",
      "                                                     'score of 610, the payback rate is around 4%, while it is closer to 25% at a '\n",
      "                                                     'score of 780 or above. Conversely, for the white group, the payback rate '\n",
      "                                                     'decreases as the credit score increases. Specifically, at a score of 350, '\n",
      "                                                     'the probability of repayment is about 19%, and drops to around 6% at a score '\n",
      "                                                     'of 620. Overall, the black group has higher payback rates than the white '\n",
      "                                                     'group with lower scores but lower payback rates than the white group with '\n",
      "                                                     'higher scores.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to:  ../Eval_outputs/SPIQA/ReAlignQA/batch_20_20250425_000533.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffed7ff5c31f4b41b31059541724d317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 30:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The highest number of network-related reboots during the deployment in the '\n",
      "                                                     'tier-1 datacenter occurred within a time window from approximately 18:05 to '\n",
      "                                                     '19:04.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The TF-IDF (Term Frequency-Inverse Document Frequency) and BM25 (Best '\n",
      "                                                     'Matching Baseline 25) features differ in their approach to estimating '\n",
      "                                                     'document relevance within the \"Unbiased Learning to Rank with Unbiased '\n",
      "                                                     'Propensity Estimation\" framework. While TF-IDF calculates a weighted score '\n",
      "                                                     'based on the frequency of query terms within a document and their rarity '\n",
      "                                                     'across all documents, BM25 employs probabilistic modelling to estimate '\n",
      "                                                     'relevance.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'The differences between these two features imply that TF-IDF may be more '\n",
      "                                                     'attuned to the specific terms used by users, as it directly considers their '\n",
      "                                                     'presence and frequency. On the other hand, BM25 might offer a better '\n",
      "                                                     'understanding of document coherence and overall relevance, thanks to its '\n",
      "                                                     'probabilistic approach.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'In the context of the experiments conducted in this paper, these differences '\n",
      "                                                     'could lead to improved ranking performance when incorporating TF-IDF or BM25 '\n",
      "                                                     'features as part of unbiased learning to rank systems. As a result, users in '\n",
      "                                                     'search engines and other information retrieval applications may benefit from '\n",
      "                                                     'more accurate results.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The \"SDVI loss term 1&3\" model, as presented in Table 1 of the \"Stochastic '\n",
      "                                                     'Dynamics for Video Infilling\" paper, exhibits lower Performance Peak '\n",
      "                                                     'Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) '\n",
      "                                                     'compared to the full SDVI model. However, the specific missing loss terms in '\n",
      "                                                     'this abbreviated model that could account for its inferior performance are '\n",
      "                                                     \"not explicitly stated in the table. It's reasonable to assume that these \"\n",
      "                                                     'missing components play a significant role in the quality of video frame '\n",
      "                                                     'regeneration. To enhance its performance, researchers could explore adding '\n",
      "                                                     'back the omitted loss terms or incorporating additional models tailored to '\n",
      "                                                     'optimize the infilling process across various datasets.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The \"Last contact\" feature can positively influence the SSL score despite '\n",
      "                                                     'not being directly utilized by the SSL algorithm because it indicates an '\n",
      "                                                     \"individual's recent engagement with the data or system. This engagement \"\n",
      "                                                     'could indirectly impact other factors that contribute to the overall SSL '\n",
      "                                                     'score, such as user trust and satisfaction. By understanding these complex '\n",
      "                                                     'relationships between data and influencing factors, we can better appreciate '\n",
      "                                                     'how multiple factors can collectively affect a single measure like the SSL '\n",
      "                                                     'score.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The higher number of identity switches (IDS) observed in the model trained '\n",
      "                                                     'on both the detection and MOT sets (\"MOT + Det\") can be attributed to the '\n",
      "                                                     'fact that this model was trained using data that encompasses both object '\n",
      "                                                     'detection and motion tracking tasks. This combined training dataset might '\n",
      "                                                     'introduce a significant amount of false positives or negatives during the '\n",
      "                                                     'detection phase, which in turn may lead to more identity switches when '\n",
      "                                                     'tracking objects over time. Although the \"MOT + Det\" model demonstrates '\n",
      "                                                     'improvements in other tracking metrics like AP (Average Precision), MOTA '\n",
      "                                                     '(Multiple Object Tracking Accuracy), and MOTP (Multiple Object Tracking '\n",
      "                                                     'Precision), the inclusion of detection tasks could result in a performance '\n",
      "                                                     'trade-off, causing more IDS during testing.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"According to Figure 4 of the paper, as the variable 'a' increases with fixed \"\n",
      "                                                     'values of b=0.3 and N=100, there is a noticeable linear increase in both the '\n",
      "                                                     'upper bound of KL divergence (left panel) and the corresponding p-value '\n",
      "                                                     '(right panel). This trend suggests that higher-order feature interactions '\n",
      "                                                     \"become increasingly statistically significant as 'a' increases. This finding \"\n",
      "                                                     'underscores the importance of considering higher-order feature interactions '\n",
      "                                                     'in machine learning models, particularly when data is limited, as it can '\n",
      "                                                     'lead to improved model performance and interpretability.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"The telescoping architecture in Bing's retrieval system implements the \"\n",
      "                                                     'rank-and-prune process across stages L1 and L2 after documents are initially '\n",
      "                                                     'matched in stage L0 using a pre-defined match plan. This process is designed '\n",
      "                                                     'to enhance search results by improving their quality through multiple '\n",
      "                                                     'stages.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'In the first stage, L1, the rank-and-prune process begins. Two pruning '\n",
      "                                                     'mechanisms are utilized at this stage: one based on after-matching '\n",
      "                                                     'probabilities and another that ranks documents based on their content. The '\n",
      "                                                     'first mechanism helps maintain a balance between diversity and relevance in '\n",
      "                                                     'search results by eliminating less relevant documents, while the second '\n",
      "                                                     'ensures that the most informative documents rise to the top of the list.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'Stage L2 is where the final rankings are generated using both pre-processing '\n",
      "                                                     'and post-processing techniques. These techniques help further improve the '\n",
      "                                                     'quality of the search results by refining the rankings and ensuring that the '\n",
      "                                                     'most relevant results are presented to users.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'Overall, the telescoping architecture effectively optimizes the retrieval '\n",
      "                                                     \"process in Bing's system by employing multiple stages with tailored \"\n",
      "                                                     'mechanisms for each stage, ultimately providing high-quality search results '\n",
      "                                                     'to users.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Figure 1 in the paper demonstrates how the QoU (Question-oriented Utility) '\n",
      "                                                     'and IoU (Image-oriented Utility) procedures address selection bias in the '\n",
      "                                                     'Visual7W dataset by generating alternative decoys or choices for each '\n",
      "                                                     'question, making both the correct answer and the decoys equally likely. This '\n",
      "                                                     'is achieved by examining either the image or the question alone, which '\n",
      "                                                     'forces machine learning models to consider all available information instead '\n",
      "                                                     'of relying solely on the most frequently occurring answers.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'In the example illustrated in Figure 1, the correct answer \"A train\" has a '\n",
      "                                                     'higher probability score because it is often selected as the correct answer '\n",
      "                                                     'due to its frequent use. However, by creating alternative decoys that are '\n",
      "                                                     'also highly likely, both the correct answer and the decoys become equally '\n",
      "                                                     'challenging for machines to distinguish, ensuring they need to consider all '\n",
      "                                                     'information together. This approach helps evaluate how well a learning '\n",
      "                                                     'algorithm can understand all information equally well, rather than being '\n",
      "                                                     'biased towards the most frequent answers or categories.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'The figure itself depicts an image of a train on tracks surrounded by '\n",
      "                                                     'various types of vehicles, such as cars, buses, and trucks. The presence of '\n",
      "                                                     'these alternative vehicles serves to create a more diverse set of examples '\n",
      "                                                     'for the model to learn from, ensuring it is not overly focused on one type '\n",
      "                                                     'or category while training. This diversity helps remedy selection bias in '\n",
      "                                                     'the dataset, allowing the machine learning model to make more accurate and '\n",
      "                                                     'balanced predictions across various types of transportation.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): (\"The improvement in the model's accuracy from 66.7% to 100% in the sequence \"\n",
      "                                                     'transduction example depicted can be attributed to the application of '\n",
      "                                                     'gradient-based inference to enforce output constraints. This technique '\n",
      "                                                     'allows the model to adapt and learn from its errors, gradually improving its '\n",
      "                                                     'performance over time. The image provided illustrates this process where the '\n",
      "                                                     'input data is processed by an algorithm that becomes more accurate as '\n",
      "                                                     'iterations progress.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the cosine similarity analysis presented in the figure, it can be '\n",
      "                                                     'observed that TRPO performs better than PPO in estimating the true gradient '\n",
      "                                                     'with fewer state-action pairs. This indicates that TRPO has a higher '\n",
      "                                                     'accuracy in approximating the true gradient, which results in improved '\n",
      "                                                     'convergence behavior for learning. In other words, TRPO requires fewer '\n",
      "                                                     'samples to converge to an optimal policy compared to PPO.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to:  ../Eval_outputs/SPIQA/ReAlignQA/batch_30_20250425_003221.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7299c2bce546b3aa3408d12b0f3ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 40:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The qualitative results from the ablation figure in the Arbitrary Talking '\n",
      "                                                     'Face Generation paper suggest that while both the baseline method and the '\n",
      "                                                     'proposed methods generate faces, there is an improvement in visual clarity '\n",
      "                                                     'and facial detail in the proposed methods compared to the baseline. '\n",
      "                                                     \"Specifically, the 'A' indicates better results for the proposed methods, \"\n",
      "                                                     \"while the '-' shows worse performance for the baseline, and the 'D' suggests \"\n",
      "                                                     'a decline in quality when using the ablation of specific components in the '\n",
      "                                                     'proposed methods (DA). However, it is important to note that this assessment '\n",
      "                                                     'is based on qualitative analysis.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The two-dimensional parameter space depicted in the provided image '\n",
      "                                                     'represents the variations in the initial conditions of a liquid drop. The '\n",
      "                                                     'position of the drop along the x-axis is represented by α1, and its size is '\n",
      "                                                     'represented by α2. This setup consists of multiple two-dimensional '\n",
      "                                                     'simulations that allow for exploration of how different positions and sizes '\n",
      "                                                     'impact the behavior of the liquid drop.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'In particular, the height or position of the drop (represented on the '\n",
      "                                                     'vertical axis) affects factors like evaporation rate and gravitational '\n",
      "                                                     'forces, while the width or size of the drop (represented on the horizontal '\n",
      "                                                     'axis) influences the contact angle between the drop and a surface or its '\n",
      "                                                     'spreading on a flat surface.\n",
      "                                                    '\n",
      "                                                     '\n",
      "                                                    '\n",
      "                                                     'By examining the variations in both dimensions within this parameter space, '\n",
      "                                                     'we can gain insights into how different initial conditions will affect the '\n",
      "                                                     'overall dynamics of the liquid drop throughout various processes such as '\n",
      "                                                     'drying, cooling, or condensation. This representation is crucial for '\n",
      "                                                     'understanding and predicting the behavior of liquid drops under diverse '\n",
      "                                                     'conditions.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The simple two-layer parameter network, as depicted in Figure 12, is less '\n",
      "                                                     'complex and functional compared to the more intricate deformation network. '\n",
      "                                                     'The difference lies in their respective roles and capabilities. While the '\n",
      "                                                     'two-layer network learns to apply multiple long-range, non-linear '\n",
      "                                                     'deformation fields, it is less advanced than the deformation network that '\n",
      "                                                     'employs de-convolutional layers. This latter network generates dense '\n",
      "                                                     'deformation fields for refining liquid surfaces, a much more sophisticated '\n",
      "                                                     'technique compared to the basic function of the two-layer parameter network.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the analysis of the provided figure that depicts real-world '\n",
      "                                                     'photographs and reconstructed images under different conditions, it appears '\n",
      "                                                     'that there is a noticeable discrepancy between the original samples (top '\n",
      "                                                     'row) and their reconstructions (bottom row). The reconstructions do not seem '\n",
      "                                                     'to closely match the detail and quality of the original photos, particularly '\n",
      "                                                     'when comparing the bottom row to the top row. This suggests that further '\n",
      "                                                     'refinement or adjustments may be necessary in the reconstruction process to '\n",
      "                                                     'improve alignment with the original samples.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the provided figure and considering that the visual model has '\n",
      "                                                     \"indicated that it doesn't have sufficient data for an accurate comparison, I \"\n",
      "                                                     'cannot definitively state the differences in the temporal patterns of '\n",
      "                                                     'phenotype magnitude, shape, and periodicity between the sickle cell anemia '\n",
      "                                                     'patient and the leukemia patient as described in the \"COPA: Constrained '\n",
      "                                                     'PARAFAC2 for Sparse & Large Datasets\" paper. To provide a detailed '\n",
      "                                                     'comparison, I would need specific data or information about the patterns '\n",
      "                                                     'observed by each method (COPA, Helwig, and SPARTan) for both patients, which '\n",
      "                                                     \"is not available from this visual model's response.\")\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the data presented in Table 5 of the \"Learning Visually Grounded '\n",
      "                                                     'Sentence Representations\" paper, the Spearman correlation scores of the '\n",
      "                                                     \"Cap2Img model's word embeddings are consistently lower than those of GloVe \"\n",
      "                                                     'embeddings across all four semantic similarity benchmarks. This suggests '\n",
      "                                                     'that, according to these benchmarks, the GloVe embeddings perform better at '\n",
      "                                                     'capturing semantic similarities between words compared to the Cap2Img '\n",
      "                                                     \"model's word embeddings.\")\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('Based on the provided context, while the figure does not explicitly show how '\n",
      "                                                     'Action Search leverages temporal context from previously and subsequently '\n",
      "                                                     'searched frames, it is implied through observations made in the failure '\n",
      "                                                     'cases. In instances where the target action location was not spotted '\n",
      "                                                     'exactly, the model is seen to oscillate around actions, suggesting that it '\n",
      "                                                     'considers both previous and subsequent frames to predict where the action '\n",
      "                                                     'might occur next. This inference is drawn from the repeated movement of the '\n",
      "                                                     'search around the action area rather than a direct demonstration of temporal '\n",
      "                                                     'context usage shown in the figure itself.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'YES'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('In the paper \"Gradient-based Inference for Networks with Output '\n",
      "                                                     'Constraints,\" the reduction in average disagreement rate for the failure set '\n",
      "                                                     'of the SRL-100 network when using GBI is reported to be 32%. However, the '\n",
      "                                                     'specific percentage of reduction for A* is not directly visible from the '\n",
      "                                                     'provided table.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('According to the data presented in the \"Randomized Experimental Design via '\n",
      "                                                     'Geographic Clustering\" paper, GeoCUTS performs comparably well for highly '\n",
      "                                                     'active users and even slightly better for highly mobile users compared to '\n",
      "                                                     \"DMA and Grid clustering methods. However, it's important to note that the \"\n",
      "                                                     'statement does not imply GeoCUTs outperforms both DMA and Grid in all '\n",
      "                                                     'aspects or for all types of users. The performance is relative, specifically '\n",
      "                                                     'for highly active and highly mobile users.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('HUMBI utilizes a diverse group of 772 models in their photoshoots, which '\n",
      "                                                     'encompass different ethnicities, genders, ages, clothing styles, and '\n",
      "                                                     'physical conditions. This diverse range allows them to capture a broad '\n",
      "                                                     'spectrum of human body expressions. Each model is documented by 107 '\n",
      "                                                     'high-definition cameras that focus on facial expressions, gazes, hand '\n",
      "                                                     'positions, body movements, and garment details. This comprehensive approach '\n",
      "                                                     'results in a vast repository of images that can appeal to a wide audience '\n",
      "                                                     'due to its inclusive representation.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to:  ../Eval_outputs/SPIQA/ReAlignQA/batch_40_20250425_005431.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1c1837c0184755815ef53f79225549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 50:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.mistral.generate_answer(detection_prompt): 'NO'\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n",
      "ic| self.mistral.generate_answer(synthesis_prompt): ('The Pairwise Confusion (PC) method improves the localization performance of '\n",
      "                                                     'the VGG-16 model by focusing more accurately on the target objects in the '\n",
      "                                                     'Grad-CAM heatmaps compared to the baseline model. This is demonstrated in '\n",
      "                                                     'Figure 3 of the paper, where PC leads to tighter and more accurate '\n",
      "                                                     'localization of target objects like birds, whereas the baseline VGG-16 '\n",
      "                                                     'network often focuses on artifacts even when making correct predictions. The '\n",
      "                                                     'comparison between the heatmaps generated from this method and the baseline '\n",
      "                                                     'model helps to enhance the accuracy and focus on the target objects within '\n",
      "                                                     'the photographs. This is particularly useful in scenarios with potential '\n",
      "                                                     'confusion between objects such as birds and people, as shown in one instance '\n",
      "                                                     'where a person was holding an umbrella, suggesting some level of overlap or '\n",
      "                                                     'confusion in object perception.')\n",
      "ic| self.mistral.generate_answer(detection_prompt): 'Yes'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m     question = row[\u001b[33m\"\u001b[39m\u001b[33mold_question\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     caption = row[\u001b[33m\"\u001b[39m\u001b[33mcaption\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     answer = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     results.append({\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: answer\n\u001b[32m     24\u001b[39m     })\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\RGU-Y4-C\\FYP\\FYIRP\\src\\text_generator.py:57\u001b[39m, in \u001b[36mMultimodalAnswerPipeline.answer_question\u001b[39m\u001b[34m(self, question, caption, image_path)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_visual_analysis(caption, question):\n\u001b[32m     56\u001b[39m     vlm_answer = \u001b[38;5;28mself\u001b[39m.llava.generate_answer(image_path=image_path, question=question, caption=caption)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     final_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynthesize_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvlm_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvlm_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     final_answer = \u001b[38;5;28mself\u001b[39m.mistral.generate_answer(question=question, context=caption)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\RGU-Y4-C\\FYP\\FYIRP\\src\\text_generator.py:52\u001b[39m, in \u001b[36mMultimodalAnswerPipeline.synthesize_answer\u001b[39m\u001b[34m(self, question, caption, vlm_answer)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msynthesize_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, question, caption, vlm_answer):\n\u001b[32m     43\u001b[39m         synthesis_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[33mYou are a scientific assistant synthesizing a final answer.\u001b[39m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33mWrite a clear, complete answer based on this information.\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ic(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmistral\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthesis_prompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\RGU-Y4-C\\FYP\\FYIRP\\src\\text_generator.py:17\u001b[39m, in \u001b[36mMistralGenerator.generate_answer\u001b[39m\u001b[34m(self, question, context)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, question, context=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     16\u001b[39m     prompt = \u001b[38;5;28mself\u001b[39m.prompt_template.format(context=context, question=question)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\openai\\_base_client.py:955\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    952\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    961\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Batch processing\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    if i in existing_batches:\n",
    "        print(f\"Skipping batch {i} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    batch_df = df.iloc[i:i + BATCH_SIZE]\n",
    "    results = []\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {i}\"):\n",
    "        try:\n",
    "            image_path = os.path.join(IMAGE_FOLDER, row[\"doc_id\"], row[\"reference_figure\"])\n",
    "            question = row[\"old_question\"]\n",
    "            caption = row[\"caption\"]\n",
    "            answer = pipeline.answer_question(question,caption,image_path)\n",
    "\n",
    "            results.append({\n",
    "                \"index\": idx,\n",
    "                \"question\": question,\n",
    "                \"caption\": caption,\n",
    "                \"image\": image_path,\n",
    "                \"response\": answer\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    # results = {}\n",
    "\n",
    "    duration = time.time() - start\n",
    "    save_batch_output(results, OUTPUT_DIR, i)\n",
    "    log_time(OUTPUT_DIR, i, duration, prefix=\"vision_only\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
